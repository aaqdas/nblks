diff --git a/Makefile b/Makefile
index 7d7cf00..fb54183 100644
--- a/Makefile
+++ b/Makefile
@@ -2,7 +2,7 @@
 VERSION = 5
 PATCHLEVEL = 4
 SUBLEVEL = 43
-EXTRAVERSION =
+EXTRAVERSION = -netchannel
 NAME = Kleptomaniac Octopus
 
 # *DOCUMENTATION*
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 757c0fd..a6f80f2 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -40,6 +40,9 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/* blk-switch header */
+#include "blk-switch.h"
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
@@ -363,6 +366,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	bool clear_ctx_on_error = false;
 	u64 alloc_time_ns = 0;
 
+	/* blk-switch variables */
+	int nr_cpus, nr_nodes = num_online_nodes();
+	bool req_steered, app_steered;
+	req_steered = app_steered = false;
+	if (blk_switch_nr_cpus <= 0 ||
+	   blk_switch_nr_cpus > num_online_cpus())
+		nr_cpus = num_online_cpus();
+	else
+		nr_cpus = blk_switch_nr_cpus;
+
 	blk_queue_enter_live(q);
 
 	/* alloc_time includes depth and tag waits */
@@ -380,6 +393,318 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	if (data->cmd_flags & REQ_NOWAIT)
 		data->flags |= BLK_MQ_REQ_NOWAIT;
 
+	/*
+	 * blk-switch: (1) reset variables, (2) print out statistics
+	 *		if there's no traffic for 1000ms
+	 */
+	if (blk_switch_request(bio, data)) {
+		if (blk_switch_reset_metrics == 0 ||
+		   time_after(jiffies, blk_switch_reset_metrics)) {
+			int i;
+
+			for (i = 0; i < nr_cpus; i++) {
+				blk_switch_T_bytes[i] = 0;
+				blk_switch_L_bytes[i] = 0;
+				blk_switch_T_metric[i] = 0;
+				blk_switch_L_metric[i] = 0;
+				blk_switch_T_appstr[i] = 0;
+				blk_switch_L_appstr[i] = 0;
+				blk_switch_stats_print[i] = 1;
+			}
+
+			blk_switch_last_appstr = 0;
+			blk_switch_appstr_app = 0;
+		}
+
+		blk_switch_reset_metrics = jiffies
+				+ msecs_to_jiffies(BLK_SWITCH_RESET_METRICS);
+
+		if (blk_switch_debug && blk_switch_reset_metrics &&
+		   blk_switch_stats_print[current->cpu]) {
+			printk("(pid %d cpu %2d) reqstr stats: gen %8d str %8d prc %8d",
+				current->pid, current->cpu,
+				blk_switch_stats_gen[current->cpu],
+				blk_switch_stats_str[current->cpu],
+				blk_switch_stats_prc[current->cpu]);
+
+			blk_switch_stats_gen[current->cpu] = 0;
+			blk_switch_stats_str[current->cpu] = 0;
+			blk_switch_stats_prc[current->cpu] = 0;
+			blk_switch_stats_print[current->cpu] = 0;
+		}
+	}
+	
+	/*
+	 * blk-switch: Application Steering
+	 */
+	if (blk_switch_on >= 2 && blk_switch_request(bio, data)) {
+		unsigned long L_core, T_core, min_metric;
+		unsigned long iter_L_core, iter_T_core;
+		int cur_cpu = data->ctx->cpu;
+		int cur_node = data->hctx->numa_node;
+		int i, sample = 0, iter_cpu, target_cpu = -1;
+
+		/* 1-1. Update T_core */
+		sample = blk_switch_T_bytes[cur_cpu];
+
+		if (blk_switch_is_thru_request(bio)) {
+			sample += bio->bi_iter.bi_size;
+
+			if (blk_switch_T_metric[cur_cpu] == 0)
+				blk_switch_T_metric[cur_cpu] = sample;
+			else {
+				blk_switch_T_metric[cur_cpu] -=
+					(blk_switch_T_metric[cur_cpu] >> 3);
+				blk_switch_T_metric[cur_cpu] +=
+					(sample >> 3);
+			}
+		}
+		if (blk_switch_T_metric[cur_cpu] < 10)
+			blk_switch_T_metric[cur_cpu] = 0;
+
+		/* 1-2. Update L_core */
+		sample = blk_switch_L_bytes[cur_cpu];
+
+		if (!blk_switch_is_thru_request(bio)) {
+			sample += bio->bi_iter.bi_size;
+
+			if (blk_switch_L_metric[cur_cpu] == 0)
+				blk_switch_L_metric[cur_cpu] = sample;
+			else {
+				blk_switch_L_metric[cur_cpu] -=
+					(blk_switch_L_metric[cur_cpu] >> 3);
+				blk_switch_L_metric[cur_cpu] +=
+					(sample >> 3);
+			}
+		}
+		if (blk_switch_L_metric[cur_cpu] < 10)
+			blk_switch_L_metric[cur_cpu] = 0;
+
+		/* 2. Determine target_cpu every 10ms */
+		if (blk_switch_last_appstr == 0)
+			blk_switch_last_appstr = jiffies +
+				msecs_to_jiffies(BLK_SWITCH_APPSTR_INTERVAL);
+		else if (time_after(jiffies, blk_switch_last_appstr)) {
+			int i;
+
+			for (i = 0; i < nr_cpus; i++) {
+				blk_switch_T_appstr[i] = blk_switch_T_metric[i];
+				blk_switch_L_appstr[i] = blk_switch_L_metric[i];
+			}
+			blk_switch_last_appstr = jiffies +
+				msecs_to_jiffies(BLK_SWITCH_APPSTR_INTERVAL);
+
+			if (blk_switch_appstr_app == BLK_SWITCH_T_APP)
+				blk_switch_appstr_app = BLK_SWITCH_L_APP;
+			else
+				blk_switch_appstr_app = BLK_SWITCH_T_APP;
+
+			/* REMOVE: only for internal debugging */
+			if (blk_switch_debug == 3)
+				printk("(pid %d cpu %d) -(%d)- %s [0](%lu %lu) [4](%lu %lu) [8](%lu %lu) [12](%lu %lu) [16](%lu %lu) [20](%lu %lu)",
+					current->pid, current->cpu, blk_switch_appstr_app,
+					IOPRIO_PRIO_CLASS(bio_prio(bio)) == BLK_SWITCH_L_APP ? "L":"T",
+					blk_switch_L_appstr[0], blk_switch_T_appstr[0],
+					blk_switch_L_appstr[4], blk_switch_T_appstr[4],
+					blk_switch_L_appstr[8], blk_switch_T_appstr[8],
+					blk_switch_L_appstr[12], blk_switch_T_appstr[12],
+					blk_switch_L_appstr[16], blk_switch_T_appstr[16],
+					blk_switch_L_appstr[20], blk_switch_T_appstr[20]);
+		}
+
+		L_core = blk_switch_L_appstr[cur_cpu];
+		T_core = blk_switch_T_appstr[cur_cpu];
+
+		if (!blk_switch_is_thru_request(bio))
+			min_metric = L_core + T_core;
+		else
+			min_metric = 8388608;
+
+		for (i = 0; i < nr_cpus/nr_nodes; i++) {
+			iter_cpu = i * nr_nodes + cur_node;
+			iter_L_core = blk_switch_L_appstr[iter_cpu];
+			iter_T_core = blk_switch_T_appstr[iter_cpu];
+
+			if (iter_cpu == data->ctx->cpu ||
+			   iter_L_core > BLK_SWITCH_THRESH_L)
+				continue;
+
+			/* Find target_cpu for L-apps */
+			if (blk_switch_appstr_app == BLK_SWITCH_L_APP &&
+			   !blk_switch_is_thru_request(bio)) {
+				if (T_core && iter_L_core &&
+				   (L_core + T_core) > (iter_L_core + iter_T_core) &&
+				   min_metric > (iter_L_core + iter_T_core)) {
+					target_cpu = iter_cpu;
+					min_metric = iter_L_core + iter_T_core;
+				}
+			}
+			/* Find target_cpu for T-apps */
+			else if (blk_switch_appstr_app == BLK_SWITCH_T_APP &&
+				blk_switch_is_thru_request(bio)) {
+				// wait for L-app to move first
+				if (L_core < iter_L_core &&
+				   iter_L_core < BLK_SWITCH_THRESH_L) {
+					target_cpu = -1;
+					break;
+				}
+				else if (L_core > iter_L_core &&
+					min_metric > iter_T_core) {
+					target_cpu = iter_cpu;
+					min_metric = iter_T_core;
+				}
+			}
+		}
+
+		/* 3. Perform app-steering */
+		if (target_cpu >= 0) {
+			struct cpumask *mask;
+			mask = kcalloc(1, sizeof(struct cpumask), GFP_KERNEL);
+
+			if (blk_switch_debug) {
+				printk(KERN_ERR "(pid %d cpu %2d) %s app (%lu %lu) -> (%lu %lu) core %d",
+					current->pid, current->cpu,
+					IOPRIO_PRIO_CLASS(bio_prio(bio)) == BLK_SWITCH_L_APP ? "L":"T",
+					L_core, T_core,
+					blk_switch_L_appstr[target_cpu],
+					blk_switch_T_appstr[target_cpu],
+					target_cpu);
+			}
+
+			if (!blk_switch_is_thru_request(bio) &&
+				atomic_read(&data->hctx->tags->active_queues) <= 1)
+				blk_switch_L_metric[cur_cpu] = 0;
+			else if (blk_switch_is_thru_request(bio) &&
+				atomic_read(&data->hctx->tags->active_queues) <= 1)
+				blk_switch_T_metric[cur_cpu] = 0;
+
+			cpumask_clear(mask);
+			cpumask_set_cpu(target_cpu, mask);
+			sched_setaffinity(current->pid, mask);
+			kfree(mask);
+		}
+
+		if (current->cpu != cur_cpu) {
+			data->ctx = per_cpu_ptr(q->queue_ctx, current->cpu);
+			data->hctx = blk_mq_map_queue(q, data->cmd_flags, data->ctx);
+			app_steered = true;
+		}
+	}
+
+	/*
+	 * blk-switch: Request Steering for T-apps
+	 */
+	if (blk_switch_on >= 1 && blk_switch_request(bio, data) && 
+	   blk_switch_is_thru_request(bio) &&
+	   !app_steered && nr_cpus >= nr_nodes * 2) {
+		struct blk_mq_hw_ctx *iter_hctx;
+		struct nvme_tcp_queue *driver_queue;
+		int cur_cpu = data->ctx->cpu;
+		int cur_node = data->hctx->numa_node;
+		int i, two_cpu[2], two_nr[2], target_cpu = -1;
+		int min_nr = 1024, min_active = 2048;
+		int T_active, req_thresh;
+		unsigned char two_rand[2];
+		unsigned long L_metric;
+
+		/* 1. Push T-requests into local queue until it becomes busy */
+		iter_hctx = data->ctx->hctxs[HCTX_TYPE_DEFAULT];
+		T_active = atomic_read(&iter_hctx->nr_active);
+
+		if (blk_switch_thresh_B > 0)
+			req_thresh = blk_switch_thresh_B * 2;
+		else {
+			if (data->hctx->blk_switch == BLK_SWITCH_TCP)
+				req_thresh = BLK_SWITCH_TCP_BATCH * 2;
+			else
+				req_thresh = BLK_SWITCH_TCP_BATCH / 2;
+		}
+
+		if (T_active <= req_thresh) {
+			target_cpu = cur_cpu;
+			goto req_steering;
+		}
+		else
+			blk_switch_stats_str[cur_cpu]++;
+
+		for (i = 0; i < nr_cpus/nr_nodes; i++) {
+			two_cpu[0] = i * nr_nodes + cur_node;
+			iter_hctx = per_cpu_ptr(q->queue_ctx, two_cpu[0])->hctxs[HCTX_TYPE_DEFAULT];
+			T_active = atomic_read(&iter_hctx->nr_active);
+			L_metric = blk_switch_L_metric[two_cpu[0]];
+
+			if (data->hctx->blk_switch == BLK_SWITCH_TCP) {
+				driver_queue = iter_hctx->driver_data;
+				two_nr[0] = BLK_SWITCH_TCP_BATCH -
+						atomic_read(&driver_queue->nr_req);
+			}
+			else
+				two_nr[0] = 0;
+
+			/* 2. Pick-up other queue considering i10 batching */
+			if (!L_metric && (data->hctx->blk_switch == BLK_SWITCH_RDMA ||
+			   (data->hctx->blk_switch == BLK_SWITCH_TCP &&
+			   T_active < BLK_SWITCH_TCP_BATCH))) {
+				if (two_nr[0] < min_nr) {
+					target_cpu = two_cpu[0];
+					min_nr = two_nr[0];
+					min_active = T_active;
+				}
+				// 2-1) considering #outstanding requests
+				else if (two_nr[0] == min_nr) {
+					if (T_active < min_active) {
+						target_cpu = two_cpu[0];
+						min_active = T_active;
+					}
+					// 2-2) considering local queue
+					else if (T_active == min_active) {
+						if (two_cpu[0] == cur_cpu)
+							target_cpu = two_cpu[0];
+						// 2-3) randomly choose one among remainings
+						else if (target_cpu != cur_cpu) {
+							get_random_bytes(&two_rand[0], 1);
+							two_rand[0] %= 2;
+							if (two_rand[0] == 0)
+								target_cpu = two_cpu[0];
+						}
+					}
+				}
+			}
+		}
+
+		/* 3. Otherwise, run power-of-two-choices among cores */
+		if (target_cpu < 0) {
+			get_random_bytes(&two_rand[0], 1);
+			two_rand[0] %= nr_cpus / nr_nodes;
+			two_cpu[0] = two_rand[0] * nr_nodes + cur_node;
+
+			do {
+				get_random_bytes(&two_rand[1], 1);
+				two_rand[1] %= nr_cpus / nr_nodes;
+				two_cpu[1] = two_rand[1] * nr_nodes + cur_node;
+			} while(two_cpu[0] == two_cpu[1]);
+
+			iter_hctx = per_cpu_ptr(q->queue_ctx, two_cpu[0])->hctxs[HCTX_TYPE_DEFAULT];
+			two_nr[0] = atomic_read(&iter_hctx->nr_active);
+			iter_hctx = per_cpu_ptr(q->queue_ctx, two_cpu[1])->hctxs[HCTX_TYPE_DEFAULT];
+			two_nr[1] = atomic_read(&iter_hctx->nr_active);
+
+			if (two_nr[0] <= two_nr[1])
+				target_cpu = two_cpu[0];
+			else
+				target_cpu = two_cpu[1];
+		}
+
+req_steering:
+		blk_switch_stats_gen[cur_cpu]++;
+		if (cur_cpu != target_cpu) {
+			blk_switch_stats_prc[target_cpu]++;
+			data->ctx = per_cpu_ptr(q->queue_ctx, target_cpu);
+			data->hctx = blk_mq_map_queue(q, data->cmd_flags, data->ctx);
+			req_steered = true;
+		}
+	}
+
 	if (e) {
 		data->flags |= BLK_MQ_REQ_INTERNAL;
 
@@ -405,6 +730,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	}
 
 	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags, alloc_time_ns);
+
+	/* blk-switch: for output-port stats */
+	if (data->hctx->blk_switch && bio && !req_steered) {
+		if (blk_switch_is_thru_request(bio))
+			blk_switch_T_bytes[data->ctx->cpu] += bio->bi_iter.bi_size;
+		else
+			blk_switch_L_bytes[data->ctx->cpu] += bio->bi_iter.bi_size;
+	}
+	rq->steered = req_steered;
+
 	if (!op_is_flush(data->cmd_flags)) {
 		rq->elv.icq = NULL;
 		if (e && e->type->ops.prepare_request) {
@@ -565,6 +900,23 @@ EXPORT_SYMBOL(__blk_mq_end_request);
 
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
+	/* blk-switch: for output-port stats */
+	if (rq->mq_hctx->blk_switch && blk_rq_bytes(rq) &&
+	   !rq->steered && rq->tag && rq->bio) {
+		if (blk_switch_is_thru_request(rq->bio)) {
+			if (blk_switch_T_bytes[rq->mq_ctx->cpu] >= blk_rq_bytes(rq))
+				blk_switch_T_bytes[rq->mq_ctx->cpu] -= blk_rq_bytes(rq);
+			else
+				blk_switch_T_bytes[rq->mq_ctx->cpu] = 0;
+		}
+		else {
+			if (blk_switch_L_bytes[rq->mq_ctx->cpu] >= blk_rq_bytes(rq))
+				blk_switch_L_bytes[rq->mq_ctx->cpu] -= blk_rq_bytes(rq);
+			else
+				blk_switch_L_bytes[rq->mq_ctx->cpu] = 0;
+		}
+	}
+
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
 		BUG();
 	__blk_mq_end_request(rq, error);
@@ -1961,6 +2313,9 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	unsigned int nr_segs;
 	blk_qc_t cookie;
 
+	/* blk-switch */
+	blk_switch_set_ioprio(current, bio);
+
 	blk_queue_bounce(q, &bio);
 	__blk_queue_split(q, &bio, &nr_segs);
 
@@ -2330,6 +2685,9 @@ static int blk_mq_init_hctx(struct request_queue *q,
 
 	hctx->tags = set->tags[hctx_idx];
 
+	/* blk-switch */
+	hctx->blk_switch = 0;
+
 	if (set->ops->init_hctx &&
 	    set->ops->init_hctx(hctx, set->driver_data, hctx_idx))
 		goto unregister_cpu_notifier;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index f207597..46d2ae6 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -111,9 +111,12 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 	 */
 	if (flags & REQ_HIPRI)
 		type = HCTX_TYPE_POLL;
-	else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
-		type = HCTX_TYPE_READ;
-	
+	//else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
+	//	type = HCTX_TYPE_READ;
+	/*  blk-switch: type for latency-sensitive apps */
+	else if (flags & REQ_PRIO && !(flags & REQ_FUA))
+		type = HCTX_TYPE_READ;	
+
 	return ctx->hctxs[type];
 }
 
diff --git a/block/blk-switch.h b/block/blk-switch.h
new file mode 100644
index 0000000..665288d
--- /dev/null
+++ b/block/blk-switch.h
@@ -0,0 +1,153 @@
+/*
+ *	"Rearchitecting Linux Storage Stack for
+ *		microsecond Latency and High Throughput",
+ *		USENIX OSDI 2021.
+ *
+ *	Authors:
+ *		Jaehyun Hwang <jaehyun.hwang@cornell.edu>
+ *		Midhul Vuppalapati <mvv25@cornell.edu>
+ *		Simon Peter <simon@cs.utexas.edu>
+ *		Rachit Agarwal <ragarwal@cs.cornell.edu>
+ *
+ *	SPDX-License-Identifier: GPL-2.0
+ */
+
+#include <linux/random.h>
+#include <linux/sched/task.h>
+
+#define BLK_SWITCH_NR_CPUS		64
+
+#define BLK_SWITCH_RESET_METRICS	1000	// 1000ms
+#define BLK_SWITCH_APPSTR_INTERVAL	10	// 10ms
+#define BLK_SWITCH_TCP_BATCH		16
+#define BLK_SWITCH_THRESH_L		98304	// (24*4096) bytes
+
+static int blk_switch_on __read_mostly;
+module_param(blk_switch_on, int, 0644);
+MODULE_PARM_DESC(blk_switch_on, "0: prio, 1: +reqstr, 2: +appstr ");
+
+static int blk_switch_thresh_B __read_mostly = 8;
+module_param(blk_switch_thresh_B, int, 0644);
+MODULE_PARM_DESC(blk_switch_thresh_B, "blk-switch threshold for B");
+
+static int blk_switch_nr_cpus __read_mostly;
+module_param(blk_switch_nr_cpus, int, 0644);
+MODULE_PARM_DESC(blk_switch_nr_cpus, "blk-switch nr_cpus");
+
+static int blk_switch_debug __read_mostly;
+module_param(blk_switch_debug, int, 0644);
+MODULE_PARM_DESC(blk_switch_debug, "blk-switch printk on/off");
+
+unsigned long blk_switch_T_bytes[BLK_SWITCH_NR_CPUS] = {0};
+unsigned long blk_switch_L_bytes[BLK_SWITCH_NR_CPUS] = {0};
+unsigned long blk_switch_T_metric[BLK_SWITCH_NR_CPUS] = {0};	// T_(cur_cpu)
+unsigned long blk_switch_L_metric[BLK_SWITCH_NR_CPUS] = {0};	// L_(cur_cpu)
+unsigned long blk_switch_T_appstr[BLK_SWITCH_NR_CPUS] = {0};
+unsigned long blk_switch_L_appstr[BLK_SWITCH_NR_CPUS] = {0};
+
+unsigned long blk_switch_reset_metrics = 0;
+unsigned long blk_switch_last_appstr = 0;
+int blk_switch_appstr_app = 0;
+
+/* stats for req-steering */
+int blk_switch_stats_print[BLK_SWITCH_NR_CPUS] = {0};
+int blk_switch_stats_gen[BLK_SWITCH_NR_CPUS] = {0};
+int blk_switch_stats_str[BLK_SWITCH_NR_CPUS] = {0};
+int blk_switch_stats_prc[BLK_SWITCH_NR_CPUS] = {0};
+
+enum blk_switch_ioprio {
+	BLK_SWITCH_T_APP = 0,
+	BLK_SWITCH_L_APP,
+};
+
+enum blk_swtich_fabric {
+	BLK_SWITCH_NONE = 0,
+	BLK_SWITCH_TCP,
+	BLK_SWITCH_RDMA,
+};
+
+struct nvme_tcp_queue {
+	struct socket           *sock;
+	struct work_struct      io_work;
+	int                     io_cpu;
+
+	struct work_struct      io_work_lat;
+	int                     prio_class;
+
+	spinlock_t              lock;
+	struct list_head        send_list;
+
+	/* recv state */
+	void                    *pdu;
+	int                     pdu_remaining;
+	int                     pdu_offset;
+	size_t                  data_remaining;
+	size_t                  ddgst_remaining;
+	unsigned int            nr_cqe;
+
+	/* send state */
+	struct nvme_tcp_request *request;
+
+	int                     queue_size;
+	size_t                  cmnd_capsule_len;
+	struct nvme_tcp_ctrl    *ctrl;
+	unsigned long           flags;
+	bool                    rd_enabled;
+
+	bool                    hdr_digest;
+	bool                    data_digest;
+	struct ahash_request    *rcv_hash;
+	struct ahash_request    *snd_hash;
+	__le32                  exp_ddgst;
+	__le32                  recv_ddgst;
+
+	/* jaehyun: For i10 caravans */
+	struct kvec             *caravan_iovs;
+	size_t                  caravan_len;
+	int                     nr_iovs;
+	bool                    send_now;
+	int                     nr_caravan_req;
+
+	struct page		**caravan_mapped;
+	int			nr_mapped;
+
+	/* jaehyun: For i10 delayed doorbells */
+	atomic_t                nr_req;
+	struct hrtimer          doorbell_timer;
+	atomic_t                timer_set;
+
+	struct page_frag_cache  pf_cache;
+
+	void (*state_change)(struct sock *);
+	void (*data_ready)(struct sock *);
+	void (*write_space)(struct sock *);
+};
+
+
+static void blk_switch_set_ioprio(struct task_struct *p, struct bio *bio)
+{
+	int ioprio;
+
+	ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);
+	task_lock(p);
+	if (p->io_context)
+		ioprio = p->io_context->ioprio;
+	task_unlock(p);
+
+	if (IOPRIO_PRIO_CLASS(ioprio) == BLK_SWITCH_L_APP) {
+		bio_set_prio(bio, ioprio);
+		bio->bi_opf |= REQ_PRIO;
+	}
+}
+
+static inline bool blk_switch_request(struct bio *bio,
+				struct blk_mq_alloc_data *data)
+{
+	return data->hctx->blk_switch && bio &&
+		bio->bi_iter.bi_size > 0;
+}
+
+static inline bool blk_switch_is_thru_request(struct bio *bio)
+{
+	return bio && IOPRIO_PRIO_CLASS(bio_prio(bio)) != BLK_SWITCH_L_APP;
+}
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index c4eed5b..e0c8ec7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -1276,6 +1276,10 @@ mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *w
 	skb->tail += headlen;
 	skb->len  += headlen;
 
+	/* For NetChannel */
+	if(rq->page_pool)
+		skb_shinfo(skb)->page_pool = rq->page_pool;
+
 	return skb;
 }
 
diff --git a/drivers/nvme/host/Kconfig b/drivers/nvme/host/Kconfig
index 2b36f05..849b92a 100644
--- a/drivers/nvme/host/Kconfig
+++ b/drivers/nvme/host/Kconfig
@@ -74,3 +74,15 @@ config NVME_TCP
 	  from https://github.com/linux-nvme/nvme-cli.
 
 	  If unsure, say N.
+
+config I10_HOST
+	tristate "i10: A New Remote Storage I/O Stack (host)"
+	depends on INET
+	depends on BLK_DEV_NVME
+	select NVME_FABRICS
+	help
+	  This provides i10 remote storage I/O stack (host). i10 is currently
+	  implemented as a new fabric of the NVMe over Fabrics protocol.
+	  (https://github.com/i10-kernel/i10-implementation)
+
+	  If unsure, say N.
diff --git a/drivers/nvme/host/Makefile b/drivers/nvme/host/Makefile
index 8a4b671..56f41ad 100644
--- a/drivers/nvme/host/Makefile
+++ b/drivers/nvme/host/Makefile
@@ -8,6 +8,7 @@ obj-$(CONFIG_NVME_FABRICS)		+= nvme-fabrics.o
 obj-$(CONFIG_NVME_RDMA)			+= nvme-rdma.o
 obj-$(CONFIG_NVME_FC)			+= nvme-fc.o
 obj-$(CONFIG_NVME_TCP)			+= nvme-tcp.o
+obj-$(CONFIG_I10_HOST)			+= i10-host.o
 
 nvme-core-y				:= core.o
 nvme-core-$(CONFIG_TRACING)		+= trace.o
@@ -24,3 +25,5 @@ nvme-rdma-y				+= rdma.o
 nvme-fc-y				+= fc.o
 
 nvme-tcp-y				+= tcp.o
+
+i10-host-y				+= i10.o
diff --git a/drivers/nvme/host/i10.c b/drivers/nvme/host/i10.c
new file mode 100644
index 0000000..1ec8965
--- /dev/null
+++ b/drivers/nvme/host/i10.c
@@ -0,0 +1,2669 @@
+/*
+ *	TCP~~RDMA: CPU-efficient Remote Storage Access
+ *			with i10
+ *		- i10 host implementation
+ *		(inspired by drivers/nvme/host/tcp.c)
+ *
+ *	Authors:
+ *		Jaehyun Hwang <jaehyun.hwang@cornell.edu>
+ *		Qizhe Cai <qc228@cornell.edu>
+ *		A. Kevin Tang <atang@cornell.edu>
+ *		Rachit Agarwal <ragarwal@cs.cornell.edu>
+ *
+ *	SPDX-License-Identifier: GPL-2.0
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/nvme-tcp.h>
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <linux/blk-mq.h>
+#include <crypto/hash.h>
+#include <net/busy_poll.h>
+
+#include <linux/bio.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+
+#include "nvme.h"
+#include "fabrics.h"
+
+#define I10_CARAVAN_CAPACITY		65536
+#define I10_AGGREGATION_SIZE		16
+#define I10_MIN_DOORBELL_TIMEOUT	25
+
+static int i10_delayed_doorbell_us __read_mostly = 50;
+module_param(i10_delayed_doorbell_us, int, 0644);
+MODULE_PARM_DESC(i10_delayed_doorbell_us,
+		"i10 delayed doorbell timer (us)");
+
+struct i10_host_queue;
+
+enum i10_host_send_state {
+	I10_HOST_SEND_CMD_PDU = 0,
+	I10_HOST_SEND_H2C_PDU,
+	I10_HOST_SEND_DATA,
+	I10_TCP_SEND_DDGST,
+};
+
+struct i10_host_request {
+	struct nvme_request	req;
+	void			*pdu;
+	struct i10_host_queue	*queue;
+	u32			data_len;
+	u32			pdu_len;
+	u32			pdu_sent;
+	u16			ttag;
+	struct list_head	entry;
+	__le32			ddgst;
+
+	struct bio		*curr_bio;
+	struct iov_iter		iter;
+
+	/* send state */
+	size_t			offset;
+	size_t			data_sent;
+	enum i10_host_send_state state;
+};
+
+enum i10_host_queue_flags {
+	I10_HOST_Q_ALLOCATED	= 0,
+	I10_HOST_Q_LIVE		= 1,
+};
+
+enum i10_host_recv_state {
+	I10_HOST_RECV_PDU = 0,
+	I10_HOST_RECV_DATA,
+	I10_HOST_RECV_DDGST,
+};
+
+struct i10_host_ctrl;
+struct i10_host_queue {
+	struct socket		*sock;
+	struct work_struct	io_work;
+	int			io_cpu;
+
+	spinlock_t		lock;
+	struct list_head	send_list;
+
+	/* recv state */
+	void			*pdu;
+	int			pdu_remaining;
+	int			pdu_offset;
+	size_t			data_remaining;
+	size_t			ddgst_remaining;
+	unsigned int		nr_cqe;
+
+	/* send state */
+	struct i10_host_request *request;
+
+	int			queue_size;
+	size_t			cmnd_capsule_len;
+	struct i10_host_ctrl	*ctrl;
+	unsigned long		flags;
+	bool			rd_enabled;
+
+	bool			hdr_digest;
+	bool			data_digest;
+	struct ahash_request	*rcv_hash;
+	struct ahash_request	*snd_hash;
+	__le32			exp_ddgst;
+	__le32			recv_ddgst;
+
+	/* For i10 caravans */
+	struct kvec		*caravan_iovs;
+	size_t			caravan_len;
+	int			nr_iovs;
+	bool			send_now;
+
+	struct page		**caravan_mapped;
+	int			nr_mapped;
+
+	/* For i10 delayed doorbells */
+	int			nr_req;
+	struct hrtimer		doorbell_timer;
+
+	struct page_frag_cache	pf_cache;
+
+	void (*state_change)(struct sock *);
+	void (*data_ready)(struct sock *);
+	void (*write_space)(struct sock *);
+};
+
+struct i10_host_ctrl {
+	/* read only in the hot path */
+	struct i10_host_queue	*queues;
+	struct blk_mq_tag_set	tag_set;
+
+	/* other member variables */
+	struct list_head	list;
+	struct blk_mq_tag_set	admin_tag_set;
+	struct sockaddr_storage addr;
+	struct sockaddr_storage src_addr;
+	struct nvme_ctrl	ctrl;
+
+	struct work_struct	err_work;
+	struct delayed_work	connect_work;
+	struct i10_host_request async_req;
+	u32			io_queues[HCTX_MAX_TYPES];
+};
+
+static LIST_HEAD(i10_host_ctrl_list);
+static DEFINE_MUTEX(i10_host_ctrl_mutex);
+static struct workqueue_struct *i10_host_wq;
+static struct blk_mq_ops i10_host_mq_ops;
+static struct blk_mq_ops i10_host_admin_mq_ops;
+
+static inline struct i10_host_ctrl *to_i10_host_ctrl(struct nvme_ctrl *ctrl)
+{
+	return container_of(ctrl, struct i10_host_ctrl, ctrl);
+}
+
+static inline int i10_host_queue_id(struct i10_host_queue *queue)
+{
+	return queue - queue->ctrl->queues;
+}
+
+static inline struct blk_mq_tags *i10_host_tagset(struct i10_host_queue *queue)
+{
+	u32 queue_idx = i10_host_queue_id(queue);
+
+	if (queue_idx == 0)
+		return queue->ctrl->admin_tag_set.tags[queue_idx];
+	return queue->ctrl->tag_set.tags[queue_idx - 1];
+}
+
+static inline u8 i10_host_hdgst_len(struct i10_host_queue *queue)
+{
+	return queue->hdr_digest ? NVME_TCP_DIGEST_LENGTH : 0;
+}
+
+static inline u8 i10_host_ddgst_len(struct i10_host_queue *queue)
+{
+	return queue->data_digest ? NVME_TCP_DIGEST_LENGTH : 0;
+}
+
+static inline size_t i10_host_inline_data_size(struct i10_host_queue *queue)
+{
+	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
+}
+
+static inline bool i10_host_async_req(struct i10_host_request *req)
+{
+	return req == &req->queue->ctrl->async_req;
+}
+
+static inline bool i10_host_has_inline_data(struct i10_host_request *req)
+{
+	struct request *rq;
+
+	if (unlikely(i10_host_async_req(req)))
+		return false; /* async events don't have a request */
+
+	rq = blk_mq_rq_from_pdu(req);
+
+	return rq_data_dir(rq) == WRITE && req->data_len &&
+		req->data_len <= i10_host_inline_data_size(req->queue);
+}
+
+static inline struct page *i10_host_req_cur_page(struct i10_host_request *req)
+{
+	return req->iter.bvec->bv_page;
+}
+
+static inline size_t i10_host_req_cur_offset(struct i10_host_request *req)
+{
+	return req->iter.bvec->bv_offset + req->iter.iov_offset;
+}
+
+static inline size_t i10_host_req_cur_length(struct i10_host_request *req)
+{
+	return min_t(size_t, req->iter.bvec->bv_len - req->iter.iov_offset,
+			req->pdu_len - req->pdu_sent);
+}
+
+static inline size_t i10_host_req_offset(struct i10_host_request *req)
+{
+	return req->iter.iov_offset;
+}
+
+static inline size_t i10_host_pdu_data_left(struct i10_host_request *req)
+{
+	return rq_data_dir(blk_mq_rq_from_pdu(req)) == WRITE ?
+			req->pdu_len - req->pdu_sent : 0;
+}
+
+static inline size_t i10_host_pdu_last_send(struct i10_host_request *req,
+		int len)
+{
+	return i10_host_pdu_data_left(req) <= len;
+}
+
+static void i10_host_init_iter(struct i10_host_request *req,
+		unsigned int dir)
+{
+	struct request *rq = blk_mq_rq_from_pdu(req);
+	struct bio_vec *vec;
+	unsigned int size;
+	int nsegs;
+	size_t offset;
+
+	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD) {
+		vec = &rq->special_vec;
+		nsegs = 1;
+		size = blk_rq_payload_bytes(rq);
+		offset = 0;
+	} else {
+		struct bio *bio = req->curr_bio;
+
+		vec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);
+		nsegs = bio_segments(bio);
+		size = bio->bi_iter.bi_size;
+		offset = bio->bi_iter.bi_bvec_done;
+	}
+
+	iov_iter_bvec(&req->iter, dir, vec, nsegs, size);
+	req->iter.iov_offset = offset;
+}
+
+static inline void i10_host_advance_req(struct i10_host_request *req,
+		int len)
+{
+	req->data_sent += len;
+	req->pdu_sent += len;
+	iov_iter_advance(&req->iter, len);
+	if (!iov_iter_count(&req->iter) &&
+	    req->data_sent < req->data_len) {
+		req->curr_bio = req->curr_bio->bi_next;
+		i10_host_init_iter(req, WRITE);
+	}
+}
+
+static inline bool i10_host_legacy_path(struct i10_host_request *req)
+{
+	return (i10_host_queue_id(req->queue) == 0) ||
+		(i10_delayed_doorbell_us < I10_MIN_DOORBELL_TIMEOUT);
+}
+
+#define I10_ALLOWED_FLAGS (REQ_OP_READ | REQ_OP_WRITE | REQ_DRV | \
+		REQ_RAHEAD | REQ_SYNC | REQ_IDLE | REQ_NOMERGE)
+
+static bool i10_host_is_nodelay_path(struct i10_host_request *req)
+{
+	return (req->curr_bio == NULL) ||
+		(req->curr_bio->bi_opf & ~I10_ALLOWED_FLAGS);
+}
+
+static void i10_host_queue_request(struct i10_host_request *req)
+{
+	struct i10_host_queue *queue = req->queue;
+
+	spin_lock(&queue->lock);
+	list_add_tail(&req->entry, &queue->send_list);
+	spin_unlock(&queue->lock);
+
+	if (!i10_host_legacy_path(req) && !i10_host_is_nodelay_path(req)) {
+		queue->nr_req++;
+
+		/* Start a delayed doorbell timer */
+		if (!hrtimer_active(&queue->doorbell_timer) &&
+			queue->nr_req == 1)
+			hrtimer_start(&queue->doorbell_timer,
+				ns_to_ktime(i10_delayed_doorbell_us *
+					NSEC_PER_USEC),
+				HRTIMER_MODE_REL);
+		/* Ring the delayed doorbell
+		 * if I/O request counter >= i10 aggregation size
+		 */
+		else if (queue->nr_req >= I10_AGGREGATION_SIZE) {
+			if (hrtimer_active(&queue->doorbell_timer))
+				hrtimer_cancel(&queue->doorbell_timer);
+			queue_work_on(queue->io_cpu, i10_host_wq,
+					&queue->io_work);
+		}
+	}
+	/* Ring the doorbell immediately for no-delay path */
+	else {
+		if (hrtimer_active(&queue->doorbell_timer))
+			hrtimer_cancel(&queue->doorbell_timer);
+		queue_work_on(queue->io_cpu, i10_host_wq, &queue->io_work);
+	}
+}
+
+static inline struct i10_host_request *
+i10_host_fetch_request(struct i10_host_queue *queue)
+{
+	struct i10_host_request *req;
+
+	spin_lock(&queue->lock);
+	req = list_first_entry_or_null(&queue->send_list,
+			struct i10_host_request, entry);
+	if (req)
+		list_del(&req->entry);
+	spin_unlock(&queue->lock);
+
+	return req;
+}
+
+static inline void i10_host_ddgst_final(struct ahash_request *hash,
+		__le32 *dgst)
+{
+	ahash_request_set_crypt(hash, NULL, (u8 *)dgst, 0);
+	crypto_ahash_final(hash);
+}
+
+static inline void i10_host_ddgst_update(struct ahash_request *hash,
+		struct page *page, off_t off, size_t len)
+{
+	struct scatterlist sg;
+
+	sg_init_marker(&sg, 1);
+	sg_set_page(&sg, page, len, off);
+	ahash_request_set_crypt(hash, &sg, NULL, len);
+	crypto_ahash_update(hash);
+}
+
+static inline void i10_host_hdgst(struct ahash_request *hash,
+		void *pdu, size_t len)
+{
+	struct scatterlist sg;
+
+	sg_init_one(&sg, pdu, len);
+	ahash_request_set_crypt(hash, &sg, pdu + len, len);
+	crypto_ahash_digest(hash);
+}
+
+static int i10_host_verify_hdgst(struct i10_host_queue *queue,
+		void *pdu, size_t pdu_len)
+{
+	struct nvme_tcp_hdr *hdr = pdu;
+	__le32 recv_digest;
+	__le32 exp_digest;
+
+	if (unlikely(!(hdr->flags & NVME_TCP_F_HDGST))) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d: header digest flag is cleared\n",
+			i10_host_queue_id(queue));
+		return -EPROTO;
+	}
+
+	recv_digest = *(__le32 *)(pdu + hdr->hlen);
+	i10_host_hdgst(queue->rcv_hash, pdu, pdu_len);
+	exp_digest = *(__le32 *)(pdu + hdr->hlen);
+	if (recv_digest != exp_digest) {
+		dev_err(queue->ctrl->ctrl.device,
+			"header digest error: recv %#x expected %#x\n",
+			le32_to_cpu(recv_digest), le32_to_cpu(exp_digest));
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int i10_host_check_ddgst(struct i10_host_queue *queue, void *pdu)
+{
+	struct nvme_tcp_hdr *hdr = pdu;
+	u8 digest_len = i10_host_hdgst_len(queue);
+	u32 len;
+
+	len = le32_to_cpu(hdr->plen) - hdr->hlen -
+		((hdr->flags & NVME_TCP_F_HDGST) ? digest_len : 0);
+
+	if (unlikely(len && !(hdr->flags & NVME_TCP_F_DDGST))) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d: data digest flag is cleared\n",
+		i10_host_queue_id(queue));
+		return -EPROTO;
+	}
+	crypto_ahash_init(queue->rcv_hash);
+
+	return 0;
+}
+
+static void i10_host_exit_request(struct blk_mq_tag_set *set,
+		struct request *rq, unsigned int hctx_idx)
+{
+	struct i10_host_request *req = blk_mq_rq_to_pdu(rq);
+
+	page_frag_free(req->pdu);
+}
+
+static int i10_host_init_request(struct blk_mq_tag_set *set,
+		struct request *rq, unsigned int hctx_idx,
+		unsigned int numa_node)
+{
+	struct i10_host_ctrl *ctrl = set->driver_data;
+	struct i10_host_request *req = blk_mq_rq_to_pdu(rq);
+	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+	struct i10_host_queue *queue = &ctrl->queues[queue_idx];
+	u8 hdgst = i10_host_hdgst_len(queue);
+
+	req->pdu = page_frag_alloc(&queue->pf_cache,
+		sizeof(struct nvme_tcp_cmd_pdu) + hdgst,
+		GFP_KERNEL | __GFP_ZERO);
+	if (!req->pdu)
+		return -ENOMEM;
+
+	req->queue = queue;
+	nvme_req(rq)->ctrl = &ctrl->ctrl;
+
+	return 0;
+}
+
+static int i10_host_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct i10_host_ctrl *ctrl = data;
+	struct i10_host_queue *queue = &ctrl->queues[hctx_idx + 1];
+
+	hctx->driver_data = queue;
+	return 0;
+}
+
+static int i10_host_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct i10_host_ctrl *ctrl = data;
+	struct i10_host_queue *queue = &ctrl->queues[0];
+
+	hctx->driver_data = queue;
+	return 0;
+}
+
+static enum i10_host_recv_state
+i10_host_recv_state(struct i10_host_queue *queue)
+{
+	return  (queue->pdu_remaining) ? I10_HOST_RECV_PDU :
+		(queue->ddgst_remaining) ? I10_HOST_RECV_DDGST :
+		I10_HOST_RECV_DATA;
+}
+
+static void i10_host_init_recv_ctx(struct i10_host_queue *queue)
+{
+	queue->pdu_remaining = sizeof(struct nvme_tcp_rsp_pdu) +
+				i10_host_hdgst_len(queue);
+	queue->pdu_offset = 0;
+	queue->data_remaining = -1;
+	queue->ddgst_remaining = 0;
+}
+
+static void i10_host_error_recovery(struct nvme_ctrl *ctrl)
+{
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
+		return;
+
+	queue_work(nvme_reset_wq, &to_i10_host_ctrl(ctrl)->err_work);
+}
+
+static int i10_host_process_nvme_cqe(struct i10_host_queue *queue,
+		struct nvme_completion *cqe)
+{
+	struct request *rq;
+
+	rq = blk_mq_tag_to_rq(i10_host_tagset(queue), cqe->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag 0x%x not found\n",
+			i10_host_queue_id(queue), cqe->command_id);
+		i10_host_error_recovery(&queue->ctrl->ctrl);
+		return -EINVAL;
+	}
+
+	nvme_end_request(rq, cqe->status, cqe->result);
+	queue->nr_cqe++;
+
+	return 0;
+}
+
+static int i10_host_handle_c2h_data(struct i10_host_queue *queue,
+		struct nvme_tcp_data_pdu *pdu)
+{
+	struct request *rq;
+
+	rq = blk_mq_tag_to_rq(i10_host_tagset(queue), pdu->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x not found\n",
+			i10_host_queue_id(queue), pdu->command_id);
+		return -ENOENT;
+	}
+
+	if (!blk_rq_payload_bytes(rq)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x unexpected data\n",
+			i10_host_queue_id(queue), rq->tag);
+		return -EIO;
+	}
+
+	queue->data_remaining = le32_to_cpu(pdu->data_length);
+
+	if (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS &&
+	    unlikely(!(pdu->hdr.flags & NVME_TCP_F_DATA_LAST))) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x SUCCESS set but not last PDU\n",
+			i10_host_queue_id(queue), rq->tag);
+		i10_host_error_recovery(&queue->ctrl->ctrl);
+		return -EPROTO;
+	}
+
+	return 0;
+}
+
+static int i10_host_handle_comp(struct i10_host_queue *queue,
+		struct nvme_tcp_rsp_pdu *pdu)
+{
+	struct nvme_completion *cqe = &pdu->cqe;
+	int ret = 0;
+
+	/*
+	 * AEN requests are special as they don't time out and can
+	 * survive any kind of queue freeze and often don't respond to
+	 * aborts.  We don't even bother to allocate a struct request
+	 * for them but rather special case them here.
+	 */
+	if (unlikely(i10_host_queue_id(queue) == 0 &&
+	    cqe->command_id >= NVME_AQ_BLK_MQ_DEPTH))
+		nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+				&cqe->result);
+	else
+		ret = i10_host_process_nvme_cqe(queue, cqe);
+
+	return ret;
+}
+
+static int i10_host_setup_h2c_data_pdu(struct i10_host_request *req,
+		struct nvme_tcp_r2t_pdu *pdu)
+{
+	struct nvme_tcp_data_pdu *data = req->pdu;
+	struct i10_host_queue *queue = req->queue;
+	struct request *rq = blk_mq_rq_from_pdu(req);
+	u8 hdgst = i10_host_hdgst_len(queue);
+	u8 ddgst = i10_host_ddgst_len(queue);
+
+	req->pdu_len = le32_to_cpu(pdu->r2t_length);
+	req->pdu_sent = 0;
+
+	if (unlikely(req->data_sent + req->pdu_len > req->data_len)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"req %d r2t len %u exceeded data len %u (%zu sent)\n",
+			rq->tag, req->pdu_len, req->data_len,
+			req->data_sent);
+		return -EPROTO;
+	}
+
+	if (unlikely(le32_to_cpu(pdu->r2t_offset) < req->data_sent)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"req %d unexpected r2t offset %u (expected %zu)\n",
+			rq->tag, le32_to_cpu(pdu->r2t_offset),
+			req->data_sent);
+		return -EPROTO;
+	}
+
+	memset(data, 0, sizeof(*data));
+	data->hdr.type = nvme_tcp_h2c_data;
+	data->hdr.flags = NVME_TCP_F_DATA_LAST;
+	if (queue->hdr_digest)
+		data->hdr.flags |= NVME_TCP_F_HDGST;
+	if (queue->data_digest)
+		data->hdr.flags |= NVME_TCP_F_DDGST;
+	data->hdr.hlen = sizeof(*data);
+	data->hdr.pdo = data->hdr.hlen + hdgst;
+	data->hdr.plen =
+		cpu_to_le32(data->hdr.hlen + hdgst + req->pdu_len + ddgst);
+	data->ttag = pdu->ttag;
+	data->command_id = rq->tag;
+	data->data_offset = cpu_to_le32(req->data_sent);
+	data->data_length = cpu_to_le32(req->pdu_len);
+	return 0;
+}
+
+static int i10_host_handle_r2t(struct i10_host_queue *queue,
+		struct nvme_tcp_r2t_pdu *pdu)
+{
+	struct i10_host_request *req;
+	struct request *rq;
+	int ret;
+
+	rq = blk_mq_tag_to_rq(i10_host_tagset(queue), pdu->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x not found\n",
+			i10_host_queue_id(queue), pdu->command_id);
+		return -ENOENT;
+	}
+	req = blk_mq_rq_to_pdu(rq);
+
+	ret = i10_host_setup_h2c_data_pdu(req, pdu);
+	if (unlikely(ret))
+		return ret;
+
+	req->state = I10_HOST_SEND_H2C_PDU;
+	req->offset = 0;
+
+	i10_host_queue_request(req);
+
+	return 0;
+}
+
+static int i10_host_recv_pdu(struct i10_host_queue *queue, struct sk_buff *skb,
+		unsigned int *offset, size_t *len)
+{
+	struct nvme_tcp_hdr *hdr;
+	char *pdu = queue->pdu;
+	size_t rcv_len = min_t(size_t, *len, queue->pdu_remaining);
+	int ret;
+
+	ret = skb_copy_bits(skb, *offset,
+		&pdu[queue->pdu_offset], rcv_len);
+	if (unlikely(ret))
+		return ret;
+
+	queue->pdu_remaining -= rcv_len;
+	queue->pdu_offset += rcv_len;
+	*offset += rcv_len;
+	*len -= rcv_len;
+	if (queue->pdu_remaining)
+		return 0;
+
+	hdr = queue->pdu;
+	if (queue->hdr_digest) {
+		ret = i10_host_verify_hdgst(queue, queue->pdu, hdr->hlen);
+		if (unlikely(ret))
+			return ret;
+	}
+
+
+	if (queue->data_digest) {
+		ret = i10_host_check_ddgst(queue, queue->pdu);
+		if (unlikely(ret))
+			return ret;
+	}
+
+	switch (hdr->type) {
+	case nvme_tcp_c2h_data:
+		return i10_host_handle_c2h_data(queue, (void *)queue->pdu);
+	case nvme_tcp_rsp:
+		i10_host_init_recv_ctx(queue);
+		return i10_host_handle_comp(queue, (void *)queue->pdu);
+	case nvme_tcp_r2t:
+		i10_host_init_recv_ctx(queue);
+		return i10_host_handle_r2t(queue, (void *)queue->pdu);
+	default:
+		dev_err(queue->ctrl->ctrl.device,
+			"unsupported pdu type (%d)\n", hdr->type);
+		return -EINVAL;
+	}
+}
+
+static inline void i10_host_end_request(struct request *rq, u16 status)
+{
+	union nvme_result res = {};
+
+	nvme_end_request(rq, cpu_to_le16(status << 1), res);
+}
+
+static int i10_host_recv_data(struct i10_host_queue *queue, struct sk_buff *skb,
+			      unsigned int *offset, size_t *len)
+{
+	struct nvme_tcp_data_pdu *pdu = (void *)queue->pdu;
+	struct i10_host_request *req;
+	struct request *rq;
+
+	rq = blk_mq_tag_to_rq(i10_host_tagset(queue), pdu->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x not found\n",
+			i10_host_queue_id(queue), pdu->command_id);
+		return -ENOENT;
+	}
+	req = blk_mq_rq_to_pdu(rq);
+
+	while (true) {
+		int recv_len, ret;
+
+		recv_len = min_t(size_t, *len, queue->data_remaining);
+		if (!recv_len)
+			break;
+
+		if (!iov_iter_count(&req->iter)) {
+			req->curr_bio = req->curr_bio->bi_next;
+
+			/*
+			 * If we don`t have any bios it means that controller
+			 * sent more data than we requested, hence error
+			 */
+			if (!req->curr_bio) {
+				dev_err(queue->ctrl->ctrl.device,
+					"queue %d no space in request %#x",
+					i10_host_queue_id(queue), rq->tag);
+				i10_host_init_recv_ctx(queue);
+				return -EIO;
+			}
+			i10_host_init_iter(req, READ);
+		}
+
+		/* we can read only from what is left in this bio */
+		recv_len = min_t(size_t, recv_len,
+				iov_iter_count(&req->iter));
+
+		if (queue->data_digest)
+			ret = skb_copy_and_hash_datagram_iter(skb, *offset,
+				&req->iter, recv_len, queue->rcv_hash);
+		else
+			ret = skb_copy_datagram_iter(skb, *offset,
+					&req->iter, recv_len);
+		if (ret) {
+			dev_err(queue->ctrl->ctrl.device,
+				"queue %d failed to copy request %#x data",
+				i10_host_queue_id(queue), rq->tag);
+			return ret;
+		}
+
+		*len -= recv_len;
+		*offset += recv_len;
+		queue->data_remaining -= recv_len;
+	}
+
+	if (!queue->data_remaining) {
+		if (queue->data_digest) {
+			i10_host_ddgst_final(queue->rcv_hash, &queue->exp_ddgst);
+			queue->ddgst_remaining = NVME_TCP_DIGEST_LENGTH;
+		} else {
+			if (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS) {
+				i10_host_end_request(rq, NVME_SC_SUCCESS);
+				queue->nr_cqe++;
+			}
+			i10_host_init_recv_ctx(queue);
+		}
+	}
+
+	return 0;
+}
+
+static int i10_host_recv_ddgst(struct i10_host_queue *queue,
+		struct sk_buff *skb, unsigned int *offset, size_t *len)
+{
+	struct nvme_tcp_data_pdu *pdu = (void *)queue->pdu;
+	char *ddgst = (char *)&queue->recv_ddgst;
+	size_t recv_len = min_t(size_t, *len, queue->ddgst_remaining);
+	off_t off = NVME_TCP_DIGEST_LENGTH - queue->ddgst_remaining;
+	int ret;
+
+	ret = skb_copy_bits(skb, *offset, &ddgst[off], recv_len);
+	if (unlikely(ret))
+		return ret;
+
+	queue->ddgst_remaining -= recv_len;
+	*offset += recv_len;
+	*len -= recv_len;
+	if (queue->ddgst_remaining)
+		return 0;
+
+	if (queue->recv_ddgst != queue->exp_ddgst) {
+		dev_err(queue->ctrl->ctrl.device,
+			"data digest error: recv %#x expected %#x\n",
+			le32_to_cpu(queue->recv_ddgst),
+			le32_to_cpu(queue->exp_ddgst));
+		return -EIO;
+	}
+
+	if (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS) {
+		struct request *rq = blk_mq_tag_to_rq(i10_host_tagset(queue),
+						pdu->command_id);
+
+		i10_host_end_request(rq, NVME_SC_SUCCESS);
+		queue->nr_cqe++;
+	}
+
+	i10_host_init_recv_ctx(queue);
+	return 0;
+}
+
+static int i10_host_recv_skb(read_descriptor_t *desc, struct sk_buff *skb,
+			     unsigned int offset, size_t len)
+{
+	struct i10_host_queue *queue = desc->arg.data;
+	size_t consumed = len;
+	int result;
+
+	while (len) {
+		switch (i10_host_recv_state(queue)) {
+		case I10_HOST_RECV_PDU:
+			result = i10_host_recv_pdu(queue, skb, &offset, &len);
+			break;
+		case I10_HOST_RECV_DATA:
+			result = i10_host_recv_data(queue, skb, &offset, &len);
+			break;
+		case I10_HOST_RECV_DDGST:
+			result = i10_host_recv_ddgst(queue, skb, &offset, &len);
+			break;
+		default:
+			result = -EFAULT;
+		}
+		if (result) {
+			dev_err(queue->ctrl->ctrl.device,
+				"receive failed:  %d\n", result);
+			queue->rd_enabled = false;
+			i10_host_error_recovery(&queue->ctrl->ctrl);
+			return result;
+		}
+	}
+
+	return consumed;
+}
+
+static void i10_host_data_ready(struct sock *sk)
+{
+	struct i10_host_queue *queue;
+
+	read_lock(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (likely(queue && queue->rd_enabled))
+			queue_work_on(queue->io_cpu, i10_host_wq, &queue->io_work);
+	read_unlock(&sk->sk_callback_lock);
+}
+
+static void i10_host_write_space(struct sock *sk)
+{
+	struct i10_host_queue *queue;
+
+	read_lock_bh(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (likely(queue && sk_stream_is_writeable(sk))) {
+		clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		queue_work_on(queue->io_cpu, i10_host_wq, &queue->io_work);
+	}
+	read_unlock_bh(&sk->sk_callback_lock);
+}
+
+static void i10_host_state_change(struct sock *sk)
+{
+	struct i10_host_queue *queue;
+
+	read_lock(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (!queue)
+		goto done;
+
+	switch (sk->sk_state) {
+	case TCP_CLOSE:
+	case TCP_CLOSE_WAIT:
+	case TCP_LAST_ACK:
+	case TCP_FIN_WAIT1:
+	case TCP_FIN_WAIT2:
+		/* fallthrough */
+		i10_host_error_recovery(&queue->ctrl->ctrl);
+		break;
+	default:
+		dev_info(queue->ctrl->ctrl.device,
+			"queue %d socket state %d\n",
+			i10_host_queue_id(queue), sk->sk_state);
+	}
+
+	queue->state_change(sk);
+done:
+	read_unlock(&sk->sk_callback_lock);
+}
+
+static inline void i10_host_done_send_req(struct i10_host_queue *queue)
+{
+	queue->request = NULL;
+}
+
+static void i10_host_fail_request(struct i10_host_request *req)
+{
+	i10_host_end_request(blk_mq_rq_from_pdu(req), NVME_SC_HOST_PATH_ERROR);
+}
+
+static inline bool i10_host_is_caravan_full(struct i10_host_queue *queue)
+{
+	return (queue->caravan_len >= I10_CARAVAN_CAPACITY) ||
+		(queue->nr_iovs >= I10_AGGREGATION_SIZE * 2) ||
+		(queue->nr_mapped >= I10_AGGREGATION_SIZE);
+}
+
+static int i10_host_try_send_data(struct i10_host_request *req)
+{
+	struct i10_host_queue *queue = req->queue;
+
+	while (true) {
+		struct page *page = i10_host_req_cur_page(req);
+		size_t offset = i10_host_req_cur_offset(req);
+		size_t len = i10_host_req_cur_length(req);
+		bool last = i10_host_pdu_last_send(req, len);
+		int ret, flags = MSG_DONTWAIT;
+
+		if (last && !queue->data_digest)
+			flags |= MSG_EOR;
+		else
+			flags |= MSG_MORE;
+
+		if (i10_host_legacy_path(req)) {
+			/* can't zcopy slab pages */
+			if (unlikely(PageSlab(page))) {
+				ret = sock_no_sendpage(queue->sock, page, offset, len,
+						flags);
+			} else {
+				ret = kernel_sendpage(queue->sock, page, offset, len,
+						flags);
+			}
+		}
+		else {
+			if (i10_host_is_caravan_full(queue)) {
+				queue->send_now = true;
+				return 1;
+			}
+			/* Caravans: I/O data aggregation */
+			queue->caravan_iovs[queue->nr_iovs].iov_base =
+				kmap(page) + offset;
+			queue->caravan_iovs[queue->nr_iovs++].iov_len = len;
+			queue->caravan_mapped[queue->nr_mapped++] = page;
+			queue->caravan_len += len;
+			ret = len;
+		}
+
+		if (ret <= 0)
+			return ret;
+
+		i10_host_advance_req(req, ret);
+		if (queue->data_digest)
+			i10_host_ddgst_update(queue->snd_hash, page,
+					offset, ret);
+
+		/* fully successful last write*/
+		if (last && ret == len) {
+			if (queue->data_digest) {
+				i10_host_ddgst_final(queue->snd_hash,
+					&req->ddgst);
+				req->state = I10_TCP_SEND_DDGST;
+				req->offset = 0;
+			} else {
+				i10_host_done_send_req(queue);
+			}
+			return 1;
+		}
+	}
+	return -EAGAIN;
+}
+
+static int i10_host_try_send_cmd_pdu(struct i10_host_request *req)
+{
+	struct i10_host_queue *queue = req->queue;
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+	bool inline_data = i10_host_has_inline_data(req);
+	int flags = MSG_DONTWAIT | (inline_data ? MSG_MORE : MSG_EOR);
+	u8 hdgst = i10_host_hdgst_len(queue);
+	int len = sizeof(*pdu) + hdgst - req->offset;
+	int ret;
+
+	if (queue->hdr_digest && !req->offset)
+		i10_host_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+
+	if (i10_host_legacy_path(req))
+		ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
+			offset_in_page(pdu) + req->offset, len,  flags);
+	else {
+		if (i10_host_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		/* Caravans: command PDU aggregation */
+		queue->caravan_iovs[queue->nr_iovs].iov_base = pdu
+			+ req->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = len;
+		queue->caravan_len += len;
+		ret = len;
+
+		if (i10_host_is_nodelay_path(req))
+			queue->send_now = true;
+	}
+
+	if (unlikely(ret <= 0))
+		return ret;
+
+	len -= ret;
+	if (!len) {
+		if (inline_data) {
+			req->state = I10_HOST_SEND_DATA;
+			if (queue->data_digest)
+				crypto_ahash_init(queue->snd_hash);
+			i10_host_init_iter(req, WRITE);
+		} else {
+			i10_host_done_send_req(queue);
+		}
+		return 1;
+	}
+	req->offset += ret;
+
+	return -EAGAIN;
+}
+
+static int i10_host_try_send_data_pdu(struct i10_host_request *req)
+{
+	struct i10_host_queue *queue = req->queue;
+	struct nvme_tcp_data_pdu *pdu = req->pdu;
+	u8 hdgst = i10_host_hdgst_len(queue);
+	int len = sizeof(*pdu) - req->offset + hdgst;
+	int ret;
+
+	if (queue->hdr_digest && !req->offset)
+		i10_host_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+
+	if (i10_host_legacy_path(req))
+		ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
+			offset_in_page(pdu) + req->offset, len,
+			MSG_DONTWAIT | MSG_MORE);
+	else {
+		if (i10_host_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		/* Caravans: data PDU aggregation */
+		queue->caravan_iovs[queue->nr_iovs].iov_base = pdu
+			+ req->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = len;
+		queue->caravan_len += len;
+		ret = len;
+	}
+
+	if (unlikely(ret <= 0))
+		return ret;
+
+	len -= ret;
+	if (!len) {
+		req->state = I10_HOST_SEND_DATA;
+		if (queue->data_digest)
+			crypto_ahash_init(queue->snd_hash);
+		if (!req->data_sent)
+			i10_host_init_iter(req, WRITE);
+		return 1;
+	}
+	req->offset += ret;
+
+	return -EAGAIN;
+}
+
+static int i10_host_try_send_ddgst(struct i10_host_request *req)
+{
+	struct i10_host_queue *queue = req->queue;
+	int ret;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_EOR };
+	struct kvec iov = {
+		.iov_base = &req->ddgst + req->offset,
+		.iov_len = NVME_TCP_DIGEST_LENGTH - req->offset
+	};
+
+	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	if (unlikely(ret <= 0))
+		return ret;
+
+	if (req->offset + ret == NVME_TCP_DIGEST_LENGTH) {
+		i10_host_done_send_req(queue);
+		return 1;
+	}
+
+	req->offset += ret;
+	return -EAGAIN;
+}
+
+/* To check if there's enough room in tcp_sndbuf */
+static inline int i10_host_sndbuf_nospace(struct i10_host_queue *queue,
+		int length)
+{
+	return sk_stream_wspace(queue->sock->sk) < length;
+}
+
+static bool i10_host_send_caravan(struct i10_host_queue *queue)
+{
+	/* 1. Caravan becomes full (64KB), or
+	 * 2. No-delay request arrives, or
+	 * 3. No more request remains in i10 queue
+	 */
+	return queue->send_now ||
+		(!hrtimer_active(&queue->doorbell_timer) &&
+		!queue->request && queue->caravan_len);
+}
+
+static int i10_host_try_send(struct i10_host_queue *queue)
+{
+	struct i10_host_request *req;
+	int ret = 1;
+
+	if (!queue->request) {
+		queue->request = i10_host_fetch_request(queue);
+		if (!queue->request && !queue->caravan_len)
+			return 0;
+	}
+
+	/* Send i10 caravans now */
+	if (i10_host_send_caravan(queue)) {
+		struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_EOR };
+		int i, i10_ret;
+
+		if (i10_host_sndbuf_nospace(queue, queue->caravan_len)) {
+			set_bit(SOCK_NOSPACE,
+				&queue->sock->sk->sk_socket->flags);
+			return 0;
+		}
+
+		i10_ret = kernel_sendmsg(queue->sock, &msg,
+				queue->caravan_iovs,
+				queue->nr_iovs,
+				queue->caravan_len);
+
+		if (unlikely(i10_ret <= 0)) {
+			dev_err(queue->ctrl->ctrl.device,
+				"I10_HOST: kernel_sendmsg fails (i10_ret %d)\n",
+				i10_ret);
+			return i10_ret;
+		}
+
+		for (i = 0; i < queue->nr_mapped; i++)
+			kunmap(queue->caravan_mapped[i]);
+
+		queue->nr_req = 0;
+		queue->nr_iovs = 0;
+		queue->nr_mapped = 0;
+		queue->caravan_len = 0;
+		queue->send_now = false;
+	}
+
+	if (queue->request)
+		req = queue->request;
+	else
+		return 0;
+
+	if (req->state == I10_HOST_SEND_CMD_PDU) {
+		ret = i10_host_try_send_cmd_pdu(req);
+		if (ret <= 0)
+			goto done;
+		if (!i10_host_has_inline_data(req))
+			return ret;
+	}
+
+	if (req->state == I10_HOST_SEND_H2C_PDU) {
+		ret = i10_host_try_send_data_pdu(req);
+		if (ret <= 0)
+			goto done;
+	}
+
+	if (req->state == I10_HOST_SEND_DATA) {
+		ret = i10_host_try_send_data(req);
+		if (ret <= 0)
+			goto done;
+	}
+
+	if (req->state == I10_TCP_SEND_DDGST)
+		ret = i10_host_try_send_ddgst(req);
+done:
+	if (ret == -EAGAIN)
+		ret = 0;
+	return ret;
+}
+
+static int i10_host_try_recv(struct i10_host_queue *queue)
+{
+	struct socket *sock = queue->sock;
+	struct sock *sk = sock->sk;
+	read_descriptor_t rd_desc;
+	int consumed;
+
+	rd_desc.arg.data = queue;
+	rd_desc.count = 1;
+	lock_sock(sk);
+	queue->nr_cqe = 0;
+	consumed = sock->ops->read_sock(sk, &rd_desc, i10_host_recv_skb);
+	release_sock(sk);
+	return consumed;
+}
+
+enum hrtimer_restart i10_host_doorbell_timeout(struct hrtimer *timer)
+{
+	struct i10_host_queue *queue =
+		container_of(timer, struct i10_host_queue,
+			doorbell_timer);
+
+	queue_work_on(queue->io_cpu, i10_host_wq, &queue->io_work);
+	return HRTIMER_NORESTART;
+}
+
+static void i10_host_io_work(struct work_struct *w)
+{
+	struct i10_host_queue *queue =
+		container_of(w, struct i10_host_queue, io_work);
+	unsigned long deadline = jiffies + msecs_to_jiffies(1);
+
+	do {
+		bool pending = false;
+		int result;
+
+		result = i10_host_try_send(queue);
+		if (result > 0) {
+			pending = true;
+		} else if (unlikely(result < 0)) {
+			dev_err(queue->ctrl->ctrl.device,
+				"failed to send request %d\n", result);
+
+			/*
+			 * Fail the request unless peer closed the connection,
+			 * in which case error recovery flow will complete all.
+			 */
+			if ((result != -EPIPE) && (result != -ECONNRESET))
+				i10_host_fail_request(queue->request);
+			i10_host_done_send_req(queue);
+			return;
+		}
+
+		result = i10_host_try_recv(queue);
+		if (result > 0)
+			pending = true;
+
+		if (!pending)
+			return;
+
+	} while (!time_after(jiffies, deadline)); /* quota is exhausted */
+
+	queue_work_on(queue->io_cpu, i10_host_wq, &queue->io_work);
+}
+
+static void i10_host_free_crypto(struct i10_host_queue *queue)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(queue->rcv_hash);
+
+	ahash_request_free(queue->rcv_hash);
+	ahash_request_free(queue->snd_hash);
+	crypto_free_ahash(tfm);
+}
+
+static int i10_host_alloc_crypto(struct i10_host_queue *queue)
+{
+	struct crypto_ahash *tfm;
+
+	tfm = crypto_alloc_ahash("crc32c", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm))
+		return PTR_ERR(tfm);
+
+	queue->snd_hash = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!queue->snd_hash)
+		goto free_tfm;
+	ahash_request_set_callback(queue->snd_hash, 0, NULL, NULL);
+
+	queue->rcv_hash = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!queue->rcv_hash)
+		goto free_snd_hash;
+	ahash_request_set_callback(queue->rcv_hash, 0, NULL, NULL);
+
+	return 0;
+free_snd_hash:
+	ahash_request_free(queue->snd_hash);
+free_tfm:
+	crypto_free_ahash(tfm);
+	return -ENOMEM;
+}
+
+static void i10_host_free_async_req(struct i10_host_ctrl *ctrl)
+{
+	struct i10_host_request *async = &ctrl->async_req;
+
+	page_frag_free(async->pdu);
+}
+
+static int i10_host_alloc_async_req(struct i10_host_ctrl *ctrl)
+{
+	struct i10_host_queue *queue = &ctrl->queues[0];
+	struct i10_host_request *async = &ctrl->async_req;
+	u8 hdgst = i10_host_hdgst_len(queue);
+
+	async->pdu = page_frag_alloc(&queue->pf_cache,
+		sizeof(struct nvme_tcp_cmd_pdu) + hdgst,
+		GFP_KERNEL | __GFP_ZERO);
+	if (!async->pdu)
+		return -ENOMEM;
+
+	async->queue = &ctrl->queues[0];
+	return 0;
+}
+
+static void i10_host_free_queue(struct nvme_ctrl *nctrl, int qid)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+	struct i10_host_queue *queue = &ctrl->queues[qid];
+
+	if (!test_and_clear_bit(I10_HOST_Q_ALLOCATED, &queue->flags))
+		return;
+
+	if (queue->hdr_digest || queue->data_digest)
+		i10_host_free_crypto(queue);
+
+	sock_release(queue->sock);
+	kfree(queue->pdu);
+	kfree(queue->caravan_iovs);
+	kfree(queue->caravan_mapped);
+	hrtimer_cancel(&queue->doorbell_timer);
+}
+
+static int i10_host_init_connection(struct i10_host_queue *queue)
+{
+	struct nvme_tcp_icreq_pdu *icreq;
+	struct nvme_tcp_icresp_pdu *icresp;
+	struct msghdr msg = {};
+	struct kvec iov;
+	bool ctrl_hdgst, ctrl_ddgst;
+	int ret;
+
+	icreq = kzalloc(sizeof(*icreq), GFP_KERNEL);
+	if (!icreq)
+		return -ENOMEM;
+
+	icresp = kzalloc(sizeof(*icresp), GFP_KERNEL);
+	if (!icresp) {
+		ret = -ENOMEM;
+		goto free_icreq;
+	}
+
+	icreq->hdr.type = nvme_tcp_icreq;
+	icreq->hdr.hlen = sizeof(*icreq);
+	icreq->hdr.pdo = 0;
+	icreq->hdr.plen = cpu_to_le32(icreq->hdr.hlen);
+	icreq->pfv = cpu_to_le16(NVME_TCP_PFV_1_0);
+	icreq->maxr2t = 0; /* single inflight r2t supported */
+	icreq->hpda = 0; /* no alignment constraint */
+	if (queue->hdr_digest)
+		icreq->digest |= NVME_TCP_HDR_DIGEST_ENABLE;
+	if (queue->data_digest)
+		icreq->digest |= NVME_TCP_DATA_DIGEST_ENABLE;
+
+	iov.iov_base = icreq;
+	iov.iov_len = sizeof(*icreq);
+	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	if (ret < 0)
+		goto free_icresp;
+
+	memset(&msg, 0, sizeof(msg));
+	iov.iov_base = icresp;
+	iov.iov_len = sizeof(*icresp);
+	ret = kernel_recvmsg(queue->sock, &msg, &iov, 1,
+			iov.iov_len, msg.msg_flags);
+	if (ret < 0)
+		goto free_icresp;
+
+	ret = -EINVAL;
+	if (icresp->hdr.type != nvme_tcp_icresp) {
+		pr_err("queue %d: bad type returned %d\n",
+			i10_host_queue_id(queue), icresp->hdr.type);
+		goto free_icresp;
+	}
+
+	if (le32_to_cpu(icresp->hdr.plen) != sizeof(*icresp)) {
+		pr_err("queue %d: bad pdu length returned %d\n",
+			i10_host_queue_id(queue), icresp->hdr.plen);
+		goto free_icresp;
+	}
+
+	if (icresp->pfv != NVME_TCP_PFV_1_0) {
+		pr_err("queue %d: bad pfv returned %d\n",
+			i10_host_queue_id(queue), icresp->pfv);
+		goto free_icresp;
+	}
+
+	ctrl_ddgst = !!(icresp->digest & NVME_TCP_DATA_DIGEST_ENABLE);
+	if ((queue->data_digest && !ctrl_ddgst) ||
+	    (!queue->data_digest && ctrl_ddgst)) {
+		pr_err("queue %d: data digest mismatch host: %s ctrl: %s\n",
+			i10_host_queue_id(queue),
+			queue->data_digest ? "enabled" : "disabled",
+			ctrl_ddgst ? "enabled" : "disabled");
+		goto free_icresp;
+	}
+
+	ctrl_hdgst = !!(icresp->digest & NVME_TCP_HDR_DIGEST_ENABLE);
+	if ((queue->hdr_digest && !ctrl_hdgst) ||
+	    (!queue->hdr_digest && ctrl_hdgst)) {
+		pr_err("queue %d: header digest mismatch host: %s ctrl: %s\n",
+			i10_host_queue_id(queue),
+			queue->hdr_digest ? "enabled" : "disabled",
+			ctrl_hdgst ? "enabled" : "disabled");
+		goto free_icresp;
+	}
+
+	if (icresp->cpda != 0) {
+		pr_err("queue %d: unsupported cpda returned %d\n",
+			i10_host_queue_id(queue), icresp->cpda);
+		goto free_icresp;
+	}
+
+	ret = 0;
+free_icresp:
+	kfree(icresp);
+free_icreq:
+	kfree(icreq);
+	return ret;
+}
+
+static int i10_host_alloc_queue(struct nvme_ctrl *nctrl,
+		int qid, size_t queue_size)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+	struct i10_host_queue *queue = &ctrl->queues[qid];
+	struct linger sol = { .l_onoff = 1, .l_linger = 0 };
+	int ret, opt, rcv_pdu_size, n;
+
+	queue->ctrl = ctrl;
+	INIT_LIST_HEAD(&queue->send_list);
+	spin_lock_init(&queue->lock);
+	INIT_WORK(&queue->io_work, i10_host_io_work);
+	queue->queue_size = queue_size;
+
+	if (qid > 0)
+		queue->cmnd_capsule_len = nctrl->ioccsz * 16;
+	else
+		queue->cmnd_capsule_len = sizeof(struct nvme_command) +
+						NVME_TCP_ADMIN_CCSZ;
+
+	ret = sock_create(ctrl->addr.ss_family, SOCK_STREAM,
+			IPPROTO_TCP, &queue->sock);
+	if (ret) {
+		dev_err(nctrl->device,
+			"failed to create socket: %d\n", ret);
+		return ret;
+	}
+
+	/* Single syn retry */
+	opt = 1;
+	ret = kernel_setsockopt(queue->sock, IPPROTO_TCP, TCP_SYNCNT,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(nctrl->device,
+			"failed to set TCP_SYNCNT sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/* Set TCP no delay */
+	opt = 1;
+	ret = kernel_setsockopt(queue->sock, IPPROTO_TCP,
+			TCP_NODELAY, (char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(nctrl->device,
+			"failed to set TCP_NODELAY sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/* Set a fixed size of sndbuf/rcvbuf (8MB) */
+	opt = 8388608;
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_SNDBUFFORCE,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_SNDBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_RCVBUFFORCE,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_RCVBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/*
+	 * Cleanup whatever is sitting in the TCP transmit queue on socket
+	 * close. This is done to prevent stale data from being sent should
+	 * the network connection be restored before TCP times out.
+	 */
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_LINGER,
+			(char *)&sol, sizeof(sol));
+	if (ret) {
+		dev_err(nctrl->device,
+			"failed to set SO_LINGER sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/* Set socket type of service */
+	if (nctrl->opts->tos >= 0) {
+		opt = nctrl->opts->tos;
+		ret = kernel_setsockopt(queue->sock, SOL_IP, IP_TOS,
+				(char *)&opt, sizeof(opt));
+		if (ret) {
+			dev_err(nctrl->device,
+				"failed to set IP_TOS sock opt %d\n", ret);
+			goto err_sock;
+		}
+	}
+
+	queue->sock->sk->sk_allocation = GFP_ATOMIC;
+	if (!qid)
+		n = 0;
+	else
+		n = (qid - 1) % num_online_cpus();
+	queue->io_cpu = cpumask_next_wrap(n - 1, cpu_online_mask, -1, false);
+	queue->request = NULL;
+	queue->data_remaining = 0;
+	queue->ddgst_remaining = 0;
+	queue->pdu_remaining = 0;
+	queue->pdu_offset = 0;
+	sk_set_memalloc(queue->sock->sk);
+
+	if (nctrl->opts->mask & NVMF_OPT_HOST_TRADDR) {
+		ret = kernel_bind(queue->sock, (struct sockaddr *)&ctrl->src_addr,
+			sizeof(ctrl->src_addr));
+		if (ret) {
+			dev_err(nctrl->device,
+				"failed to bind queue %d socket %d\n",
+				qid, ret);
+			goto err_sock;
+		}
+	}
+
+	/* i10 initialization */
+	queue->caravan_iovs = kcalloc(I10_AGGREGATION_SIZE * 2,
+				sizeof(*queue->caravan_iovs), GFP_KERNEL);
+	if (!queue->caravan_iovs) {
+		ret = -ENOMEM;
+		goto err_sock;
+	}
+
+	queue->caravan_mapped = kcalloc(I10_AGGREGATION_SIZE,
+				sizeof(**queue->caravan_mapped), GFP_KERNEL);
+	if (!queue->caravan_mapped) {
+		ret = -ENOMEM;
+		goto err_caravan_iovs;
+	}
+
+	queue->nr_req = 0;
+	queue->nr_iovs = 0;
+	queue->nr_mapped = 0;
+	queue->caravan_len = 0;
+	queue->send_now = false;
+
+	/* i10 delayed doorbell setup */
+	hrtimer_init(&queue->doorbell_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	queue->doorbell_timer.function = &i10_host_doorbell_timeout;
+
+	queue->hdr_digest = nctrl->opts->hdr_digest;
+	queue->data_digest = nctrl->opts->data_digest;
+	if (queue->hdr_digest || queue->data_digest) {
+		ret = i10_host_alloc_crypto(queue);
+		if (ret) {
+			dev_err(nctrl->device,
+				"failed to allocate queue %d crypto\n", qid);
+			goto err_caravan_mapped;
+		}
+	}
+
+	rcv_pdu_size = sizeof(struct nvme_tcp_rsp_pdu) +
+			i10_host_hdgst_len(queue);
+	queue->pdu = kmalloc(rcv_pdu_size, GFP_KERNEL);
+	if (!queue->pdu) {
+		ret = -ENOMEM;
+		goto err_crypto;
+	}
+
+	dev_dbg(nctrl->device, "connecting queue %d\n",
+			i10_host_queue_id(queue));
+
+	ret = kernel_connect(queue->sock, (struct sockaddr *)&ctrl->addr,
+		sizeof(ctrl->addr), 0);
+	if (ret) {
+		dev_err(nctrl->device,
+			"failed to connect socket: %d\n", ret);
+		goto err_rcv_pdu;
+	}
+
+	ret = i10_host_init_connection(queue);
+	if (ret)
+		goto err_init_connect;
+
+	queue->rd_enabled = true;
+	set_bit(I10_HOST_Q_ALLOCATED, &queue->flags);
+	i10_host_init_recv_ctx(queue);
+
+	write_lock_bh(&queue->sock->sk->sk_callback_lock);
+	queue->sock->sk->sk_user_data = queue;
+	queue->state_change = queue->sock->sk->sk_state_change;
+	queue->data_ready = queue->sock->sk->sk_data_ready;
+	queue->write_space = queue->sock->sk->sk_write_space;
+	queue->sock->sk->sk_data_ready = i10_host_data_ready;
+	queue->sock->sk->sk_state_change = i10_host_state_change;
+	queue->sock->sk->sk_write_space = i10_host_write_space;
+#ifdef CONFIG_NET_RX_BUSY_POLL
+	queue->sock->sk->sk_ll_usec = 1;
+#endif
+	write_unlock_bh(&queue->sock->sk->sk_callback_lock);
+
+	return 0;
+
+err_init_connect:
+	kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+err_rcv_pdu:
+	kfree(queue->pdu);
+err_crypto:
+	if (queue->hdr_digest || queue->data_digest)
+		i10_host_free_crypto(queue);
+err_caravan_mapped:
+	kfree(queue->caravan_mapped);
+err_caravan_iovs:
+	kfree(queue->caravan_iovs);
+err_sock:
+	sock_release(queue->sock);
+	queue->sock = NULL;
+	return ret;
+}
+
+static void i10_host_restore_sock_calls(struct i10_host_queue *queue)
+{
+	struct socket *sock = queue->sock;
+
+	write_lock_bh(&sock->sk->sk_callback_lock);
+	sock->sk->sk_user_data  = NULL;
+	sock->sk->sk_data_ready = queue->data_ready;
+	sock->sk->sk_state_change = queue->state_change;
+	sock->sk->sk_write_space  = queue->write_space;
+	write_unlock_bh(&sock->sk->sk_callback_lock);
+}
+
+static void __i10_host_stop_queue(struct i10_host_queue *queue)
+{
+	kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+	i10_host_restore_sock_calls(queue);
+	cancel_work_sync(&queue->io_work);
+}
+
+static void i10_host_stop_queue(struct nvme_ctrl *nctrl, int qid)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+	struct i10_host_queue *queue = &ctrl->queues[qid];
+
+	if (!test_and_clear_bit(I10_HOST_Q_LIVE, &queue->flags))
+		return;
+
+	__i10_host_stop_queue(queue);
+}
+
+static int i10_host_start_queue(struct nvme_ctrl *nctrl, int idx)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+	int ret;
+
+	if (idx)
+		ret = nvmf_connect_io_queue(nctrl, idx, false);
+	else
+		ret = nvmf_connect_admin_queue(nctrl);
+
+	if (!ret) {
+		set_bit(I10_HOST_Q_LIVE, &ctrl->queues[idx].flags);
+	} else {
+		if (test_bit(I10_HOST_Q_ALLOCATED, &ctrl->queues[idx].flags))
+			__i10_host_stop_queue(&ctrl->queues[idx]);
+		dev_err(nctrl->device,
+			"failed to connect queue: %d ret=%d\n", idx, ret);
+	}
+	return ret;
+}
+
+static struct blk_mq_tag_set *i10_host_alloc_tagset(struct nvme_ctrl *nctrl,
+		bool admin)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+	struct blk_mq_tag_set *set;
+	int ret;
+
+	if (admin) {
+		set = &ctrl->admin_tag_set;
+		memset(set, 0, sizeof(*set));
+		set->ops = &i10_host_admin_mq_ops;
+		set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+		set->reserved_tags = 2; /* connect + keep-alive */
+		set->numa_node = NUMA_NO_NODE;
+		set->cmd_size = sizeof(struct i10_host_request);
+		set->driver_data = ctrl;
+		set->nr_hw_queues = 1;
+		set->timeout = ADMIN_TIMEOUT;
+	} else {
+		set = &ctrl->tag_set;
+		memset(set, 0, sizeof(*set));
+		set->ops = &i10_host_mq_ops;
+		set->queue_depth = nctrl->sqsize + 1;
+		set->reserved_tags = 1; /* fabric connect */
+		set->numa_node = NUMA_NO_NODE;
+		set->flags = BLK_MQ_F_SHOULD_MERGE;
+		set->cmd_size = sizeof(struct i10_host_request);
+		set->driver_data = ctrl;
+		set->nr_hw_queues = nctrl->queue_count - 1;
+		set->timeout = NVME_IO_TIMEOUT;
+		set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	}
+
+	ret = blk_mq_alloc_tag_set(set);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return set;
+}
+
+static void i10_host_free_admin_queue(struct nvme_ctrl *ctrl)
+{
+	if (to_i10_host_ctrl(ctrl)->async_req.pdu) {
+		i10_host_free_async_req(to_i10_host_ctrl(ctrl));
+		to_i10_host_ctrl(ctrl)->async_req.pdu = NULL;
+	}
+
+	i10_host_free_queue(ctrl, 0);
+}
+
+static void i10_host_free_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->queue_count; i++)
+		i10_host_free_queue(ctrl, i);
+}
+
+static void i10_host_stop_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->queue_count; i++)
+		i10_host_stop_queue(ctrl, i);
+}
+
+static int i10_host_start_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i, ret = 0;
+
+	for (i = 1; i < ctrl->queue_count; i++) {
+		ret = i10_host_start_queue(ctrl, i);
+		if (ret)
+			goto out_stop_queues;
+	}
+
+	return 0;
+
+out_stop_queues:
+	for (i--; i >= 1; i--)
+		i10_host_stop_queue(ctrl, i);
+	return ret;
+}
+
+static int i10_host_alloc_admin_queue(struct nvme_ctrl *ctrl)
+{
+	int ret;
+
+	ret = i10_host_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+	if (ret)
+		return ret;
+
+	ret = i10_host_alloc_async_req(to_i10_host_ctrl(ctrl));
+	if (ret)
+		goto out_free_queue;
+
+	return 0;
+
+out_free_queue:
+	i10_host_free_queue(ctrl, 0);
+	return ret;
+}
+
+static int __i10_host_alloc_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i, ret;
+
+	for (i = 1; i < ctrl->queue_count; i++) {
+		ret = i10_host_alloc_queue(ctrl, i,
+				ctrl->sqsize + 1);
+		if (ret)
+			goto out_free_queues;
+	}
+
+	return 0;
+
+out_free_queues:
+	for (i--; i >= 1; i--)
+		i10_host_free_queue(ctrl, i);
+
+	return ret;
+}
+
+static unsigned int i10_host_nr_io_queues(struct nvme_ctrl *ctrl)
+{
+	unsigned int nr_io_queues;
+
+	nr_io_queues = min(ctrl->opts->nr_io_queues, num_online_cpus());
+	nr_io_queues += min(ctrl->opts->nr_write_queues, num_online_cpus());
+	nr_io_queues += min(ctrl->opts->nr_poll_queues, num_online_cpus());
+
+	return nr_io_queues;
+}
+
+static void i10_host_set_io_queues(struct nvme_ctrl *nctrl,
+		unsigned int nr_io_queues)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+	struct nvmf_ctrl_options *opts = nctrl->opts;
+
+	if (opts->nr_write_queues && opts->nr_io_queues < nr_io_queues) {
+		/*
+		 * separate read/write queues
+		 * hand out dedicated default queues only after we have
+		 * sufficient read queues.
+		 */
+		ctrl->io_queues[HCTX_TYPE_READ] = opts->nr_io_queues;
+		nr_io_queues -= ctrl->io_queues[HCTX_TYPE_READ];
+		ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+			min(opts->nr_write_queues, nr_io_queues);
+		nr_io_queues -= ctrl->io_queues[HCTX_TYPE_DEFAULT];
+	} else {
+		/*
+		 * shared read/write queues
+		 * either no write queues were requested, or we don't have
+		 * sufficient queue count to have dedicated default queues.
+		 */
+		ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+			min(opts->nr_io_queues, nr_io_queues);
+		nr_io_queues -= ctrl->io_queues[HCTX_TYPE_DEFAULT];
+	}
+
+	if (opts->nr_poll_queues && nr_io_queues) {
+		/* map dedicated poll queues only if we have queues left */
+		ctrl->io_queues[HCTX_TYPE_POLL] =
+			min(opts->nr_poll_queues, nr_io_queues);
+	}
+}
+
+static int i10_host_alloc_io_queues(struct nvme_ctrl *ctrl)
+{
+	unsigned int nr_io_queues;
+	int ret;
+
+	nr_io_queues = i10_host_nr_io_queues(ctrl);
+	ret = nvme_set_queue_count(ctrl, &nr_io_queues);
+	if (ret)
+		return ret;
+
+	ctrl->queue_count = nr_io_queues + 1;
+	if (ctrl->queue_count < 2)
+		return 0;
+
+	dev_info(ctrl->device,
+		"creating %d I/O queues.\n", nr_io_queues);
+
+	i10_host_set_io_queues(ctrl, nr_io_queues);
+
+	return __i10_host_alloc_io_queues(ctrl);
+}
+
+static void i10_host_destroy_io_queues(struct nvme_ctrl *ctrl, bool remove)
+{
+	i10_host_stop_io_queues(ctrl);
+	if (remove) {
+		blk_cleanup_queue(ctrl->connect_q);
+		blk_mq_free_tag_set(ctrl->tagset);
+	}
+	i10_host_free_io_queues(ctrl);
+}
+
+static int i10_host_configure_io_queues(struct nvme_ctrl *ctrl, bool new)
+{
+	int ret;
+
+	ret = i10_host_alloc_io_queues(ctrl);
+	if (ret)
+		return ret;
+
+	if (new) {
+		ctrl->tagset = i10_host_alloc_tagset(ctrl, false);
+		if (IS_ERR(ctrl->tagset)) {
+			ret = PTR_ERR(ctrl->tagset);
+			goto out_free_io_queues;
+		}
+
+		ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+		if (IS_ERR(ctrl->connect_q)) {
+			ret = PTR_ERR(ctrl->connect_q);
+			goto out_free_tag_set;
+		}
+	} else {
+		blk_mq_update_nr_hw_queues(ctrl->tagset,
+			ctrl->queue_count - 1);
+	}
+
+	ret = i10_host_start_io_queues(ctrl);
+	if (ret)
+		goto out_cleanup_connect_q;
+
+	return 0;
+
+out_cleanup_connect_q:
+	if (new)
+		blk_cleanup_queue(ctrl->connect_q);
+out_free_tag_set:
+	if (new)
+		blk_mq_free_tag_set(ctrl->tagset);
+out_free_io_queues:
+	i10_host_free_io_queues(ctrl);
+	return ret;
+}
+
+static void i10_host_destroy_admin_queue(struct nvme_ctrl *ctrl, bool remove)
+{
+	i10_host_stop_queue(ctrl, 0);
+	if (remove) {
+		blk_cleanup_queue(ctrl->admin_q);
+		blk_cleanup_queue(ctrl->fabrics_q);
+		blk_mq_free_tag_set(ctrl->admin_tagset);
+	}
+	i10_host_free_admin_queue(ctrl);
+}
+
+static int i10_host_configure_admin_queue(struct nvme_ctrl *ctrl, bool new)
+{
+	int error;
+
+	error = i10_host_alloc_admin_queue(ctrl);
+	if (error)
+		return error;
+
+	if (new) {
+		ctrl->admin_tagset = i10_host_alloc_tagset(ctrl, true);
+		if (IS_ERR(ctrl->admin_tagset)) {
+			error = PTR_ERR(ctrl->admin_tagset);
+			goto out_free_queue;
+		}
+
+		ctrl->fabrics_q = blk_mq_init_queue(ctrl->admin_tagset);
+		if (IS_ERR(ctrl->fabrics_q)) {
+			error = PTR_ERR(ctrl->fabrics_q);
+			goto out_free_tagset;
+		}
+
+		ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+		if (IS_ERR(ctrl->admin_q)) {
+			error = PTR_ERR(ctrl->admin_q);
+			goto out_cleanup_fabrics_q;
+		}
+	}
+
+	error = i10_host_start_queue(ctrl, 0);
+	if (error)
+		goto out_cleanup_queue;
+
+	error = nvme_enable_ctrl(ctrl);
+	if (error)
+		goto out_stop_queue;
+
+	blk_mq_unquiesce_queue(ctrl->admin_q);
+
+	error = nvme_init_identify(ctrl);
+	if (error)
+		goto out_stop_queue;
+
+	return 0;
+
+out_stop_queue:
+	i10_host_stop_queue(ctrl, 0);
+out_cleanup_queue:
+	if (new)
+		blk_cleanup_queue(ctrl->admin_q);
+out_cleanup_fabrics_q:
+	if (new)
+		blk_cleanup_queue(ctrl->fabrics_q);
+out_free_tagset:
+	if (new)
+		blk_mq_free_tag_set(ctrl->admin_tagset);
+out_free_queue:
+	i10_host_free_admin_queue(ctrl);
+	return error;
+}
+
+static void i10_host_teardown_admin_queue(struct nvme_ctrl *ctrl,
+		bool remove)
+{
+	blk_mq_quiesce_queue(ctrl->admin_q);
+	i10_host_stop_queue(ctrl, 0);
+	if (ctrl->admin_tagset) {
+		blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+			nvme_cancel_request, ctrl);
+		blk_mq_tagset_wait_completed_request(ctrl->admin_tagset);
+	}
+	if (remove)
+		blk_mq_unquiesce_queue(ctrl->admin_q);
+	i10_host_destroy_admin_queue(ctrl, remove);
+}
+
+static void i10_host_teardown_io_queues(struct nvme_ctrl *ctrl,
+		bool remove)
+{
+	if (ctrl->queue_count <= 1)
+		return;
+	nvme_stop_queues(ctrl);
+	i10_host_stop_io_queues(ctrl);
+	if (ctrl->tagset) {
+		blk_mq_tagset_busy_iter(ctrl->tagset,
+			nvme_cancel_request, ctrl);
+		blk_mq_tagset_wait_completed_request(ctrl->tagset);
+	}
+	if (remove)
+		nvme_start_queues(ctrl);
+	i10_host_destroy_io_queues(ctrl, remove);
+}
+
+static void i10_host_reconnect_or_remove(struct nvme_ctrl *ctrl)
+{
+	/* If we are resetting/deleting then do nothing */
+	if (ctrl->state != NVME_CTRL_CONNECTING) {
+		WARN_ON_ONCE(ctrl->state == NVME_CTRL_NEW ||
+			ctrl->state == NVME_CTRL_LIVE);
+		return;
+	}
+
+	if (nvmf_should_reconnect(ctrl)) {
+		dev_info(ctrl->device, "Reconnecting in %d seconds...\n",
+			ctrl->opts->reconnect_delay);
+		queue_delayed_work(nvme_wq, &to_i10_host_ctrl(ctrl)->connect_work,
+				ctrl->opts->reconnect_delay * HZ);
+	} else {
+		dev_info(ctrl->device, "Removing controller...\n");
+		nvme_delete_ctrl(ctrl);
+	}
+}
+
+static int i10_host_setup_ctrl(struct nvme_ctrl *ctrl, bool new)
+{
+	struct nvmf_ctrl_options *opts = ctrl->opts;
+	int ret;
+
+	ret = i10_host_configure_admin_queue(ctrl, new);
+	if (ret)
+		return ret;
+
+	if (ctrl->icdoff) {
+		dev_err(ctrl->device, "icdoff is not supported!\n");
+		goto destroy_admin;
+	}
+
+	if (opts->queue_size > ctrl->sqsize + 1)
+		dev_warn(ctrl->device,
+			"queue_size %zu > ctrl sqsize %u, clamping down\n",
+			opts->queue_size, ctrl->sqsize + 1);
+
+	if (ctrl->sqsize + 1 > ctrl->maxcmd) {
+		dev_warn(ctrl->device,
+			"sqsize %u > ctrl maxcmd %u, clamping down\n",
+			ctrl->sqsize + 1, ctrl->maxcmd);
+		ctrl->sqsize = ctrl->maxcmd - 1;
+	}
+
+	if (ctrl->queue_count > 1) {
+		ret = i10_host_configure_io_queues(ctrl, new);
+		if (ret)
+			goto destroy_admin;
+	}
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_LIVE)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->state != NVME_CTRL_DELETING);
+		ret = -EINVAL;
+		goto destroy_io;
+	}
+
+	nvme_start_ctrl(ctrl);
+	return 0;
+
+destroy_io:
+	if (ctrl->queue_count > 1)
+		i10_host_destroy_io_queues(ctrl, new);
+destroy_admin:
+	i10_host_stop_queue(ctrl, 0);
+	i10_host_destroy_admin_queue(ctrl, new);
+	return ret;
+}
+
+static void i10_host_reconnect_ctrl_work(struct work_struct *work)
+{
+	struct i10_host_ctrl *tcp_ctrl = container_of(to_delayed_work(work),
+			struct i10_host_ctrl, connect_work);
+	struct nvme_ctrl *ctrl = &tcp_ctrl->ctrl;
+
+	++ctrl->nr_reconnects;
+
+	if (i10_host_setup_ctrl(ctrl, false))
+		goto requeue;
+
+	dev_info(ctrl->device, "Successfully reconnected (%d attempt)\n",
+			ctrl->nr_reconnects);
+
+	ctrl->nr_reconnects = 0;
+
+	return;
+
+requeue:
+	dev_info(ctrl->device, "Failed reconnect attempt %d\n",
+			ctrl->nr_reconnects);
+	i10_host_reconnect_or_remove(ctrl);
+}
+
+static void i10_host_error_recovery_work(struct work_struct *work)
+{
+	struct i10_host_ctrl *tcp_ctrl = container_of(work,
+				struct i10_host_ctrl, err_work);
+	struct nvme_ctrl *ctrl = &tcp_ctrl->ctrl;
+
+	nvme_stop_keep_alive(ctrl);
+	i10_host_teardown_io_queues(ctrl, false);
+	/* unquiesce to fail fast pending requests */
+	nvme_start_queues(ctrl);
+	i10_host_teardown_admin_queue(ctrl, false);
+	blk_mq_unquiesce_queue(ctrl->admin_q);
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->state != NVME_CTRL_DELETING);
+		return;
+	}
+
+	i10_host_reconnect_or_remove(ctrl);
+}
+
+static void i10_host_teardown_ctrl(struct nvme_ctrl *ctrl, bool shutdown)
+{
+	cancel_work_sync(&to_i10_host_ctrl(ctrl)->err_work);
+	cancel_delayed_work_sync(&to_i10_host_ctrl(ctrl)->connect_work);
+
+	i10_host_teardown_io_queues(ctrl, shutdown);
+	blk_mq_quiesce_queue(ctrl->admin_q);
+	if (shutdown)
+		nvme_shutdown_ctrl(ctrl);
+	else
+		nvme_disable_ctrl(ctrl);
+	i10_host_teardown_admin_queue(ctrl, shutdown);
+}
+
+static void i10_host_delete_ctrl(struct nvme_ctrl *ctrl)
+{
+	i10_host_teardown_ctrl(ctrl, true);
+}
+
+static void nvme_reset_ctrl_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(work, struct nvme_ctrl, reset_work);
+
+	nvme_stop_ctrl(ctrl);
+	i10_host_teardown_ctrl(ctrl, false);
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->state != NVME_CTRL_DELETING);
+		return;
+	}
+
+	if (i10_host_setup_ctrl(ctrl, false))
+		goto out_fail;
+
+	return;
+
+out_fail:
+	++ctrl->nr_reconnects;
+	i10_host_reconnect_or_remove(ctrl);
+}
+
+static void i10_host_free_ctrl(struct nvme_ctrl *nctrl)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(nctrl);
+
+	if (list_empty(&ctrl->list))
+		goto free_ctrl;
+
+	mutex_lock(&i10_host_ctrl_mutex);
+	list_del(&ctrl->list);
+	mutex_unlock(&i10_host_ctrl_mutex);
+
+	nvmf_free_options(nctrl->opts);
+free_ctrl:
+	kfree(ctrl->queues);
+	kfree(ctrl);
+}
+
+static void i10_host_set_sg_null(struct nvme_command *c)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+
+	sg->addr = 0;
+	sg->length = 0;
+	sg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |
+			NVME_SGL_FMT_TRANSPORT_A;
+}
+
+static void i10_host_set_sg_inline(struct i10_host_queue *queue,
+		struct nvme_command *c, u32 data_len)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+
+	sg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);
+	sg->length = cpu_to_le32(data_len);
+	sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
+}
+
+static void i10_host_set_sg_host_data(struct nvme_command *c,
+		u32 data_len)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+
+	sg->addr = 0;
+	sg->length = cpu_to_le32(data_len);
+	sg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |
+			NVME_SGL_FMT_TRANSPORT_A;
+}
+
+static void i10_host_submit_async_event(struct nvme_ctrl *arg)
+{
+	struct i10_host_ctrl *ctrl = to_i10_host_ctrl(arg);
+	struct i10_host_queue *queue = &ctrl->queues[0];
+	struct nvme_tcp_cmd_pdu *pdu = ctrl->async_req.pdu;
+	struct nvme_command *cmd = &pdu->cmd;
+	u8 hdgst = i10_host_hdgst_len(queue);
+
+	memset(pdu, 0, sizeof(*pdu));
+	pdu->hdr.type = nvme_tcp_cmd;
+	if (queue->hdr_digest)
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);
+
+	cmd->common.opcode = nvme_admin_async_event;
+	cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+	cmd->common.flags |= NVME_CMD_SGL_METABUF;
+	i10_host_set_sg_null(cmd);
+
+	ctrl->async_req.state = I10_HOST_SEND_CMD_PDU;
+	ctrl->async_req.offset = 0;
+	ctrl->async_req.curr_bio = NULL;
+	ctrl->async_req.data_len = 0;
+
+	i10_host_queue_request(&ctrl->async_req);
+}
+
+static enum blk_eh_timer_return
+i10_host_timeout(struct request *rq, bool reserved)
+{
+	struct i10_host_request *req = blk_mq_rq_to_pdu(rq);
+	struct i10_host_ctrl *ctrl = req->queue->ctrl;
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+
+	/*
+	 * Restart the timer if a controller reset is already scheduled. Any
+	 * timed out commands would be handled before entering the connecting
+	 * state.
+	 */
+	if (ctrl->ctrl.state == NVME_CTRL_RESETTING)
+		return BLK_EH_RESET_TIMER;
+
+	dev_warn(ctrl->ctrl.device,
+		"queue %d: timeout request %#x type %d\n",
+		i10_host_queue_id(req->queue), rq->tag, pdu->hdr.type);
+
+	if (ctrl->ctrl.state != NVME_CTRL_LIVE) {
+		/*
+		 * Teardown immediately if controller times out while starting
+		 * or we are already started error recovery. all outstanding
+		 * requests are completed on shutdown, so we return BLK_EH_DONE.
+		 */
+		flush_work(&ctrl->err_work);
+		i10_host_teardown_io_queues(&ctrl->ctrl, false);
+		i10_host_teardown_admin_queue(&ctrl->ctrl, false);
+		return BLK_EH_DONE;
+	}
+
+	dev_warn(ctrl->ctrl.device, "starting error recovery\n");
+	i10_host_error_recovery(&ctrl->ctrl);
+
+	return BLK_EH_RESET_TIMER;
+}
+
+static blk_status_t i10_host_map_data(struct i10_host_queue *queue,
+			struct request *rq)
+{
+	struct i10_host_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+	struct nvme_command *c = &pdu->cmd;
+
+	c->common.flags |= NVME_CMD_SGL_METABUF;
+
+	if (!blk_rq_nr_phys_segments(rq))
+		i10_host_set_sg_null(c);
+	else if (rq_data_dir(rq) == WRITE &&
+	    req->data_len <= i10_host_inline_data_size(queue))
+		i10_host_set_sg_inline(queue, c, req->data_len);
+	else
+		i10_host_set_sg_host_data(c, req->data_len);
+
+	return 0;
+}
+
+static blk_status_t i10_host_setup_cmd_pdu(struct nvme_ns *ns,
+		struct request *rq)
+{
+	struct i10_host_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+	struct i10_host_queue *queue = req->queue;
+	u8 hdgst = i10_host_hdgst_len(queue), ddgst = 0;
+	blk_status_t ret;
+
+	ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+	if (ret)
+		return ret;
+
+	req->state = I10_HOST_SEND_CMD_PDU;
+	req->offset = 0;
+	req->data_sent = 0;
+	req->pdu_len = 0;
+	req->pdu_sent = 0;
+	req->data_len = blk_rq_nr_phys_segments(rq) ?
+				blk_rq_payload_bytes(rq) : 0;
+	req->curr_bio = rq->bio;
+
+	if (rq_data_dir(rq) == WRITE &&
+	    req->data_len <= i10_host_inline_data_size(queue))
+		req->pdu_len = req->data_len;
+	else if (req->curr_bio)
+		i10_host_init_iter(req, READ);
+
+	pdu->hdr.type = nvme_tcp_cmd;
+	pdu->hdr.flags = 0;
+	if (queue->hdr_digest)
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+	if (queue->data_digest && req->pdu_len) {
+		pdu->hdr.flags |= NVME_TCP_F_DDGST;
+		ddgst = i10_host_ddgst_len(queue);
+	}
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.pdo = req->pdu_len ? pdu->hdr.hlen + hdgst : 0;
+	pdu->hdr.plen =
+		cpu_to_le32(pdu->hdr.hlen + hdgst + req->pdu_len + ddgst);
+
+	ret = i10_host_map_data(queue, rq);
+	if (unlikely(ret)) {
+		nvme_cleanup_cmd(rq);
+		dev_err(queue->ctrl->ctrl.device,
+			"Failed to map data (%d)\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static blk_status_t i10_host_queue_rq(struct blk_mq_hw_ctx *hctx,
+		const struct blk_mq_queue_data *bd)
+{
+	struct nvme_ns *ns = hctx->queue->queuedata;
+	struct i10_host_queue *queue = hctx->driver_data;
+	struct request *rq = bd->rq;
+	struct i10_host_request *req = blk_mq_rq_to_pdu(rq);
+	bool queue_ready = test_bit(I10_HOST_Q_LIVE, &queue->flags);
+	blk_status_t ret;
+
+	if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+
+	ret = i10_host_setup_cmd_pdu(ns, rq);
+	if (unlikely(ret))
+		return ret;
+
+	blk_mq_start_request(rq);
+
+	i10_host_queue_request(req);
+
+	return BLK_STS_OK;
+}
+
+static int i10_host_map_queues(struct blk_mq_tag_set *set)
+{
+	struct i10_host_ctrl *ctrl = set->driver_data;
+	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+
+	if (opts->nr_write_queues && ctrl->io_queues[HCTX_TYPE_READ]) {
+		/* separate read/write queues */
+		set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			ctrl->io_queues[HCTX_TYPE_DEFAULT];
+		set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		set->map[HCTX_TYPE_READ].nr_queues =
+			ctrl->io_queues[HCTX_TYPE_READ];
+		set->map[HCTX_TYPE_READ].queue_offset =
+			ctrl->io_queues[HCTX_TYPE_DEFAULT];
+	} else {
+		/* shared read/write queues */
+		set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			ctrl->io_queues[HCTX_TYPE_DEFAULT];
+		set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		set->map[HCTX_TYPE_READ].nr_queues =
+			ctrl->io_queues[HCTX_TYPE_DEFAULT];
+		set->map[HCTX_TYPE_READ].queue_offset = 0;
+	}
+	blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+	blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+
+	if (opts->nr_poll_queues && ctrl->io_queues[HCTX_TYPE_POLL]) {
+		/* map dedicated poll queues only if we have queues left */
+		set->map[HCTX_TYPE_POLL].nr_queues =
+				ctrl->io_queues[HCTX_TYPE_POLL];
+		set->map[HCTX_TYPE_POLL].queue_offset =
+			ctrl->io_queues[HCTX_TYPE_DEFAULT] +
+			ctrl->io_queues[HCTX_TYPE_READ];
+		blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+	}
+
+	dev_info(ctrl->ctrl.device,
+		"mapped %d/%d/%d default/read/poll queues.\n",
+		ctrl->io_queues[HCTX_TYPE_DEFAULT],
+		ctrl->io_queues[HCTX_TYPE_READ],
+		ctrl->io_queues[HCTX_TYPE_POLL]);
+
+	return 0;
+}
+
+static int i10_host_poll(struct blk_mq_hw_ctx *hctx)
+{
+	struct i10_host_queue *queue = hctx->driver_data;
+	struct sock *sk = queue->sock->sk;
+
+	if (sk_can_busy_loop(sk) && skb_queue_empty_lockless(&sk->sk_receive_queue))
+		sk_busy_loop(sk, true);
+	i10_host_try_recv(queue);
+	return queue->nr_cqe;
+}
+
+static struct blk_mq_ops i10_host_mq_ops = {
+	.queue_rq	= i10_host_queue_rq,
+	.complete	= nvme_complete_rq,
+	.init_request	= i10_host_init_request,
+	.exit_request	= i10_host_exit_request,
+	.init_hctx	= i10_host_init_hctx,
+	.timeout	= i10_host_timeout,
+	.map_queues	= i10_host_map_queues,
+	.poll		= i10_host_poll,
+};
+
+static struct blk_mq_ops i10_host_admin_mq_ops = {
+	.queue_rq	= i10_host_queue_rq,
+	.complete	= nvme_complete_rq,
+	.init_request	= i10_host_init_request,
+	.exit_request	= i10_host_exit_request,
+	.init_hctx	= i10_host_init_admin_hctx,
+	.timeout	= i10_host_timeout,
+};
+
+static const struct nvme_ctrl_ops i10_host_ctrl_ops = {
+	.name			= "i10",
+	.module			= THIS_MODULE,
+	.flags			= NVME_F_FABRICS,
+	.reg_read32		= nvmf_reg_read32,
+	.reg_read64		= nvmf_reg_read64,
+	.reg_write32		= nvmf_reg_write32,
+	.free_ctrl		= i10_host_free_ctrl,
+	.submit_async_event	= i10_host_submit_async_event,
+	.delete_ctrl		= i10_host_delete_ctrl,
+	.get_address		= nvmf_get_address,
+};
+
+static bool
+i10_host_existing_controller(struct nvmf_ctrl_options *opts)
+{
+	struct i10_host_ctrl *ctrl;
+	bool found = false;
+
+	mutex_lock(&i10_host_ctrl_mutex);
+	list_for_each_entry(ctrl, &i10_host_ctrl_list, list) {
+		found = nvmf_ip_options_match(&ctrl->ctrl, opts);
+		if (found)
+			break;
+	}
+	mutex_unlock(&i10_host_ctrl_mutex);
+
+	return found;
+}
+
+static struct nvme_ctrl *i10_host_create_ctrl(struct device *dev,
+		struct nvmf_ctrl_options *opts)
+{
+	struct i10_host_ctrl *ctrl;
+	int ret;
+
+	ctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);
+	if (!ctrl)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&ctrl->list);
+	ctrl->ctrl.opts = opts;
+	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
+				opts->nr_poll_queues + 1;
+	ctrl->ctrl.sqsize = opts->queue_size - 1;
+	ctrl->ctrl.kato = opts->kato;
+
+	INIT_DELAYED_WORK(&ctrl->connect_work,
+			i10_host_reconnect_ctrl_work);
+	INIT_WORK(&ctrl->err_work, i10_host_error_recovery_work);
+	INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+
+	if (!(opts->mask & NVMF_OPT_TRSVCID)) {
+		opts->trsvcid =
+			kstrdup(__stringify(NVME_TCP_DISC_PORT), GFP_KERNEL);
+		if (!opts->trsvcid) {
+			ret = -ENOMEM;
+			goto out_free_ctrl;
+		}
+		opts->mask |= NVMF_OPT_TRSVCID;
+	}
+
+	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
+			opts->traddr, opts->trsvcid, &ctrl->addr);
+	if (ret) {
+		pr_err("malformed address passed: %s:%s\n",
+			opts->traddr, opts->trsvcid);
+		goto out_free_ctrl;
+	}
+
+	if (opts->mask & NVMF_OPT_HOST_TRADDR) {
+		ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
+			opts->host_traddr, NULL, &ctrl->src_addr);
+		if (ret) {
+			pr_err("malformed src address passed: %s\n",
+			       opts->host_traddr);
+			goto out_free_ctrl;
+		}
+	}
+
+	if (!opts->duplicate_connect && i10_host_existing_controller(opts)) {
+		ret = -EALREADY;
+		goto out_free_ctrl;
+	}
+
+	ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+				GFP_KERNEL);
+	if (!ctrl->queues) {
+		ret = -ENOMEM;
+		goto out_free_ctrl;
+	}
+
+	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &i10_host_ctrl_ops, 0);
+	if (ret)
+		goto out_kfree_queues;
+
+	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+		WARN_ON_ONCE(1);
+		ret = -EINTR;
+		goto out_uninit_ctrl;
+	}
+
+	ret = i10_host_setup_ctrl(&ctrl->ctrl, true);
+	if (ret)
+		goto out_uninit_ctrl;
+
+	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISp\n",
+		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
+
+	nvme_get_ctrl(&ctrl->ctrl);
+
+	mutex_lock(&i10_host_ctrl_mutex);
+	list_add_tail(&ctrl->list, &i10_host_ctrl_list);
+	mutex_unlock(&i10_host_ctrl_mutex);
+
+	return &ctrl->ctrl;
+
+out_uninit_ctrl:
+	nvme_uninit_ctrl(&ctrl->ctrl);
+	nvme_put_ctrl(&ctrl->ctrl);
+	if (ret > 0)
+		ret = -EIO;
+	return ERR_PTR(ret);
+out_kfree_queues:
+	kfree(ctrl->queues);
+out_free_ctrl:
+	kfree(ctrl);
+	return ERR_PTR(ret);
+}
+
+static struct nvmf_transport_ops i10_host_transport = {
+	.name		= "i10",
+	.module		= THIS_MODULE,
+	.required_opts	= NVMF_OPT_TRADDR,
+	.allowed_opts	= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |
+			  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO |
+			  NVMF_OPT_HDR_DIGEST | NVMF_OPT_DATA_DIGEST |
+			  NVMF_OPT_NR_WRITE_QUEUES | NVMF_OPT_NR_POLL_QUEUES |
+			  NVMF_OPT_TOS,
+	.create_ctrl	= i10_host_create_ctrl,
+};
+
+static int __init i10_host_init_module(void)
+{
+	i10_host_wq = alloc_workqueue("i10_host_wq",
+			WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
+	if (!i10_host_wq)
+		return -ENOMEM;
+
+	nvmf_register_transport(&i10_host_transport);
+	return 0;
+}
+
+static void __exit i10_host_cleanup_module(void)
+{
+	struct i10_host_ctrl *ctrl;
+
+	nvmf_unregister_transport(&i10_host_transport);
+
+	mutex_lock(&i10_host_ctrl_mutex);
+	list_for_each_entry(ctrl, &i10_host_ctrl_list, list)
+		nvme_delete_ctrl(&ctrl->ctrl);
+	mutex_unlock(&i10_host_ctrl_mutex);
+	flush_workqueue(nvme_delete_wq);
+
+	destroy_workqueue(i10_host_wq);
+}
+
+module_init(i10_host_init_module);
+module_exit(i10_host_cleanup_module);
+
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 73e8475..e219f5b 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -311,6 +311,8 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 
 	BUG_ON(hctx_idx >= ctrl->ctrl.queue_count);
 
+	/* blk-switch */
+	hctx->blk_switch = 2;
 	hctx->driver_data = queue;
 	return 0;
 }
diff --git a/drivers/nvme/host/tcp.c b/drivers/nvme/host/tcp.c
index 11e84ed..b832f8e 100644
--- a/drivers/nvme/host/tcp.c
+++ b/drivers/nvme/host/tcp.c
@@ -15,9 +15,26 @@
 #include <crypto/hash.h>
 #include <net/busy_poll.h>
 
+#include <linux/bio.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+
 #include "nvme.h"
 #include "fabrics.h"
 
+#define I10_CARAVAN_CAPACITY		65536
+#define I10_AGGREGATION_SIZE		16
+#define I10_MAX_AGGREGATION_SIZE	32
+#define I10_MIN_DOORBELL_TIMEOUT	25
+
+static int i10_delayed_doorbell_us __read_mostly = 50;
+module_param(i10_delayed_doorbell_us, int, 0644);
+MODULE_PARM_DESC(i10_delayed_doorbell_us, "i10 delayed doorbell timer (us) for thru");
+
+static int i10_thru_nice __read_mostly = -19;
+module_param(i10_thru_nice, int, 0644);
+MODULE_PARM_DESC(i10_thru_nice, "i10 thru kthread nice");
+
 struct nvme_tcp_queue;
 
 enum nvme_tcp_send_state {
@@ -64,6 +81,10 @@ struct nvme_tcp_queue {
 	struct work_struct	io_work;
 	int			io_cpu;
 
+	/* for blk-switch */
+        struct work_struct      io_work_lat;
+        int                     prio_class;
+
 	spinlock_t		lock;
 	struct list_head	send_list;
 
@@ -91,6 +112,21 @@ struct nvme_tcp_queue {
 	__le32			exp_ddgst;
 	__le32			recv_ddgst;
 
+	/* For i10 caravans */
+	struct kvec		*caravan_iovs;
+	size_t			caravan_len;
+	int			nr_iovs;
+	bool			send_now;
+	int			nr_caravan_req;
+
+	struct page		**caravan_mapped;
+	int			nr_mapped;
+
+	/* For i10 delayed doorbells */
+	atomic_t		nr_req;
+	struct hrtimer		doorbell_timer;
+	atomic_t		timer_set;
+
 	struct page_frag_cache	pf_cache;
 
 	void (*state_change)(struct sock *);
@@ -119,6 +155,7 @@ struct nvme_tcp_ctrl {
 static LIST_HEAD(nvme_tcp_ctrl_list);
 static DEFINE_MUTEX(nvme_tcp_ctrl_mutex);
 static struct workqueue_struct *nvme_tcp_wq;
+static struct workqueue_struct *nvme_tcp_wq_lat;
 static struct blk_mq_ops nvme_tcp_mq_ops;
 static struct blk_mq_ops nvme_tcp_admin_mq_ops;
 
@@ -247,15 +284,84 @@ static inline void nvme_tcp_advance_req(struct nvme_tcp_request *req,
 	}
 }
 
+static inline bool i10_host_is_latency(struct nvme_tcp_queue *queue)
+{
+	return queue->prio_class == 1;
+}
+
+static inline bool i10_host_legacy_path(struct nvme_tcp_request *req)
+{
+	struct nvme_tcp_queue *queue = req->queue;
+
+	return nvme_tcp_queue_id(req->queue) == 0 ||
+		i10_host_is_latency(queue) ||
+		(i10_delayed_doorbell_us < I10_MIN_DOORBELL_TIMEOUT);
+}
+
+#define I10_ALLOWED_FLAGS (REQ_OP_READ | REQ_OP_WRITE | REQ_DRV | \
+		REQ_RAHEAD | REQ_SYNC | REQ_IDLE | REQ_NOMERGE)
+
+static inline bool i10_host_is_nodelay_path(struct nvme_tcp_request *req)
+{
+	return (req->curr_bio == NULL) ||
+		(req->curr_bio->bi_opf & ~I10_ALLOWED_FLAGS);
+}
+
+/* blk-switch: i10 implementation */
 static inline void nvme_tcp_queue_request(struct nvme_tcp_request *req)
 {
 	struct nvme_tcp_queue *queue = req->queue;
+	bool no_delay = true;
+
+	if (nvme_tcp_queue_id(queue) <= num_online_cpus())
+		queue->prio_class = 0;
+	else
+		queue->prio_class = 1;
 
 	spin_lock(&queue->lock);
+
 	list_add_tail(&req->entry, &queue->send_list);
+	if (!i10_host_legacy_path(req) && !i10_host_is_nodelay_path(req)) {
+		atomic_inc(&queue->nr_req);
+
+		if (atomic_read(&queue->nr_req) == 1 &&
+		   !hrtimer_active(&queue->doorbell_timer)) {
+			hrtimer_start(&queue->doorbell_timer,
+				ns_to_ktime(i10_delayed_doorbell_us *
+					NSEC_PER_USEC),
+				HRTIMER_MODE_REL);
+			atomic_inc(&queue->timer_set);
+		}
+
+		no_delay = false;
+	}
+
 	spin_unlock(&queue->lock);
 
-	queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+	if (!no_delay) {
+		if (atomic_read(&queue->nr_req) >= I10_AGGREGATION_SIZE) {
+			if (hrtimer_active(&queue->doorbell_timer))	
+				hrtimer_cancel(&queue->doorbell_timer);
+
+			atomic_set(&queue->timer_set, 0);
+
+			if (i10_host_is_latency(queue))
+				queue_work_on(queue->io_cpu, nvme_tcp_wq_lat, &queue->io_work_lat);
+			else
+				queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+		}
+	}
+	else {
+		if (hrtimer_active(&queue->doorbell_timer))
+			hrtimer_cancel(&queue->doorbell_timer);
+
+		atomic_set(&queue->timer_set, 0);
+
+		if (i10_host_is_latency(queue))
+			queue_work_on(queue->io_cpu, nvme_tcp_wq_lat, &queue->io_work_lat);
+		else
+			queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+	}
 }
 
 static inline struct nvme_tcp_request *
@@ -266,8 +372,11 @@ nvme_tcp_fetch_request(struct nvme_tcp_queue *queue)
 	spin_lock(&queue->lock);
 	req = list_first_entry_or_null(&queue->send_list,
 			struct nvme_tcp_request, entry);
-	if (req)
+	if (req) {
 		list_del(&req->entry);
+		if (!i10_host_legacy_path(req) && !i10_host_is_nodelay_path(req))
+			queue->nr_caravan_req++;
+	}
 	spin_unlock(&queue->lock);
 
 	return req;
@@ -384,6 +493,7 @@ static int nvme_tcp_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	struct nvme_tcp_ctrl *ctrl = data;
 	struct nvme_tcp_queue *queue = &ctrl->queues[hctx_idx + 1];
 
+	hctx->blk_switch = 1;
 	hctx->driver_data = queue;
 	return 0;
 }
@@ -786,8 +896,12 @@ static void nvme_tcp_data_ready(struct sock *sk)
 
 	read_lock(&sk->sk_callback_lock);
 	queue = sk->sk_user_data;
-	if (likely(queue && queue->rd_enabled))
-		queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+	if (likely(queue && queue->rd_enabled)) {
+		if (i10_host_is_latency(queue))
+			queue_work_on(queue->io_cpu, nvme_tcp_wq_lat, &queue->io_work_lat);
+		else
+			queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+	}
 	read_unlock(&sk->sk_callback_lock);
 }
 
@@ -799,7 +913,10 @@ static void nvme_tcp_write_space(struct sock *sk)
 	queue = sk->sk_user_data;
 	if (likely(queue && sk_stream_is_writeable(sk))) {
 		clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
-		queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+		if (i10_host_is_latency(queue))
+			queue_work_on(queue->io_cpu, nvme_tcp_wq_lat, &queue->io_work_lat);
+		else
+			queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
 	}
 	read_unlock_bh(&sk->sk_callback_lock);
 }
@@ -843,6 +960,14 @@ static void nvme_tcp_fail_request(struct nvme_tcp_request *req)
 	nvme_tcp_end_request(blk_mq_rq_from_pdu(req), NVME_SC_HOST_PATH_ERROR);
 }
 
+/* blk-switch: i10 */
+static inline bool i10_host_is_caravan_full(struct nvme_tcp_queue *queue)
+{
+	return (queue->caravan_len >= I10_CARAVAN_CAPACITY) ||
+		(queue->nr_iovs >= I10_MAX_AGGREGATION_SIZE * 2) ||
+		(queue->nr_mapped >= I10_MAX_AGGREGATION_SIZE);
+}
+
 static int nvme_tcp_try_send_data(struct nvme_tcp_request *req)
 {
 	struct nvme_tcp_queue *queue = req->queue;
@@ -859,14 +984,31 @@ static int nvme_tcp_try_send_data(struct nvme_tcp_request *req)
 		else
 			flags |= MSG_MORE;
 
-		/* can't zcopy slab pages */
-		if (unlikely(PageSlab(page))) {
-			ret = sock_no_sendpage(queue->sock, page, offset, len,
-					flags);
-		} else {
-			ret = kernel_sendpage(queue->sock, page, offset, len,
-					flags);
+		/* blk-switch: i10 */
+		if (i10_host_legacy_path(req)) {
+			/* can't zcopy slab pages */
+			if (unlikely(PageSlab(page))) {
+				ret = sock_no_sendpage(queue->sock, page, offset, len,
+						flags);
+			} else {
+				ret = kernel_sendpage(queue->sock, page, offset, len,
+						flags);
+			}
 		}
+		else {
+			if (i10_host_is_caravan_full(queue)) {
+				queue->send_now = true;
+				return 1;
+			}
+			/* Caravans: I/O data aggregation */
+			queue->caravan_iovs[queue->nr_iovs].iov_base =
+				kmap(page) + offset;
+			queue->caravan_iovs[queue->nr_iovs++].iov_len = len;
+			queue->caravan_mapped[queue->nr_mapped++] = page;
+			queue->caravan_len += len;
+			ret = len;
+		}
+
 		if (ret <= 0)
 			return ret;
 
@@ -904,8 +1046,26 @@ static int nvme_tcp_try_send_cmd_pdu(struct nvme_tcp_request *req)
 	if (queue->hdr_digest && !req->offset)
 		nvme_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
 
-	ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
+	/* blk-switch: i10 */
+	if (i10_host_legacy_path(req))
+		ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
 			offset_in_page(pdu) + req->offset, len,  flags);
+	else {
+		if (i10_host_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		/* Caravans: command PDU aggregation */
+		queue->caravan_iovs[queue->nr_iovs].iov_base = pdu
+			+ req->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = len;
+		queue->caravan_len += len;
+		ret = len;
+
+		if (i10_host_is_nodelay_path(req))
+			queue->send_now = true;
+	}
+
 	if (unlikely(ret <= 0))
 		return ret;
 
@@ -937,9 +1097,24 @@ static int nvme_tcp_try_send_data_pdu(struct nvme_tcp_request *req)
 	if (queue->hdr_digest && !req->offset)
 		nvme_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
 
-	ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
+	/* blk-switch: i10 */
+	if (i10_host_legacy_path(req))
+		ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
 			offset_in_page(pdu) + req->offset, len,
 			MSG_DONTWAIT | MSG_MORE);
+	else {
+		if (i10_host_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		/* Caravans: data PDU aggregation */
+		queue->caravan_iovs[queue->nr_iovs].iov_base = pdu
+			+ req->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = len;
+		queue->caravan_len += len;
+		ret = len;
+	}
+
 	if (unlikely(ret <= 0))
 		return ret;
 
@@ -980,17 +1155,84 @@ static int nvme_tcp_try_send_ddgst(struct nvme_tcp_request *req)
 	return -EAGAIN;
 }
 
+/* i10: to check if there's enough room in tcp_sndbuf */
+static inline int nvme_tcp_sndbuf_nospace(struct nvme_tcp_queue *queue,
+		int length)
+{
+	return sk_stream_wspace(queue->sock->sk) < length;
+}
+
 static int nvme_tcp_try_send(struct nvme_tcp_queue *queue)
 {
 	struct nvme_tcp_request *req;
 	int ret = 1;
 
 	if (!queue->request) {
-		queue->request = nvme_tcp_fetch_request(queue);
-		if (!queue->request)
+		spin_lock(&queue->lock);
+
+		req = list_first_entry_or_null(&queue->send_list,
+			struct nvme_tcp_request, entry);
+
+		if (req) {
+			list_del(&req->entry);
+
+			if (!i10_host_legacy_path(req) &&
+			   !i10_host_is_nodelay_path(req))
+				queue->nr_caravan_req++;
+
+			queue->request = req;
+		}
+		else if ((!atomic_read(&queue->timer_set) ||
+			!hrtimer_active(&queue->doorbell_timer)) &&
+			queue->caravan_len) {
+			atomic_sub(queue->nr_caravan_req, &queue->nr_req);
+			queue->nr_caravan_req = 0;
+			atomic_set(&queue->timer_set, 0);
+			queue->send_now = true;
+		}			
+
+		spin_unlock(&queue->lock);
+
+		if (!queue->request && !queue->caravan_len)
 			return 0;
 	}
-	req = queue->request;
+
+	/* blk-switch: i10 pdu aggregation */
+	if (queue->send_now) {
+		struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_EOR };
+		int i, i10_ret;
+
+		if (nvme_tcp_sndbuf_nospace(queue, queue->caravan_len)) {
+			set_bit(SOCK_NOSPACE,
+				&queue->sock->sk->sk_socket->flags);
+			return 0;
+		}
+
+		i10_ret = kernel_sendmsg(queue->sock, &msg,
+				queue->caravan_iovs,
+				queue->nr_iovs,
+				queue->caravan_len);
+
+		if (i10_ret <= 0) {
+			dev_err(queue->ctrl->ctrl.device,
+				"blk-switch: i10 kernel_sendmsg fails (i10_ret %d)\n",
+				i10_ret);
+			return i10_ret;
+		}
+
+		for (i = 0; i < queue->nr_mapped; i++)
+			kunmap(queue->caravan_mapped[i]);
+
+		queue->nr_iovs = 0;
+		queue->nr_mapped = 0;
+		queue->caravan_len = 0;
+		queue->send_now = false;
+	}
+
+	if (queue->request)
+		req = queue->request;
+	else
+		return 0;
 
 	if (req->state == NVME_TCP_SEND_CMD_PDU) {
 		ret = nvme_tcp_try_send_cmd_pdu(req);
@@ -1036,12 +1278,33 @@ static int nvme_tcp_try_recv(struct nvme_tcp_queue *queue)
 	return consumed;
 }
 
+/* blk-switch: i10 doorbell timer handler */
+enum hrtimer_restart i10_host_doorbell_timeout(struct hrtimer *timer)
+{
+	struct nvme_tcp_queue *queue =
+		container_of(timer, struct nvme_tcp_queue, doorbell_timer);
+
+	atomic_set(&queue->timer_set, 0);
+
+	if (i10_host_is_latency(queue))
+		queue_work_on(queue->io_cpu, nvme_tcp_wq_lat, &queue->io_work_lat);
+	else
+		queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+
+	return HRTIMER_NORESTART;
+}
+
+/* blk-switch: io_work for thru-requests */
 static void nvme_tcp_io_work(struct work_struct *w)
 {
 	struct nvme_tcp_queue *queue =
 		container_of(w, struct nvme_tcp_queue, io_work);
 	unsigned long deadline = jiffies + msecs_to_jiffies(1);
 
+	/* blk-switch: set thru kthread's nice value */
+	if (task_nice(current) != i10_thru_nice)
+		set_user_nice(current, i10_thru_nice);
+
 	do {
 		bool pending = false;
 		int result;
@@ -1075,6 +1338,46 @@ static void nvme_tcp_io_work(struct work_struct *w)
 	queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
 }
 
+/* blk-switch: io_work_lat for lat-requests */
+static void nvme_tcp_io_work_lat(struct work_struct *w)
+{
+	struct nvme_tcp_queue *queue =
+		container_of(w, struct nvme_tcp_queue, io_work_lat);
+	unsigned long deadline = jiffies + msecs_to_jiffies(1);
+
+	do {
+		bool pending = false;
+		int result;
+
+		result = nvme_tcp_try_send(queue);
+		if (result > 0) {
+			pending = true;
+		} else if (unlikely(result < 0)) {
+			dev_err(queue->ctrl->ctrl.device,
+				"failed to send request %d\n", result);
+
+			/*
+			 * Fail the request unless peer closed the connection,
+			 * in which case error recovery flow will complete all.
+			 */
+			if ((result != -EPIPE) && (result != -ECONNRESET))
+				nvme_tcp_fail_request(queue->request);
+			nvme_tcp_done_send_req(queue);
+			return;
+		}
+
+		result = nvme_tcp_try_recv(queue);
+		if (result > 0)
+			pending = true;
+
+		if (!pending)
+			return;
+
+	} while (!time_after(jiffies, deadline)); /* quota is exhausted */
+
+	queue_work_on(queue->io_cpu, nvme_tcp_wq_lat, &queue->io_work_lat);
+}
+
 static void nvme_tcp_free_crypto(struct nvme_tcp_queue *queue)
 {
 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(queue->rcv_hash);
@@ -1146,6 +1449,10 @@ static void nvme_tcp_free_queue(struct nvme_ctrl *nctrl, int qid)
 
 	sock_release(queue->sock);
 	kfree(queue->pdu);
+	/* i10 */
+	kfree(queue->caravan_iovs);
+	kfree(queue->caravan_mapped);
+	hrtimer_cancel(&queue->doorbell_timer);
 }
 
 static int nvme_tcp_init_connection(struct nvme_tcp_queue *queue)
@@ -1258,6 +1565,7 @@ static int nvme_tcp_alloc_queue(struct nvme_ctrl *nctrl,
 	INIT_LIST_HEAD(&queue->send_list);
 	spin_lock_init(&queue->lock);
 	INIT_WORK(&queue->io_work, nvme_tcp_io_work);
+	INIT_WORK(&queue->io_work_lat, nvme_tcp_io_work_lat);
 	queue->queue_size = queue_size;
 
 	if (qid > 0)
@@ -1266,6 +1574,31 @@ static int nvme_tcp_alloc_queue(struct nvme_ctrl *nctrl,
 		queue->cmnd_capsule_len = sizeof(struct nvme_command) +
 						NVME_TCP_ADMIN_CCSZ;
 
+	/* i10: alloc for pdu aggregation */
+	queue->caravan_iovs = kcalloc(I10_MAX_AGGREGATION_SIZE * 2,
+				sizeof(*queue->caravan_iovs), GFP_KERNEL);
+	if (!queue->caravan_iovs) {
+		ret = -ENOMEM;
+		goto err_sock;
+	}
+	queue->caravan_mapped = kcalloc(I10_MAX_AGGREGATION_SIZE,
+				sizeof(**queue->caravan_mapped), GFP_KERNEL);
+	if (!queue->caravan_mapped) {
+		ret = -ENOMEM;
+		goto err_sock;
+	}
+
+	atomic_set(&queue->nr_req, 0);
+	atomic_set(&queue->timer_set, 0);
+	queue->nr_iovs = 0;
+	queue->nr_caravan_req = 0;
+	queue->nr_mapped = 0;
+	queue->caravan_len = 0;
+	queue->send_now = false;
+
+	hrtimer_init(&queue->doorbell_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	queue->doorbell_timer.function = &i10_host_doorbell_timeout;
+
 	ret = sock_create(ctrl->addr.ss_family, SOCK_STREAM,
 			IPPROTO_TCP, &queue->sock);
 	if (ret) {
@@ -1294,6 +1627,27 @@ static int nvme_tcp_alloc_queue(struct nvme_ctrl *nctrl,
 		goto err_sock;
 	}
 
+	/* i10: Set a large sndbuf/rcvbuf */
+	opt = 8388608;
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_SNDBUFFORCE,
+		(char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_SNDBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_RCVBUFFORCE,
+		(char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_RCVBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/* Set a higher priority to lat-socket */
+	
+
 	/*
 	 * Cleanup whatever is sitting in the TCP transmit queue on socket
 	 * close. This is done to prevent stale data from being sent should
@@ -1426,6 +1780,8 @@ static void __nvme_tcp_stop_queue(struct nvme_tcp_queue *queue)
 	kernel_sock_shutdown(queue->sock, SHUT_RDWR);
 	nvme_tcp_restore_sock_calls(queue);
 	cancel_work_sync(&queue->io_work);
+	/* i10 */
+	cancel_work_sync(&queue->io_work_lat);
 }
 
 static void nvme_tcp_stop_queue(struct nvme_ctrl *nctrl, int qid)
@@ -2395,11 +2751,18 @@ static struct nvmf_transport_ops nvme_tcp_transport = {
 
 static int __init nvme_tcp_init_module(void)
 {
+	/* blk-switch: workqueue for thru-apps */
 	nvme_tcp_wq = alloc_workqueue("nvme_tcp_wq",
-			WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
+			WQ_MEM_RECLAIM, 0);
 	if (!nvme_tcp_wq)
 		return -ENOMEM;
 
+	/* blk-switch: workqueue for lat-apps */
+	nvme_tcp_wq_lat = alloc_workqueue("nvme_tcp_wq_lat",
+			WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
+	if (!nvme_tcp_wq_lat)
+		return -ENOMEM;
+
 	nvmf_register_transport(&nvme_tcp_transport);
 	return 0;
 }
@@ -2417,6 +2780,7 @@ static void __exit nvme_tcp_cleanup_module(void)
 	flush_workqueue(nvme_delete_wq);
 
 	destroy_workqueue(nvme_tcp_wq);
+	destroy_workqueue(nvme_tcp_wq_lat);
 }
 
 module_init(nvme_tcp_init_module);
diff --git a/drivers/nvme/target/Kconfig b/drivers/nvme/target/Kconfig
index d7f48c0..e998c3a 100644
--- a/drivers/nvme/target/Kconfig
+++ b/drivers/nvme/target/Kconfig
@@ -72,3 +72,14 @@ config NVME_TARGET_TCP
 	  devices over TCP.
 
 	  If unsure, say N.
+
+config I10_TARGET
+	tristate "i10: A New Remote Storage I/O Stack (target)"
+	depends on INET
+	depends on NVME_TARGET
+	help
+	  This enables the i10 target support, which currently implemented as
+	  a new fabric of the NVMe over Fabrics target.
+	  (https://github.com/i10-kernel/i10-implementation)
+
+	  If unsure, say N.
diff --git a/drivers/nvme/target/Makefile b/drivers/nvme/target/Makefile
index 2b33836..faae1b4 100644
--- a/drivers/nvme/target/Makefile
+++ b/drivers/nvme/target/Makefile
@@ -8,6 +8,7 @@ obj-$(CONFIG_NVME_TARGET_RDMA)		+= nvmet-rdma.o
 obj-$(CONFIG_NVME_TARGET_FC)		+= nvmet-fc.o
 obj-$(CONFIG_NVME_TARGET_FCLOOP)	+= nvme-fcloop.o
 obj-$(CONFIG_NVME_TARGET_TCP)		+= nvmet-tcp.o
+obj-$(CONFIG_I10_TARGET)		+= i10-target.o
 
 nvmet-y		+= core.o configfs.o admin-cmd.o fabrics-cmd.o \
 			discovery.o io-cmd-file.o io-cmd-bdev.o
@@ -16,4 +17,5 @@ nvmet-rdma-y	+= rdma.o
 nvmet-fc-y	+= fc.o
 nvme-fcloop-y	+= fcloop.o
 nvmet-tcp-y	+= tcp.o
+i10-target-y	+= i10.o
 nvmet-$(CONFIG_TRACING)	+= trace.o
diff --git a/drivers/nvme/target/configfs.c b/drivers/nvme/target/configfs.c
index 98613a4..364ca04 100644
--- a/drivers/nvme/target/configfs.c
+++ b/drivers/nvme/target/configfs.c
@@ -27,6 +27,7 @@ static const struct nvmet_transport_name {
 	{ NVMF_TRTYPE_RDMA,	"rdma" },
 	{ NVMF_TRTYPE_FC,	"fc" },
 	{ NVMF_TRTYPE_TCP,	"tcp" },
+	{ NVMF_TRTYPE_I10,	"i10" },
 	{ NVMF_TRTYPE_LOOP,	"loop" },
 };
 
diff --git a/drivers/nvme/target/i10.c b/drivers/nvme/target/i10.c
new file mode 100644
index 0000000..dc6a84b
--- /dev/null
+++ b/drivers/nvme/target/i10.c
@@ -0,0 +1,1964 @@
+/*
+ *	TCP~~RDMA: CPU-efficient Remote Storage Access
+ *			with i10
+ *		- i10 target implementation
+ *		(inspired by drivers/nvme/target/tcp.c)
+ *
+ *	Authors:
+ *		Jaehyun Hwang <jaehyun.hwang@cornell.edu>
+ *		Qizhe Cai <qc228@cornell.edu>
+ *		A. Kevin Tang <atang@cornell.edu>
+ *		Rachit Agarwal <ragarwal@cs.cornell.edu>
+ *
+ *	SPDX-License-Identifier: GPL-2.0
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/nvme-tcp.h>
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <linux/inet.h>
+#include <linux/llist.h>
+#include <crypto/hash.h>
+
+#include "nvmet.h"
+
+#define I10_TARGET_DEF_INLINE_DATA_SIZE	(4 * PAGE_SIZE)
+
+#define I10_CARAVAN_CAPACITY		65536
+#define I10_TARGET_RECV_BUDGET		16
+#define I10_TARGET_SEND_BUDGET		16
+#define I10_TARGET_IO_WORK_BUDGET	64
+
+enum i10_target_send_state {
+	I10_TARGET_SEND_DATA_PDU,
+	I10_TARGET_SEND_DATA,
+	I10_TARGET_SEND_R2T,
+	I10_TARGET_SEND_DDGST,
+	I10_TARGET_SEND_RESPONSE
+};
+
+enum i10_target_recv_state {
+	I10_TARGET_RECV_PDU,
+	I10_TARGET_RECV_DATA,
+	I10_TARGET_RECV_DDGST,
+	I10_TARGET_RECV_ERR,
+};
+
+enum {
+	I10_TARGET_F_INIT_FAILED = (1 << 0),
+};
+
+struct i10_target_cmd {
+	struct i10_target_queue		*queue;
+	struct nvmet_req		req;
+
+	struct nvme_tcp_cmd_pdu		*cmd_pdu;
+	struct nvme_tcp_rsp_pdu		*rsp_pdu;
+	struct nvme_tcp_data_pdu	*data_pdu;
+	struct nvme_tcp_r2t_pdu		*r2t_pdu;
+
+	u32				rbytes_done;
+	u32				wbytes_done;
+
+	u32				pdu_len;
+	u32				pdu_recv;
+	int				sg_idx;
+	int				nr_mapped;
+	struct msghdr			recv_msg;
+	struct kvec			*iov;
+	u32				flags;
+
+	struct list_head		entry;
+	struct llist_node		lentry;
+
+	/* send state */
+	u32				offset;
+	struct scatterlist		*cur_sg;
+	enum i10_target_send_state	state;
+
+	__le32				exp_ddgst;
+	__le32				recv_ddgst;
+};
+
+enum i10_target_queue_state {
+	I10_TARGET_Q_CONNECTING,
+	I10_TARGET_Q_LIVE,
+	I10_TARGET_Q_DISCONNECTING,
+};
+
+struct i10_target_cmd_caravan {
+	struct i10_target_cmd	*cmd;
+};
+
+struct i10_target_queue {
+	struct socket		*sock;
+	struct i10_target_port	*port;
+	struct work_struct	io_work;
+	int			cpu;
+	struct nvmet_cq		nvme_cq;
+	struct nvmet_sq		nvme_sq;
+
+	/* send state */
+	struct i10_target_cmd	*cmds;
+	unsigned int		nr_cmds;
+	struct list_head	free_list;
+	struct llist_head	resp_list;
+	struct list_head	resp_send_list;
+	int			send_list_len;
+	struct i10_target_cmd	*snd_cmd;
+
+	/* For i10 target caravans */
+	struct kvec		*caravan_iovs;
+	int			nr_iovs;
+	size_t			caravan_len;
+	struct i10_target_cmd_caravan *caravan_cmds;
+	int			nr_caravan_cmds;
+	bool			send_now;
+
+	struct page		**caravan_mapped;
+	int			nr_caravan_mapped;
+
+	/* recv state */
+	int			offset;
+	int			left;
+	enum i10_target_recv_state rcv_state;
+	struct i10_target_cmd	*cmd;
+	union nvme_tcp_pdu	pdu;
+
+	/* digest state */
+	bool			hdr_digest;
+	bool			data_digest;
+	struct ahash_request	*snd_hash;
+	struct ahash_request	*rcv_hash;
+
+	spinlock_t		state_lock;
+	enum i10_target_queue_state state;
+
+	struct sockaddr_storage	sockaddr;
+	struct sockaddr_storage	sockaddr_peer;
+	struct work_struct	release_work;
+
+	int			idx;
+	struct list_head	queue_list;
+
+	struct i10_target_cmd	connect;
+
+	struct page_frag_cache	pf_cache;
+
+	void (*data_ready)(struct sock *);
+	void (*state_change)(struct sock *);
+	void (*write_space)(struct sock *);
+};
+
+struct i10_target_port {
+	struct socket		*sock;
+	struct work_struct	accept_work;
+	struct nvmet_port	*nport;
+	struct sockaddr_storage addr;
+	int			last_cpu;
+	void (*data_ready)(struct sock *);
+};
+
+static DEFINE_IDA(i10_target_queue_ida);
+static LIST_HEAD(i10_target_queue_list);
+static DEFINE_MUTEX(i10_target_queue_mutex);
+
+static struct workqueue_struct *i10_target_wq;
+static struct nvmet_fabrics_ops i10_target_ops;
+static void i10_target_free_cmd(struct i10_target_cmd *c);
+static void i10_target_finish_cmd(struct i10_target_cmd *cmd);
+
+static inline u16 i10_target_cmd_tag(struct i10_target_queue *queue,
+		struct i10_target_cmd *cmd)
+{
+	return cmd - queue->cmds;
+}
+
+static inline bool i10_target_has_data_in(struct i10_target_cmd *cmd)
+{
+	return nvme_is_write(cmd->req.cmd) &&
+		cmd->rbytes_done < cmd->req.transfer_len;
+}
+
+static inline bool i10_target_need_data_in(struct i10_target_cmd *cmd)
+{
+	return i10_target_has_data_in(cmd) && !cmd->req.cqe->status;
+}
+
+static inline bool i10_target_need_data_out(struct i10_target_cmd *cmd)
+{
+	return !nvme_is_write(cmd->req.cmd) &&
+		cmd->req.transfer_len > 0 &&
+		!cmd->req.cqe->status;
+}
+
+static inline bool i10_target_has_inline_data(struct i10_target_cmd *cmd)
+{
+	return nvme_is_write(cmd->req.cmd) && cmd->pdu_len &&
+		!cmd->rbytes_done;
+}
+
+static inline struct i10_target_cmd *
+i10_target_get_cmd(struct i10_target_queue *queue)
+{
+	struct i10_target_cmd *cmd;
+
+	cmd = list_first_entry_or_null(&queue->free_list,
+				struct i10_target_cmd, entry);
+	if (!cmd)
+		return NULL;
+	list_del_init(&cmd->entry);
+
+	cmd->rbytes_done = cmd->wbytes_done = 0;
+	cmd->pdu_len = 0;
+	cmd->pdu_recv = 0;
+	cmd->iov = NULL;
+	cmd->flags = 0;
+	return cmd;
+}
+
+static inline void i10_target_put_cmd(struct i10_target_cmd *cmd)
+{
+	if (unlikely(cmd == &cmd->queue->connect))
+		return;
+
+	list_add_tail(&cmd->entry, &cmd->queue->free_list);
+}
+
+static inline u8 i10_target_hdgst_len(struct i10_target_queue *queue)
+{
+	return queue->hdr_digest ? NVME_TCP_DIGEST_LENGTH : 0;
+}
+
+static inline u8 i10_target_ddgst_len(struct i10_target_queue *queue)
+{
+	return queue->data_digest ? NVME_TCP_DIGEST_LENGTH : 0;
+}
+
+static inline void i10_target_hdgst(struct ahash_request *hash,
+		void *pdu, size_t len)
+{
+	struct scatterlist sg;
+
+	sg_init_one(&sg, pdu, len);
+	ahash_request_set_crypt(hash, &sg, pdu + len, len);
+	crypto_ahash_digest(hash);
+}
+
+static int i10_target_verify_hdgst(struct i10_target_queue *queue,
+	void *pdu, size_t len)
+{
+	struct nvme_tcp_hdr *hdr = pdu;
+	__le32 recv_digest;
+	__le32 exp_digest;
+
+	if (unlikely(!(hdr->flags & NVME_TCP_F_HDGST))) {
+		pr_err("queue %d: header digest enabled but no header digest\n",
+			queue->idx);
+		return -EPROTO;
+	}
+
+	recv_digest = *(__le32 *)(pdu + hdr->hlen);
+	i10_target_hdgst(queue->rcv_hash, pdu, len);
+	exp_digest = *(__le32 *)(pdu + hdr->hlen);
+	if (recv_digest != exp_digest) {
+		pr_err("queue %d: header digest error: recv %#x expected %#x\n",
+			queue->idx, le32_to_cpu(recv_digest),
+			le32_to_cpu(exp_digest));
+		return -EPROTO;
+	}
+
+	return 0;
+}
+
+static int i10_target_check_ddgst(struct i10_target_queue *queue, void *pdu)
+{
+	struct nvme_tcp_hdr *hdr = pdu;
+	u8 digest_len = i10_target_hdgst_len(queue);
+	u32 len;
+
+	len = le32_to_cpu(hdr->plen) - hdr->hlen -
+		(hdr->flags & NVME_TCP_F_HDGST ? digest_len : 0);
+
+	if (unlikely(len && !(hdr->flags & NVME_TCP_F_DDGST))) {
+		pr_err("queue %d: data digest flag is cleared\n", queue->idx);
+		return -EPROTO;
+	}
+
+	return 0;
+}
+
+static void i10_target_unmap_pdu_iovec(struct i10_target_cmd *cmd)
+{
+	struct scatterlist *sg;
+	int i;
+
+	sg = &cmd->req.sg[cmd->sg_idx];
+
+	for (i = 0; i < cmd->nr_mapped; i++)
+		kunmap(sg_page(&sg[i]));
+}
+
+static void i10_target_map_pdu_iovec(struct i10_target_cmd *cmd)
+{
+	struct kvec *iov = cmd->iov;
+	struct scatterlist *sg;
+	u32 length, offset, sg_offset;
+
+	length = cmd->pdu_len;
+	cmd->nr_mapped = DIV_ROUND_UP(length, PAGE_SIZE);
+	offset = cmd->rbytes_done;
+	cmd->sg_idx = DIV_ROUND_UP(offset, PAGE_SIZE);
+	sg_offset = offset % PAGE_SIZE;
+	sg = &cmd->req.sg[cmd->sg_idx];
+
+	while (length) {
+		u32 iov_len = min_t(u32, length, sg->length - sg_offset);
+
+		iov->iov_base = kmap(sg_page(sg)) + sg->offset + sg_offset;
+		iov->iov_len = iov_len;
+
+		length -= iov_len;
+		sg = sg_next(sg);
+		iov++;
+	}
+
+	iov_iter_kvec(&cmd->recv_msg.msg_iter, READ, cmd->iov,
+		cmd->nr_mapped, cmd->pdu_len);
+}
+
+static void i10_target_fatal_error(struct i10_target_queue *queue)
+{
+	queue->rcv_state = I10_TARGET_RECV_ERR;
+	if (queue->nvme_sq.ctrl)
+		nvmet_ctrl_fatal_error(queue->nvme_sq.ctrl);
+	else
+		kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+}
+
+static int i10_target_map_data(struct i10_target_cmd *cmd)
+{
+	struct nvme_sgl_desc *sgl = &cmd->req.cmd->common.dptr.sgl;
+	u32 len = le32_to_cpu(sgl->length);
+
+	if (!cmd->req.data_len)
+		return 0;
+
+	if (sgl->type == ((NVME_SGL_FMT_DATA_DESC << 4) |
+			  NVME_SGL_FMT_OFFSET)) {
+		if (!nvme_is_write(cmd->req.cmd))
+			return NVME_SC_INVALID_FIELD | NVME_SC_DNR;
+
+		if (len > cmd->req.port->inline_data_size)
+			return NVME_SC_SGL_INVALID_OFFSET | NVME_SC_DNR;
+		cmd->pdu_len = len;
+	}
+	cmd->req.transfer_len += len;
+
+	cmd->req.sg = sgl_alloc(len, GFP_KERNEL, &cmd->req.sg_cnt);
+	if (!cmd->req.sg)
+		return NVME_SC_INTERNAL;
+	cmd->cur_sg = cmd->req.sg;
+
+	if (i10_target_has_data_in(cmd)) {
+		cmd->iov = kmalloc_array(cmd->req.sg_cnt,
+				sizeof(*cmd->iov), GFP_KERNEL);
+		if (!cmd->iov)
+			goto err;
+	}
+
+	return 0;
+err:
+	sgl_free(cmd->req.sg);
+	return NVME_SC_INTERNAL;
+}
+
+static void i10_target_ddgst(struct ahash_request *hash,
+		struct i10_target_cmd *cmd)
+{
+	ahash_request_set_crypt(hash, cmd->req.sg,
+		(void *)&cmd->exp_ddgst, cmd->req.transfer_len);
+	crypto_ahash_digest(hash);
+}
+
+static void i10_target_setup_c2h_data_pdu(struct i10_target_cmd *cmd)
+{
+	struct nvme_tcp_data_pdu *pdu = cmd->data_pdu;
+	struct i10_target_queue *queue = cmd->queue;
+	u8 hdgst = i10_target_hdgst_len(cmd->queue);
+	u8 ddgst = i10_target_ddgst_len(cmd->queue);
+
+	cmd->offset = 0;
+	cmd->state = I10_TARGET_SEND_DATA_PDU;
+
+	pdu->hdr.type = nvme_tcp_c2h_data;
+	pdu->hdr.flags = NVME_TCP_F_DATA_LAST | (queue->nvme_sq.sqhd_disabled ?
+						NVME_TCP_F_DATA_SUCCESS : 0);
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.pdo = pdu->hdr.hlen + hdgst;
+	pdu->hdr.plen =
+		cpu_to_le32(pdu->hdr.hlen + hdgst +
+				cmd->req.transfer_len + ddgst);
+	pdu->command_id = cmd->req.cqe->command_id;
+	pdu->data_length = cpu_to_le32(cmd->req.transfer_len);
+	pdu->data_offset = cpu_to_le32(cmd->wbytes_done);
+
+	if (queue->data_digest) {
+		pdu->hdr.flags |= NVME_TCP_F_DDGST;
+		i10_target_ddgst(queue->snd_hash, cmd);
+	}
+
+	if (cmd->queue->hdr_digest) {
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+		i10_target_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+	}
+}
+
+static void i10_target_setup_r2t_pdu(struct i10_target_cmd *cmd)
+{
+	struct nvme_tcp_r2t_pdu *pdu = cmd->r2t_pdu;
+	struct i10_target_queue *queue = cmd->queue;
+	u8 hdgst = i10_target_hdgst_len(cmd->queue);
+
+	cmd->offset = 0;
+	cmd->state = I10_TARGET_SEND_R2T;
+
+	pdu->hdr.type = nvme_tcp_r2t;
+	pdu->hdr.flags = 0;
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.pdo = 0;
+	pdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);
+
+	pdu->command_id = cmd->req.cmd->common.command_id;
+	pdu->ttag = i10_target_cmd_tag(cmd->queue, cmd);
+	pdu->r2t_length = cpu_to_le32(cmd->req.transfer_len - cmd->rbytes_done);
+	pdu->r2t_offset = cpu_to_le32(cmd->rbytes_done);
+	if (cmd->queue->hdr_digest) {
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+		i10_target_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+	}
+}
+
+static void i10_target_setup_response_pdu(struct i10_target_cmd *cmd)
+{
+	struct nvme_tcp_rsp_pdu *pdu = cmd->rsp_pdu;
+	struct i10_target_queue *queue = cmd->queue;
+	u8 hdgst = i10_target_hdgst_len(cmd->queue);
+
+	cmd->offset = 0;
+	cmd->state = I10_TARGET_SEND_RESPONSE;
+
+	pdu->hdr.type = nvme_tcp_rsp;
+	pdu->hdr.flags = 0;
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.pdo = 0;
+	pdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);
+	if (cmd->queue->hdr_digest) {
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+		i10_target_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+	}
+}
+
+static void i10_target_process_resp_list(struct i10_target_queue *queue)
+{
+	struct llist_node *node;
+
+	node = llist_del_all(&queue->resp_list);
+	if (!node)
+		return;
+
+	while (node) {
+		struct i10_target_cmd *cmd = llist_entry(node,
+					struct i10_target_cmd, lentry);
+
+		list_add(&cmd->entry, &queue->resp_send_list);
+		node = node->next;
+		queue->send_list_len++;
+	}
+}
+
+static inline bool i10_target_is_admin_queue(struct i10_target_queue *queue)
+{
+	return queue->nvme_sq.qid == 0; 
+}
+
+static inline bool i10_target_is_caravan_full(struct i10_target_queue *queue)
+{
+	return (queue->caravan_len >= I10_CARAVAN_CAPACITY) ||
+		(queue->nr_iovs >= I10_TARGET_SEND_BUDGET * 3) ||
+		(queue->nr_caravan_cmds >= I10_TARGET_SEND_BUDGET) ||
+		(queue->nr_caravan_mapped >= I10_TARGET_SEND_BUDGET);
+}
+
+static struct i10_target_cmd *i10_target_fetch_cmd(struct i10_target_queue *queue)
+{
+	queue->snd_cmd = list_first_entry_or_null(&queue->resp_send_list,
+				struct i10_target_cmd, entry);
+	if (!queue->snd_cmd) {
+		i10_target_process_resp_list(queue);
+		queue->snd_cmd =
+			list_first_entry_or_null(&queue->resp_send_list,
+					struct i10_target_cmd, entry);
+		if (unlikely(!queue->snd_cmd))
+			return NULL;
+	}
+
+	list_del_init(&queue->snd_cmd->entry);
+	queue->send_list_len--;
+
+	if (i10_target_need_data_out(queue->snd_cmd))
+		i10_target_setup_c2h_data_pdu(queue->snd_cmd);
+	else if (i10_target_need_data_in(queue->snd_cmd))
+		i10_target_setup_r2t_pdu(queue->snd_cmd);
+	else
+		i10_target_setup_response_pdu(queue->snd_cmd);
+
+	return queue->snd_cmd;
+}
+
+static void i10_target_queue_response(struct nvmet_req *req)
+{
+	struct i10_target_cmd *cmd =
+		container_of(req, struct i10_target_cmd, req);
+	struct i10_target_queue	*queue = cmd->queue;
+
+	llist_add(&cmd->lentry, &queue->resp_list);
+	queue_work_on(cmd->queue->cpu, i10_target_wq, &cmd->queue->io_work);
+}
+
+static int i10_target_try_send_data_pdu(struct i10_target_cmd *cmd)
+{
+	struct i10_target_queue *queue = cmd->queue;
+	u8 hdgst = i10_target_hdgst_len(cmd->queue);
+	int left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;
+	int ret;
+
+	/* Caravans: data PDU aggregation */
+	if (!i10_target_is_admin_queue(queue)) {
+		if (i10_target_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		queue->caravan_iovs[queue->nr_iovs].iov_base =
+			cmd->data_pdu + cmd->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+		queue->caravan_len += left;
+		ret = left;
+	}
+	else
+		ret = kernel_sendpage(cmd->queue->sock,
+			virt_to_page(cmd->data_pdu),
+			offset_in_page(cmd->data_pdu) + cmd->offset,
+			left, MSG_DONTWAIT | MSG_MORE);
+	if (ret <= 0)
+		return ret;
+
+	cmd->offset += ret;
+	left -= ret;
+
+	if (left)
+		return -EAGAIN;
+
+	cmd->state = I10_TARGET_SEND_DATA;
+	cmd->offset  = 0;
+	return 1;
+}
+
+static int i10_target_try_send_data(struct i10_target_cmd *cmd, bool last_in_batch)
+{
+	struct i10_target_queue *queue = cmd->queue;
+	int ret;
+
+	while (cmd->cur_sg) {
+		struct page *page = sg_page(cmd->cur_sg);
+		u32 left = cmd->cur_sg->length - cmd->offset;
+		int flags = MSG_DONTWAIT;
+
+		/* Caravans: I/O data aggregation */
+		if (!i10_target_is_admin_queue(queue)) {
+			if (i10_target_is_caravan_full(queue)) {
+				queue->send_now = true;
+				return 1;
+			}
+			queue->caravan_iovs[queue->nr_iovs].iov_base =
+				kmap(page) + cmd->offset;
+			queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+			queue->caravan_mapped[queue->nr_caravan_mapped++] = page;
+			queue->caravan_len += left;
+			ret = left;
+		}
+		else {
+			if ((!last_in_batch && cmd->queue->send_list_len) ||
+				cmd->wbytes_done + left < cmd->req.transfer_len ||
+				queue->data_digest || !queue->nvme_sq.sqhd_disabled)
+				flags |= MSG_MORE;
+
+			ret = kernel_sendpage(cmd->queue->sock, page, cmd->offset,
+					left, flags);
+		}
+		if (ret <= 0)
+			return ret;
+
+		cmd->offset += ret;
+		cmd->wbytes_done += ret;
+
+		/* Done with sg?*/
+		if (cmd->offset == cmd->cur_sg->length) {
+			cmd->cur_sg = sg_next(cmd->cur_sg);
+			cmd->offset = 0;
+		}
+	}
+
+	if (queue->data_digest) {
+		cmd->state = I10_TARGET_SEND_DDGST;
+		cmd->offset = 0;
+	} else {
+		if (queue->nvme_sq.sqhd_disabled) {
+			cmd->queue->snd_cmd = NULL;
+			i10_target_put_cmd(cmd);
+		} else {
+			i10_target_setup_response_pdu(cmd);
+		}
+	}
+
+	if (queue->nvme_sq.sqhd_disabled) {
+		kfree(cmd->iov);
+		sgl_free(cmd->req.sg);
+	}
+
+	return 1;
+}
+
+static int i10_target_try_send_response(struct i10_target_cmd *cmd,
+		bool last_in_batch)
+{
+	u8 hdgst = i10_target_hdgst_len(cmd->queue);
+	int left = sizeof(*cmd->rsp_pdu) - cmd->offset + hdgst;
+	int flags = MSG_DONTWAIT;
+	int ret;
+
+	struct i10_target_queue *queue = cmd->queue;
+
+	if (!last_in_batch && cmd->queue->send_list_len)
+		flags |= MSG_MORE;
+	else
+		flags |= MSG_EOR;
+
+	/* Caravans: response PDU aggregation */
+	if (!i10_target_is_admin_queue(queue)) {
+		if (i10_target_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		queue->caravan_iovs[queue->nr_iovs].iov_base =
+			cmd->rsp_pdu + cmd->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+		queue->caravan_cmds[queue->nr_caravan_cmds++].cmd = cmd;
+		queue->caravan_len += left;
+		cmd->queue->snd_cmd = NULL;
+
+		cmd->offset += left;
+		return 1;
+	}
+
+	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->rsp_pdu),
+		offset_in_page(cmd->rsp_pdu) + cmd->offset, left, flags);
+	if (ret <= 0)
+		return ret;
+	cmd->offset += ret;
+	left -= ret;
+
+	if (left)
+		return -EAGAIN;
+
+	kfree(cmd->iov);
+	sgl_free(cmd->req.sg);
+	cmd->queue->snd_cmd = NULL;
+	i10_target_put_cmd(cmd);
+	return 1;
+}
+
+static int i10_target_try_send_r2t(struct i10_target_cmd *cmd, bool last_in_batch)
+{
+	u8 hdgst = i10_target_hdgst_len(cmd->queue);
+	int left = sizeof(*cmd->r2t_pdu) - cmd->offset + hdgst;
+	int flags = MSG_DONTWAIT;
+	int ret;
+	
+	struct i10_target_queue *queue = cmd->queue;
+
+	if (!last_in_batch && cmd->queue->send_list_len)
+		flags |= MSG_MORE;
+	else
+		flags |= MSG_EOR;
+
+	/* Caravans: r2t PDU aggregation */
+	if (!i10_target_is_admin_queue(queue)) {
+		if (i10_target_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		queue->caravan_iovs[queue->nr_iovs].iov_base = cmd->r2t_pdu
+			+ cmd->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+		queue->caravan_len += left;
+		ret = left;
+	}
+	else
+		ret = kernel_sendpage(cmd->queue->sock,
+			virt_to_page(cmd->r2t_pdu),
+			offset_in_page(cmd->r2t_pdu) + cmd->offset,
+			left, flags);
+	if (ret <= 0)
+		return ret;
+	cmd->offset += ret;
+	left -= ret;
+
+	if (left)
+		return -EAGAIN;
+
+	cmd->queue->snd_cmd = NULL;
+	return 1;
+}
+
+static int i10_target_try_send_ddgst(struct i10_target_cmd *cmd)
+{
+	struct i10_target_queue *queue = cmd->queue;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct kvec iov = {
+		.iov_base = &cmd->exp_ddgst + cmd->offset,
+		.iov_len = NVME_TCP_DIGEST_LENGTH - cmd->offset
+	};
+	int ret;
+
+	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	if (unlikely(ret <= 0))
+		return ret;
+
+	cmd->offset += ret;
+
+	if (queue->nvme_sq.sqhd_disabled) {
+		cmd->queue->snd_cmd = NULL;
+		i10_target_put_cmd(cmd);
+	} else {
+		i10_target_setup_response_pdu(cmd);
+	}
+	return 1;
+}
+
+static int i10_target_try_send_one(struct i10_target_queue *queue,
+		bool last_in_batch)
+{
+	struct i10_target_cmd *cmd = queue->snd_cmd;
+	int ret = 0;
+
+	if (!cmd || queue->state == I10_TARGET_Q_DISCONNECTING) {
+		cmd = i10_target_fetch_cmd(queue);
+		if (unlikely(!cmd))
+			return 0;
+	}
+
+	if (cmd->state == I10_TARGET_SEND_DATA_PDU) {
+		ret = i10_target_try_send_data_pdu(cmd);
+		if (ret <= 0)
+			goto done_send;
+	}
+
+	if (cmd->state == I10_TARGET_SEND_DATA) {
+		ret = i10_target_try_send_data(cmd, last_in_batch);
+		if (ret <= 0)
+			goto done_send;
+	}
+
+	if (cmd->state == I10_TARGET_SEND_DDGST) {
+		ret = i10_target_try_send_ddgst(cmd);
+		if (ret <= 0)
+			goto done_send;
+	}
+
+	if (cmd->state == I10_TARGET_SEND_R2T) {
+		ret = i10_target_try_send_r2t(cmd, last_in_batch);
+		if (ret <= 0)
+			goto done_send;
+	}
+
+	if (cmd->state == I10_TARGET_SEND_RESPONSE)
+		ret = i10_target_try_send_response(cmd, last_in_batch);
+
+done_send:
+	if (ret < 0) {
+		if (ret == -EAGAIN)
+			return 0;
+		return ret;
+	}
+
+	return 1;
+}
+
+/* To check if there's enough room in tcp_sndbuf */
+static inline int i10_target_sndbuf_nospace(struct i10_target_queue *queue,
+		int length)
+{
+	return sk_stream_wspace(queue->sock->sk) < length;
+}	
+
+static int i10_target_try_send(struct i10_target_queue *queue,
+		int budget, int *sends)
+{
+	int i, ret = 0;
+
+	for (i = 0; i < budget; i++) {
+		ret = i10_target_try_send_one(queue, i == budget - 1);
+
+		/* Send i10 caravans */
+		if ((queue->send_now || ret <= 0 || i == budget - 1) &&
+			queue->caravan_len) {
+			struct msghdr msg =
+				{ .msg_flags = MSG_DONTWAIT | MSG_EOR };
+			int i10_ret, j;
+
+			if (i10_target_sndbuf_nospace(queue,
+				queue->caravan_len)) {
+				set_bit(SOCK_NOSPACE,
+					&queue->sock->sk->sk_socket->flags);
+				return 0;
+			}
+
+			i10_ret = kernel_sendmsg(queue->sock, &msg,
+					queue->caravan_iovs,
+					queue->nr_iovs, queue->caravan_len);
+			if (unlikely(i10_ret <= 0))
+				pr_err("I10_TARGET: kernel_sendmsg fails (i10_ret %d)\n",
+					i10_ret);
+
+			for (j = 0; j < queue->nr_caravan_cmds; j++) {
+				kfree(queue->caravan_cmds[j].cmd->iov);
+				sgl_free(queue->caravan_cmds[j].cmd->req.sg);
+				i10_target_put_cmd(queue->caravan_cmds[j].cmd);
+			}
+
+			for (j = 0; j < queue->nr_caravan_mapped; j++)
+				kunmap(queue->caravan_mapped[j]);
+
+			queue->nr_iovs = 0;
+			queue->nr_caravan_cmds = 0;
+			queue->nr_caravan_mapped = 0;
+			queue->caravan_len = 0;
+			queue->send_now = false;
+		}
+
+		if (ret <= 0)
+			break;
+		(*sends)++;
+	}
+
+	return ret;
+}
+
+static void i10_target_prepare_receive_pdu(struct i10_target_queue *queue)
+{
+	queue->offset = 0;
+	queue->left = sizeof(struct nvme_tcp_hdr);
+	queue->cmd = NULL;
+	queue->rcv_state = I10_TARGET_RECV_PDU;
+}
+
+static void i10_target_free_crypto(struct i10_target_queue *queue)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(queue->rcv_hash);
+
+	ahash_request_free(queue->rcv_hash);
+	ahash_request_free(queue->snd_hash);
+	crypto_free_ahash(tfm);
+}
+
+static int i10_target_alloc_crypto(struct i10_target_queue *queue)
+{
+	struct crypto_ahash *tfm;
+
+	tfm = crypto_alloc_ahash("crc32c", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm))
+		return PTR_ERR(tfm);
+
+	queue->snd_hash = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!queue->snd_hash)
+		goto free_tfm;
+	ahash_request_set_callback(queue->snd_hash, 0, NULL, NULL);
+
+	queue->rcv_hash = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!queue->rcv_hash)
+		goto free_snd_hash;
+	ahash_request_set_callback(queue->rcv_hash, 0, NULL, NULL);
+
+	return 0;
+free_snd_hash:
+	ahash_request_free(queue->snd_hash);
+free_tfm:
+	crypto_free_ahash(tfm);
+	return -ENOMEM;
+}
+
+
+static int i10_target_handle_icreq(struct i10_target_queue *queue)
+{
+	struct nvme_tcp_icreq_pdu *icreq = &queue->pdu.icreq;
+	struct nvme_tcp_icresp_pdu *icresp = &queue->pdu.icresp;
+	struct msghdr msg = {};
+	struct kvec iov;
+	int ret;
+
+	if (le32_to_cpu(icreq->hdr.plen) != sizeof(struct nvme_tcp_icreq_pdu)) {
+		pr_err("bad nvme-tcp pdu length (%d)\n",
+			le32_to_cpu(icreq->hdr.plen));
+		i10_target_fatal_error(queue);
+	}
+
+	if (icreq->pfv != NVME_TCP_PFV_1_0) {
+		pr_err("queue %d: bad pfv %d\n", queue->idx, icreq->pfv);
+		return -EPROTO;
+	}
+
+	if (icreq->hpda != 0) {
+		pr_err("queue %d: unsupported hpda %d\n", queue->idx,
+			icreq->hpda);
+		return -EPROTO;
+	}
+
+	queue->hdr_digest = !!(icreq->digest & NVME_TCP_HDR_DIGEST_ENABLE);
+	queue->data_digest = !!(icreq->digest & NVME_TCP_DATA_DIGEST_ENABLE);
+	if (queue->hdr_digest || queue->data_digest) {
+		ret = i10_target_alloc_crypto(queue);
+		if (ret)
+			return ret;
+	}
+
+	memset(icresp, 0, sizeof(*icresp));
+	icresp->hdr.type = nvme_tcp_icresp;
+	icresp->hdr.hlen = sizeof(*icresp);
+	icresp->hdr.pdo = 0;
+	icresp->hdr.plen = cpu_to_le32(icresp->hdr.hlen);
+	icresp->pfv = cpu_to_le16(NVME_TCP_PFV_1_0);
+	icresp->maxdata = cpu_to_le32(0x400000); /* 16M arbitrary limit */
+	icresp->cpda = 0;
+	if (queue->hdr_digest)
+		icresp->digest |= NVME_TCP_HDR_DIGEST_ENABLE;
+	if (queue->data_digest)
+		icresp->digest |= NVME_TCP_DATA_DIGEST_ENABLE;
+
+	iov.iov_base = icresp;
+	iov.iov_len = sizeof(*icresp);
+	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	if (ret < 0)
+		goto free_crypto;
+
+	queue->state = I10_TARGET_Q_LIVE;
+	i10_target_prepare_receive_pdu(queue);
+	return 0;
+free_crypto:
+	if (queue->hdr_digest || queue->data_digest)
+		i10_target_free_crypto(queue);
+	return ret;
+}
+
+static void i10_target_handle_req_failure(struct i10_target_queue *queue,
+		struct i10_target_cmd *cmd, struct nvmet_req *req)
+{
+	int ret;
+
+	/* recover the expected data transfer length */
+	req->data_len = le32_to_cpu(req->cmd->common.dptr.sgl.length);
+
+	if (!nvme_is_write(cmd->req.cmd) ||
+	    req->data_len > cmd->req.port->inline_data_size) {
+		i10_target_prepare_receive_pdu(queue);
+		return;
+	}
+
+	ret = i10_target_map_data(cmd);
+	if (unlikely(ret)) {
+		pr_err("queue %d: failed to map data\n", queue->idx);
+		i10_target_fatal_error(queue);
+		return;
+	}
+
+	queue->rcv_state = I10_TARGET_RECV_DATA;
+	i10_target_map_pdu_iovec(cmd);
+	cmd->flags |= I10_TARGET_F_INIT_FAILED;
+}
+
+static int i10_target_handle_h2c_data_pdu(struct i10_target_queue *queue)
+{
+	struct nvme_tcp_data_pdu *data = &queue->pdu.data;
+	struct i10_target_cmd *cmd;
+
+	cmd = &queue->cmds[data->ttag];
+
+	if (le32_to_cpu(data->data_offset) != cmd->rbytes_done) {
+		pr_err("ttag %u unexpected data offset %u (expected %u)\n",
+			data->ttag, le32_to_cpu(data->data_offset),
+			cmd->rbytes_done);
+		/* FIXME: use path and transport errors */
+		nvmet_req_complete(&cmd->req,
+			NVME_SC_INVALID_FIELD | NVME_SC_DNR);
+		return -EPROTO;
+	}
+
+	cmd->pdu_len = le32_to_cpu(data->data_length);
+	cmd->pdu_recv = 0;
+	i10_target_map_pdu_iovec(cmd);
+	queue->cmd = cmd;
+	queue->rcv_state = I10_TARGET_RECV_DATA;
+
+	return 0;
+}
+
+static int i10_target_done_recv_pdu(struct i10_target_queue *queue)
+{
+	struct nvme_tcp_hdr *hdr = &queue->pdu.cmd.hdr;
+	struct nvme_command *nvme_cmd = &queue->pdu.cmd.cmd;
+	struct nvmet_req *req;
+	int ret;
+
+	if (unlikely(queue->state == I10_TARGET_Q_CONNECTING)) {
+		if (hdr->type != nvme_tcp_icreq) {
+			pr_err("unexpected pdu type (%d) before icreq\n",
+				hdr->type);
+			i10_target_fatal_error(queue);
+			return -EPROTO;
+		}
+		return i10_target_handle_icreq(queue);
+	}
+
+	if (hdr->type == nvme_tcp_h2c_data) {
+		ret = i10_target_handle_h2c_data_pdu(queue);
+		if (unlikely(ret))
+			return ret;
+		return 0;
+	}
+
+	queue->cmd = i10_target_get_cmd(queue);
+	if (unlikely(!queue->cmd)) {
+		/* This should never happen */
+		pr_err("queue %d: out of commands (%d) send_list_len: %d, opcode: %d",
+			queue->idx, queue->nr_cmds, queue->send_list_len,
+			nvme_cmd->common.opcode);
+		i10_target_fatal_error(queue);
+		return -ENOMEM;
+	}
+
+	req = &queue->cmd->req;
+	memcpy(req->cmd, nvme_cmd, sizeof(*nvme_cmd));
+
+	if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
+			&queue->nvme_sq, &i10_target_ops))) {
+		pr_err("failed cmd %p id %d opcode %d, data_len: %d\n",
+			req->cmd, req->cmd->common.command_id,
+			req->cmd->common.opcode,
+			le32_to_cpu(req->cmd->common.dptr.sgl.length));
+
+		i10_target_handle_req_failure(queue, queue->cmd, req);
+		return -EAGAIN;
+	}
+
+	ret = i10_target_map_data(queue->cmd);
+	if (unlikely(ret)) {
+		pr_err("queue %d: failed to map data\n", queue->idx);
+		if (i10_target_has_inline_data(queue->cmd))
+			i10_target_fatal_error(queue);
+		else
+			nvmet_req_complete(req, ret);
+		ret = -EAGAIN;
+		goto out;
+	}
+
+	if (i10_target_need_data_in(queue->cmd)) {
+		if (i10_target_has_inline_data(queue->cmd)) {
+			queue->rcv_state = I10_TARGET_RECV_DATA;
+			i10_target_map_pdu_iovec(queue->cmd);
+			return 0;
+		}
+		/* send back R2T */
+		i10_target_queue_response(&queue->cmd->req);
+		goto out;
+	}
+
+	nvmet_req_execute(&queue->cmd->req);
+out:
+	i10_target_prepare_receive_pdu(queue);
+	return ret;
+}
+
+static const u8 nvme_tcp_pdu_sizes[] = {
+	[nvme_tcp_icreq]	= sizeof(struct nvme_tcp_icreq_pdu),
+	[nvme_tcp_cmd]		= sizeof(struct nvme_tcp_cmd_pdu),
+	[nvme_tcp_h2c_data]	= sizeof(struct nvme_tcp_data_pdu),
+};
+
+static inline u8 i10_target_pdu_size(u8 type)
+{
+	size_t idx = type;
+
+	return (idx < ARRAY_SIZE(nvme_tcp_pdu_sizes) &&
+		nvme_tcp_pdu_sizes[idx]) ?
+			nvme_tcp_pdu_sizes[idx] : 0;
+}
+
+static inline bool i10_target_pdu_valid(u8 type)
+{
+	switch (type) {
+	case nvme_tcp_icreq:
+	case nvme_tcp_cmd:
+	case nvme_tcp_h2c_data:
+		/* fallthru */
+		return true;
+	}
+
+	return false;
+}
+
+static int i10_target_try_recv_pdu(struct i10_target_queue *queue)
+{
+	struct nvme_tcp_hdr *hdr = &queue->pdu.cmd.hdr;
+	int len;
+	struct kvec iov;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+
+recv:
+	iov.iov_base = (void *)&queue->pdu + queue->offset;
+	iov.iov_len = queue->left;
+	len = kernel_recvmsg(queue->sock, &msg, &iov, 1,
+			iov.iov_len, msg.msg_flags);
+	if (unlikely(len < 0))
+		return len;
+
+	queue->offset += len;
+	queue->left -= len;
+	if (queue->left)
+		return -EAGAIN;
+
+	if (queue->offset == sizeof(struct nvme_tcp_hdr)) {
+		u8 hdgst = i10_target_hdgst_len(queue);
+
+		if (unlikely(!i10_target_pdu_valid(hdr->type))) {
+			pr_err("unexpected pdu type %d\n", hdr->type);
+			i10_target_fatal_error(queue);
+			return -EIO;
+		}
+
+		if (unlikely(hdr->hlen != i10_target_pdu_size(hdr->type))) {
+			pr_err("pdu %d bad hlen %d\n", hdr->type, hdr->hlen);
+			return -EIO;
+		}
+
+		queue->left = hdr->hlen - queue->offset + hdgst;
+		goto recv;
+	}
+
+	if (queue->hdr_digest &&
+	    i10_target_verify_hdgst(queue, &queue->pdu, queue->offset)) {
+		i10_target_fatal_error(queue); /* fatal */
+		return -EPROTO;
+	}
+
+	if (queue->data_digest &&
+	    i10_target_check_ddgst(queue, &queue->pdu)) {
+		i10_target_fatal_error(queue); /* fatal */
+		return -EPROTO;
+	}
+
+	return i10_target_done_recv_pdu(queue);
+}
+
+static void i10_target_prep_recv_ddgst(struct i10_target_cmd *cmd)
+{
+	struct i10_target_queue *queue = cmd->queue;
+
+	i10_target_ddgst(queue->rcv_hash, cmd);
+	queue->offset = 0;
+	queue->left = NVME_TCP_DIGEST_LENGTH;
+	queue->rcv_state = I10_TARGET_RECV_DDGST;
+}
+
+static int i10_target_try_recv_data(struct i10_target_queue *queue)
+{
+	struct i10_target_cmd  *cmd = queue->cmd;
+	int ret;
+
+	while (msg_data_left(&cmd->recv_msg)) {
+		ret = sock_recvmsg(cmd->queue->sock, &cmd->recv_msg,
+			cmd->recv_msg.msg_flags);
+		if (ret <= 0)
+			return ret;
+
+		cmd->pdu_recv += ret;
+		cmd->rbytes_done += ret;
+	}
+
+	i10_target_unmap_pdu_iovec(cmd);
+
+	if (!(cmd->flags & I10_TARGET_F_INIT_FAILED) &&
+	    cmd->rbytes_done == cmd->req.transfer_len) {
+		if (queue->data_digest) {
+			i10_target_prep_recv_ddgst(cmd);
+			return 0;
+		}
+		nvmet_req_execute(&cmd->req);
+	}
+
+	i10_target_prepare_receive_pdu(queue);
+	return 0;
+}
+
+static int i10_target_try_recv_ddgst(struct i10_target_queue *queue)
+{
+	struct i10_target_cmd *cmd = queue->cmd;
+	int ret;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct kvec iov = {
+		.iov_base = (void *)&cmd->recv_ddgst + queue->offset,
+		.iov_len = queue->left
+	};
+
+	ret = kernel_recvmsg(queue->sock, &msg, &iov, 1,
+			iov.iov_len, msg.msg_flags);
+	if (unlikely(ret < 0))
+		return ret;
+
+	queue->offset += ret;
+	queue->left -= ret;
+	if (queue->left)
+		return -EAGAIN;
+
+	if (queue->data_digest && cmd->exp_ddgst != cmd->recv_ddgst) {
+		pr_err("queue %d: cmd %d pdu (%d) data digest error: recv %#x expected %#x\n",
+			queue->idx, cmd->req.cmd->common.command_id,
+			queue->pdu.cmd.hdr.type, le32_to_cpu(cmd->recv_ddgst),
+			le32_to_cpu(cmd->exp_ddgst));
+		i10_target_finish_cmd(cmd);
+		i10_target_fatal_error(queue);
+		ret = -EPROTO;
+		goto out;
+	}
+
+	if (!(cmd->flags & I10_TARGET_F_INIT_FAILED) &&
+	    cmd->rbytes_done == cmd->req.transfer_len)
+		nvmet_req_execute(&cmd->req);
+	ret = 0;
+out:
+	i10_target_prepare_receive_pdu(queue);
+	return ret;
+}
+
+static int i10_target_try_recv_one(struct i10_target_queue *queue)
+{
+	int result;
+
+	if (unlikely(queue->rcv_state == I10_TARGET_RECV_ERR))
+		return 0;
+
+	if (queue->rcv_state == I10_TARGET_RECV_PDU) {
+		result = i10_target_try_recv_pdu(queue);
+		if (result != 0)
+			goto done_recv;
+	}
+
+	if (queue->rcv_state == I10_TARGET_RECV_DATA) {
+		result = i10_target_try_recv_data(queue);
+		if (result != 0)
+			goto done_recv;
+	}
+
+	if (queue->rcv_state == I10_TARGET_RECV_DDGST) {
+		result = i10_target_try_recv_ddgst(queue);
+		if (result != 0)
+			goto done_recv;
+	}
+
+done_recv:
+	if (result < 0) {
+		if (result == -EAGAIN)
+			return 0;
+		return result;
+	}
+	return 1;
+}
+
+static int i10_target_try_recv(struct i10_target_queue *queue,
+		int budget, int *recvs)
+{
+	int i, ret = 0;
+
+	for (i = 0; i < budget; i++) {
+		ret = i10_target_try_recv_one(queue);
+		if (ret <= 0)
+			break;
+		(*recvs)++;
+	}
+
+	return ret;
+}
+
+static void i10_target_schedule_release_queue(struct i10_target_queue *queue)
+{
+	spin_lock(&queue->state_lock);
+	if (queue->state != I10_TARGET_Q_DISCONNECTING) {
+		queue->state = I10_TARGET_Q_DISCONNECTING;
+		schedule_work(&queue->release_work);
+	}
+	spin_unlock(&queue->state_lock);
+}
+
+static void i10_target_io_work(struct work_struct *w)
+{
+	struct i10_target_queue *queue =
+		container_of(w, struct i10_target_queue, io_work);
+	bool pending;
+	int ret, ops = 0;
+	
+	do {
+		pending = false;
+
+		ret = i10_target_try_recv(queue, I10_TARGET_RECV_BUDGET, &ops);
+		if (ret > 0) {
+			pending = true;
+		} else if (ret < 0) {
+			if (ret == -EPIPE || ret == -ECONNRESET)
+				kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+			else
+				i10_target_fatal_error(queue);
+			return;
+		}
+
+		ret = i10_target_try_send(queue, I10_TARGET_SEND_BUDGET, &ops);
+		if (ret > 0) {
+			/* transmitted message/data */
+			pending = true;
+		} else if (ret < 0) {
+			if (ret == -EPIPE || ret == -ECONNRESET)
+				kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+			else
+				i10_target_fatal_error(queue);
+			return;
+		}
+
+	} while (pending && ops < I10_TARGET_IO_WORK_BUDGET);
+
+	/*
+	 * We exahusted our budget, requeue our selves
+	 */
+	if (pending)
+		queue_work_on(queue->cpu, i10_target_wq, &queue->io_work);
+}
+
+static int i10_target_alloc_cmd(struct i10_target_queue *queue,
+		struct i10_target_cmd *c)
+{
+	u8 hdgst = i10_target_hdgst_len(queue);
+
+	c->queue = queue;
+	c->req.port = queue->port->nport;
+
+	c->cmd_pdu = page_frag_alloc(&queue->pf_cache,
+			sizeof(*c->cmd_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);
+	if (!c->cmd_pdu)
+		return -ENOMEM;
+	c->req.cmd = &c->cmd_pdu->cmd;
+
+	c->rsp_pdu = page_frag_alloc(&queue->pf_cache,
+			sizeof(*c->rsp_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);
+	if (!c->rsp_pdu)
+		goto out_free_cmd;
+	c->req.cqe = &c->rsp_pdu->cqe;
+
+	c->data_pdu = page_frag_alloc(&queue->pf_cache,
+			sizeof(*c->data_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);
+	if (!c->data_pdu)
+		goto out_free_rsp;
+
+	c->r2t_pdu = page_frag_alloc(&queue->pf_cache,
+			sizeof(*c->r2t_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);
+	if (!c->r2t_pdu)
+		goto out_free_data;
+
+	c->recv_msg.msg_flags = MSG_DONTWAIT | MSG_NOSIGNAL;
+
+	list_add_tail(&c->entry, &queue->free_list);
+
+	return 0;
+out_free_data:
+	page_frag_free(c->data_pdu);
+out_free_rsp:
+	page_frag_free(c->rsp_pdu);
+out_free_cmd:
+	page_frag_free(c->cmd_pdu);
+	return -ENOMEM;
+}
+
+static void i10_target_free_cmd(struct i10_target_cmd *c)
+{
+	page_frag_free(c->r2t_pdu);
+	page_frag_free(c->data_pdu);
+	page_frag_free(c->rsp_pdu);
+	page_frag_free(c->cmd_pdu);
+}
+
+static int i10_target_alloc_cmds(struct i10_target_queue *queue)
+{
+	struct i10_target_cmd *cmds;
+	int i, ret = -EINVAL, nr_cmds = queue->nr_cmds;
+
+	cmds = kcalloc(nr_cmds, sizeof(struct i10_target_cmd), GFP_KERNEL);
+	if (!cmds)
+		goto out;
+
+	for (i = 0; i < nr_cmds; i++) {
+		ret = i10_target_alloc_cmd(queue, cmds + i);
+		if (ret)
+			goto out_free;
+	}
+
+	queue->cmds = cmds;
+
+	return 0;
+out_free:
+	while (--i >= 0)
+		i10_target_free_cmd(cmds + i);
+	kfree(cmds);
+out:
+	return ret;
+}
+
+static void i10_target_free_cmds(struct i10_target_queue *queue)
+{
+	struct i10_target_cmd *cmds = queue->cmds;
+	int i;
+
+	for (i = 0; i < queue->nr_cmds; i++)
+		i10_target_free_cmd(cmds + i);
+
+	i10_target_free_cmd(&queue->connect);
+	kfree(cmds);
+}
+
+static void i10_target_restore_socket_callbacks(struct i10_target_queue *queue)
+{
+	struct socket *sock = queue->sock;
+
+	write_lock_bh(&sock->sk->sk_callback_lock);
+	sock->sk->sk_data_ready =  queue->data_ready;
+	sock->sk->sk_state_change = queue->state_change;
+	sock->sk->sk_write_space = queue->write_space;
+	sock->sk->sk_user_data = NULL;
+	write_unlock_bh(&sock->sk->sk_callback_lock);
+}
+
+static void i10_target_finish_cmd(struct i10_target_cmd *cmd)
+{
+	nvmet_req_uninit(&cmd->req);
+	i10_target_unmap_pdu_iovec(cmd);
+	kfree(cmd->iov);
+	sgl_free(cmd->req.sg);
+}
+
+static void i10_target_uninit_data_in_cmds(struct i10_target_queue *queue)
+{
+	struct i10_target_cmd *cmd = queue->cmds;
+	int i;
+
+	for (i = 0; i < queue->nr_cmds; i++, cmd++) {
+		if (i10_target_need_data_in(cmd))
+			i10_target_finish_cmd(cmd);
+	}
+
+	if (!queue->nr_cmds && i10_target_need_data_in(&queue->connect)) {
+		/* failed in connect */
+		i10_target_finish_cmd(&queue->connect);
+	}
+}
+
+static void i10_target_release_queue_work(struct work_struct *w)
+{
+	struct i10_target_queue *queue =
+		container_of(w, struct i10_target_queue, release_work);
+
+	mutex_lock(&i10_target_queue_mutex);
+	list_del_init(&queue->queue_list);
+	mutex_unlock(&i10_target_queue_mutex);
+
+	i10_target_restore_socket_callbacks(queue);
+	flush_work(&queue->io_work);
+
+	i10_target_uninit_data_in_cmds(queue);
+	nvmet_sq_destroy(&queue->nvme_sq);
+	cancel_work_sync(&queue->io_work);
+	sock_release(queue->sock);
+	i10_target_free_cmds(queue);
+	if (queue->hdr_digest || queue->data_digest)
+		i10_target_free_crypto(queue);
+	ida_simple_remove(&i10_target_queue_ida, queue->idx);
+
+	kfree(queue->caravan_iovs);
+	kfree(queue->caravan_cmds);
+	kfree(queue->caravan_mapped);
+	kfree(queue);
+}
+
+static void i10_target_data_ready(struct sock *sk)
+{
+	struct i10_target_queue *queue;
+
+	read_lock_bh(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (likely(queue))
+		queue_work_on(queue->cpu, i10_target_wq, &queue->io_work);
+	read_unlock_bh(&sk->sk_callback_lock);
+}
+
+static void i10_target_write_space(struct sock *sk)
+{
+	struct i10_target_queue *queue;
+
+	read_lock_bh(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (unlikely(!queue))
+		goto out;
+
+	if (unlikely(queue->state == I10_TARGET_Q_CONNECTING)) {
+		queue->write_space(sk);
+		goto out;
+	}
+
+	if (sk_stream_is_writeable(sk)) {
+		clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		queue_work_on(queue->cpu, i10_target_wq, &queue->io_work);
+	}
+out:
+	read_unlock_bh(&sk->sk_callback_lock);
+}
+
+static void i10_target_state_change(struct sock *sk)
+{
+	struct i10_target_queue *queue;
+
+	write_lock_bh(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (!queue)
+		goto done;
+
+	switch (sk->sk_state) {
+	case TCP_FIN_WAIT1:
+	case TCP_CLOSE_WAIT:
+	case TCP_CLOSE:
+		/* FALLTHRU */
+		sk->sk_user_data = NULL;
+		i10_target_schedule_release_queue(queue);
+		break;
+	default:
+		pr_warn("queue %d unhandled state %d\n",
+			queue->idx, sk->sk_state);
+	}
+done:
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+static int i10_target_set_queue_sock(struct i10_target_queue *queue)
+{
+	struct socket *sock = queue->sock;
+	struct inet_sock *inet = inet_sk(sock->sk);
+	struct linger sol = { .l_onoff = 1, .l_linger = 0 };
+	int ret;
+
+	ret = kernel_getsockname(sock,
+		(struct sockaddr *)&queue->sockaddr);
+	if (ret < 0)
+		return ret;
+
+	ret = kernel_getpeername(sock,
+		(struct sockaddr *)&queue->sockaddr_peer);
+	if (ret < 0)
+		return ret;
+
+	/*
+	 * Cleanup whatever is sitting in the TCP transmit queue on socket
+	 * close. This is done to prevent stale data from being sent should
+	 * the network connection be restored before TCP times out.
+	 */
+	ret = kernel_setsockopt(sock, SOL_SOCKET, SO_LINGER,
+			(char *)&sol, sizeof(sol));
+	if (ret)
+		return ret;
+
+	/* Set socket type of service */
+	if (inet->rcv_tos > 0) {
+		int tos = inet->rcv_tos;
+
+		ret = kernel_setsockopt(sock, SOL_IP, IP_TOS,
+				(char *)&tos, sizeof(tos));
+		if (ret)
+			return ret;
+	}
+
+	write_lock_bh(&sock->sk->sk_callback_lock);
+	sock->sk->sk_user_data = queue;
+	queue->data_ready = sock->sk->sk_data_ready;
+	sock->sk->sk_data_ready = i10_target_data_ready;
+	queue->state_change = sock->sk->sk_state_change;
+	sock->sk->sk_state_change = i10_target_state_change;
+	queue->write_space = sock->sk->sk_write_space;
+	sock->sk->sk_write_space = i10_target_write_space;
+	write_unlock_bh(&sock->sk->sk_callback_lock);
+
+	return 0;
+}
+
+static int i10_target_alloc_queue(struct i10_target_port *port,
+		struct socket *newsock)
+{
+	struct i10_target_queue *queue;
+	int ret;
+
+	queue = kzalloc(sizeof(*queue), GFP_KERNEL);
+	if (!queue)
+		return -ENOMEM;
+
+	INIT_WORK(&queue->release_work, i10_target_release_queue_work);
+	INIT_WORK(&queue->io_work, i10_target_io_work);
+	queue->sock = newsock;
+	queue->port = port;
+	queue->nr_cmds = 0;
+	spin_lock_init(&queue->state_lock);
+	queue->state = I10_TARGET_Q_CONNECTING;
+	INIT_LIST_HEAD(&queue->free_list);
+	init_llist_head(&queue->resp_list);
+	INIT_LIST_HEAD(&queue->resp_send_list);
+
+	queue->caravan_iovs = kcalloc(I10_TARGET_SEND_BUDGET * 3,
+				sizeof(*queue->caravan_iovs), GFP_KERNEL);
+	if (!queue->caravan_iovs) {
+		ret = -ENOMEM;
+		goto out_free_queue;
+	}
+	queue->caravan_cmds = kcalloc(I10_TARGET_SEND_BUDGET,
+				sizeof(*queue->caravan_cmds), GFP_KERNEL);
+	if (!queue->caravan_cmds) {
+		ret = -ENOMEM;
+		goto out_free_iovs;
+	}
+	queue->caravan_mapped = kcalloc(I10_TARGET_SEND_BUDGET,
+				sizeof(*queue->caravan_mapped), GFP_KERNEL);
+	if (!queue->caravan_mapped) {
+		ret = -ENOMEM;
+		goto out_free_cmds;
+	}
+	queue->nr_iovs = 0;
+	queue->nr_caravan_cmds = 0;
+	queue->nr_caravan_mapped = 0;
+	queue->caravan_len = 0;
+	queue->send_now = false;
+
+	queue->idx = ida_simple_get(&i10_target_queue_ida, 0, 0, GFP_KERNEL);
+	if (queue->idx < 0) {
+		ret = queue->idx;
+		goto out_free_mapped;
+	}
+
+	ret = i10_target_alloc_cmd(queue, &queue->connect);
+	if (ret)
+		goto out_ida_remove;
+
+	ret = nvmet_sq_init(&queue->nvme_sq);
+	if (ret)
+		goto out_free_connect;
+
+	port->last_cpu = cpumask_next_wrap(port->last_cpu,
+				cpu_online_mask, -1, false);
+	queue->cpu = port->last_cpu;
+	i10_target_prepare_receive_pdu(queue);
+
+	mutex_lock(&i10_target_queue_mutex);
+	list_add_tail(&queue->queue_list, &i10_target_queue_list);
+	mutex_unlock(&i10_target_queue_mutex);
+
+	ret = i10_target_set_queue_sock(queue);
+	if (ret)
+		goto out_destroy_sq;
+
+	queue_work_on(queue->cpu, i10_target_wq, &queue->io_work);
+
+	return 0;
+out_destroy_sq:
+	mutex_lock(&i10_target_queue_mutex);
+	list_del_init(&queue->queue_list);
+	mutex_unlock(&i10_target_queue_mutex);
+	nvmet_sq_destroy(&queue->nvme_sq);
+out_free_connect:
+	i10_target_free_cmd(&queue->connect);
+out_ida_remove:
+	ida_simple_remove(&i10_target_queue_ida, queue->idx);
+out_free_mapped:
+	kfree(queue->caravan_mapped);
+out_free_cmds:
+	kfree(queue->caravan_cmds);
+out_free_iovs:
+	kfree(queue->caravan_iovs);
+out_free_queue:
+	kfree(queue);
+	return ret;
+}
+
+static void i10_target_accept_work(struct work_struct *w)
+{
+	struct i10_target_port *port =
+		container_of(w, struct i10_target_port, accept_work);
+	struct socket *newsock;
+	int ret;
+
+	while (true) {
+		ret = kernel_accept(port->sock, &newsock, O_NONBLOCK);
+		if (ret < 0) {
+			if (ret != -EAGAIN)
+				pr_warn("failed to accept err=%d\n", ret);
+			return;
+		}
+		ret = i10_target_alloc_queue(port, newsock);
+		if (ret) {
+			pr_err("failed to allocate queue\n");
+			sock_release(newsock);
+		}
+	}
+}
+
+static void i10_target_listen_data_ready(struct sock *sk)
+{
+	struct i10_target_port *port;
+
+	read_lock_bh(&sk->sk_callback_lock);
+	port = sk->sk_user_data;
+	if (!port)
+		goto out;
+
+	if (sk->sk_state == TCP_LISTEN)
+		schedule_work(&port->accept_work);
+out:
+	read_unlock_bh(&sk->sk_callback_lock);
+}
+
+static int i10_target_add_port(struct nvmet_port *nport)
+{
+	struct i10_target_port *port;
+	__kernel_sa_family_t af;
+	int opt, ret;
+
+	port = kzalloc(sizeof(*port), GFP_KERNEL);
+	if (!port)
+		return -ENOMEM;
+
+	switch (nport->disc_addr.adrfam) {
+	case NVMF_ADDR_FAMILY_IP4:
+		af = AF_INET;
+		break;
+	case NVMF_ADDR_FAMILY_IP6:
+		af = AF_INET6;
+		break;
+	default:
+		pr_err("address family %d not supported\n",
+				nport->disc_addr.adrfam);
+		ret = -EINVAL;
+		goto err_port;
+	}
+
+	ret = inet_pton_with_scope(&init_net, af, nport->disc_addr.traddr,
+			nport->disc_addr.trsvcid, &port->addr);
+	if (ret) {
+		pr_err("malformed ip/port passed: %s:%s\n",
+			nport->disc_addr.traddr, nport->disc_addr.trsvcid);
+		goto err_port;
+	}
+
+	port->nport = nport;
+	port->last_cpu = -1;
+	INIT_WORK(&port->accept_work, i10_target_accept_work);
+	if (port->nport->inline_data_size < 0)
+		port->nport->inline_data_size = I10_TARGET_DEF_INLINE_DATA_SIZE;
+
+	ret = sock_create(port->addr.ss_family, SOCK_STREAM,
+				IPPROTO_TCP, &port->sock);
+	if (ret) {
+		pr_err("failed to create a socket\n");
+		goto err_port;
+	}
+
+	port->sock->sk->sk_user_data = port;
+	port->data_ready = port->sock->sk->sk_data_ready;
+	port->sock->sk->sk_data_ready = i10_target_listen_data_ready;
+
+	opt = 1;
+	ret = kernel_setsockopt(port->sock, IPPROTO_TCP,
+			TCP_NODELAY, (char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set TCP_NODELAY sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_setsockopt(port->sock, SOL_SOCKET, SO_REUSEADDR,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set SO_REUSEADDR sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/* Set a fixed size of sndbuf/rcvbuf (8MB) */
+	opt = 8388608;
+	ret = kernel_setsockopt(port->sock, SOL_SOCKET, SO_RCVBUFFORCE,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set SO_RCVBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_setsockopt(port->sock, SOL_SOCKET, SO_SNDBUFFORCE,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set SO_SNDBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_bind(port->sock, (struct sockaddr *)&port->addr,
+			sizeof(port->addr));
+	if (ret) {
+		pr_err("failed to bind port socket %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_listen(port->sock, 128);
+	if (ret) {
+		pr_err("failed to listen %d on port sock\n", ret);
+		goto err_sock;
+	}
+
+	nport->priv = port;
+	pr_info("enabling port %d (%pISpc)\n",
+		le16_to_cpu(nport->disc_addr.portid), &port->addr);
+
+	return 0;
+
+err_sock:
+	sock_release(port->sock);
+err_port:
+	kfree(port);
+	return ret;
+}
+
+static void i10_target_remove_port(struct nvmet_port *nport)
+{
+	struct i10_target_port *port = nport->priv;
+
+	write_lock_bh(&port->sock->sk->sk_callback_lock);
+	port->sock->sk->sk_data_ready = port->data_ready;
+	port->sock->sk->sk_user_data = NULL;
+	write_unlock_bh(&port->sock->sk->sk_callback_lock);
+	cancel_work_sync(&port->accept_work);
+
+	sock_release(port->sock);
+	kfree(port);
+}
+
+static void i10_target_delete_ctrl(struct nvmet_ctrl *ctrl)
+{
+	struct i10_target_queue *queue;
+
+	mutex_lock(&i10_target_queue_mutex);
+	list_for_each_entry(queue, &i10_target_queue_list, queue_list)
+		if (queue->nvme_sq.ctrl == ctrl)
+			kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+	mutex_unlock(&i10_target_queue_mutex);
+}
+
+static u16 i10_target_install_queue(struct nvmet_sq *sq)
+{
+	struct i10_target_queue *queue =
+		container_of(sq, struct i10_target_queue, nvme_sq);
+
+	if (sq->qid == 0) {
+		/* Let inflight controller teardown complete */
+		flush_scheduled_work();
+	}
+
+	queue->nr_cmds = sq->size * 2;
+	if (i10_target_alloc_cmds(queue))
+		return NVME_SC_INTERNAL;
+	return 0;
+}
+
+static void i10_target_disc_port_addr(struct nvmet_req *req,
+		struct nvmet_port *nport, char *traddr)
+{
+	struct i10_target_port *port = nport->priv;
+
+	if (inet_addr_is_any((struct sockaddr *)&port->addr)) {
+		struct i10_target_cmd *cmd =
+			container_of(req, struct i10_target_cmd, req);
+		struct i10_target_queue *queue = cmd->queue;
+
+		sprintf(traddr, "%pISc", (struct sockaddr *)&queue->sockaddr);
+	} else {
+		memcpy(traddr, nport->disc_addr.traddr, NVMF_TRADDR_SIZE);
+	}
+}
+
+static struct nvmet_fabrics_ops i10_target_ops = {
+	.owner			= THIS_MODULE,
+	.type			= NVMF_TRTYPE_I10,
+	.msdbd			= 1,
+	.has_keyed_sgls		= 0,
+	.add_port		= i10_target_add_port,
+	.remove_port		= i10_target_remove_port,
+	.queue_response		= i10_target_queue_response,
+	.delete_ctrl		= i10_target_delete_ctrl,
+	.install_queue		= i10_target_install_queue,
+	.disc_traddr		= i10_target_disc_port_addr,
+};
+
+static int __init i10_target_init(void)
+{
+	int ret;
+
+	i10_target_wq = alloc_workqueue("i10_target_wq", WQ_HIGHPRI, 0);
+	if (!i10_target_wq)
+		return -ENOMEM;
+
+	ret = nvmet_register_transport(&i10_target_ops);
+	if (ret)
+		goto err;
+
+	return 0;
+err:
+	destroy_workqueue(i10_target_wq);
+	return ret;
+}
+
+static void __exit i10_target_exit(void)
+{
+	struct i10_target_queue *queue;
+
+	nvmet_unregister_transport(&i10_target_ops);
+
+	flush_scheduled_work();
+	mutex_lock(&i10_target_queue_mutex);
+	list_for_each_entry(queue, &i10_target_queue_list, queue_list)
+		kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+	mutex_unlock(&i10_target_queue_mutex);
+	flush_scheduled_work();
+
+	destroy_workqueue(i10_target_wq);
+}
+
+module_init(i10_target_init);
+module_exit(i10_target_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("nvmet-transport-4"); /* 4 == NVMF_TRTYPE_I10 */
diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 22014e7..4dbfa36 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -19,10 +19,44 @@
 
 #define NVMET_TCP_DEF_INLINE_DATA_SIZE	(4 * PAGE_SIZE)
 
+#define I10_CARAVAN_CAPACITY		65536
+#define I10_TARGET_RECV_BUDGET		16
+#define I10_TARGET_SEND_BUDGET		16
+#define I10_TARGET_IO_WORK_BUDGET	64
+
 #define NVMET_TCP_RECV_BUDGET		8
 #define NVMET_TCP_SEND_BUDGET		8
 #define NVMET_TCP_IO_WORK_BUDGET	64
 
+/* jaehyun: proc for pdu aggregation */
+static int nvmet_tcp_io_recv_budget __read_mostly = 16;
+module_param(nvmet_tcp_io_recv_budget, int, 0644);
+MODULE_PARM_DESC(nvmet_tcp_io_recv_budget, "nvmet tcp io recv budget");
+
+static int nvmet_tcp_io_send_budget __read_mostly = 16;
+module_param(nvmet_tcp_io_send_budget, int, 0644);
+MODULE_PARM_DESC(nvmet_tcp_io_send_budget, "nvmet tcp io send budget");
+
+static int nvmet_tcp_io_work_budget __read_mostly = 64;
+module_param(nvmet_tcp_io_work_budget, int, 0644);
+MODULE_PARM_DESC(nvmet_tcp_io_work_budget, "nvmet tcp io work budget");
+
+static int nvmet_tcp_aggr __read_mostly = 1;
+module_param(nvmet_tcp_aggr, int, 0644);
+MODULE_PARM_DESC(nvmet_tcp_aggr, "nvmet tcp aggregation");
+
+static int i10_target_steering __read_mostly = 0;
+module_param(i10_target_steering, int, 0644);
+MODULE_PARM_DESC(i10_target_steering, "i10 thru kthread nice");
+
+static int i10_thru_nice __read_mostly = 0;
+module_param(i10_thru_nice, int, 0644);
+MODULE_PARM_DESC(i10_thru_nice, "i10 thru kthread nice");
+
+static int i10_thru_printk __read_mostly = 0;
+module_param(i10_thru_printk, int, 0644);
+MODULE_PARM_DESC(i10_thru_printk, "i10 thru kthread nice");
+
 enum nvmet_tcp_send_state {
 	NVMET_TCP_SEND_DATA_PDU,
 	NVMET_TCP_SEND_DATA,
@@ -80,6 +114,10 @@ enum nvmet_tcp_queue_state {
 	NVMET_TCP_Q_DISCONNECTING,
 };
 
+struct i10_target_cmd_caravan {
+	struct nvmet_tcp_cmd   *cmd;
+};
+
 struct nvmet_tcp_queue {
 	struct socket		*sock;
 	struct nvmet_tcp_port	*port;
@@ -88,6 +126,13 @@ struct nvmet_tcp_queue {
 	struct nvmet_cq		nvme_cq;
 	struct nvmet_sq		nvme_sq;
 
+	/* for blk-switch */
+	struct work_struct	io_work_lat;
+	int			prio_class;
+	int			orig_cpu;
+	int			numa_node;
+	unsigned int		metric;
+
 	/* send state */
 	struct nvmet_tcp_cmd	*cmds;
 	unsigned int		nr_cmds;
@@ -97,6 +142,17 @@ struct nvmet_tcp_queue {
 	int			send_list_len;
 	struct nvmet_tcp_cmd	*snd_cmd;
 
+	/* For i10 target caravans */
+	struct kvec		*caravan_iovs;
+	int			nr_iovs;
+	size_t			caravan_len;
+	struct i10_target_cmd_caravan *caravan_cmds;
+	int			nr_caravan_cmds;
+	bool			send_now;
+
+	struct page		**caravan_mapped;
+	int			nr_caravan_mapped;
+
 	/* recv state */
 	int			offset;
 	int			left;
@@ -135,6 +191,8 @@ struct nvmet_tcp_port {
 	struct nvmet_port	*nport;
 	struct sockaddr_storage addr;
 	int			last_cpu;
+	unsigned long		reset_metric;
+	unsigned long		print_time;
 	void (*data_ready)(struct sock *);
 };
 
@@ -143,6 +201,7 @@ static LIST_HEAD(nvmet_tcp_queue_list);
 static DEFINE_MUTEX(nvmet_tcp_queue_mutex);
 
 static struct workqueue_struct *nvmet_tcp_wq;
+static struct workqueue_struct *nvmet_tcp_wq_lat;
 static struct nvmet_fabrics_ops nvmet_tcp_ops;
 static void nvmet_tcp_free_cmd(struct nvmet_tcp_cmd *c);
 static void nvmet_tcp_finish_cmd(struct nvmet_tcp_cmd *cmd);
@@ -456,6 +515,26 @@ static void nvmet_tcp_process_resp_list(struct nvmet_tcp_queue *queue)
 	}
 }
 
+static inline bool i10_target_is_latency(struct nvmet_tcp_queue *queue)
+{
+	return queue->prio_class == 1;
+}
+
+static inline bool i10_target_is_admin_queue(struct nvmet_tcp_queue *queue)
+{
+	//return (queue->nvme_sq.qid == 0);
+	return (queue->nvme_sq.qid == 0 || queue->prio_class == 1);
+}
+
+
+static inline bool i10_target_is_caravan_full(struct nvmet_tcp_queue *queue)
+{
+	return (queue->caravan_len >= I10_CARAVAN_CAPACITY) ||
+		(queue->nr_iovs >= I10_TARGET_SEND_BUDGET * 3) ||
+		(queue->nr_caravan_cmds >= I10_TARGET_SEND_BUDGET) ||
+		(queue->nr_caravan_mapped >= I10_TARGET_SEND_BUDGET);
+}
+
 static struct nvmet_tcp_cmd *nvmet_tcp_fetch_cmd(struct nvmet_tcp_queue *queue)
 {
 	queue->snd_cmd = list_first_entry_or_null(&queue->resp_send_list,
@@ -489,7 +568,12 @@ static void nvmet_tcp_queue_response(struct nvmet_req *req)
 	struct nvmet_tcp_queue	*queue = cmd->queue;
 
 	llist_add(&cmd->lentry, &queue->resp_list);
-	queue_work_on(cmd->queue->cpu, nvmet_tcp_wq, &cmd->queue->io_work);
+
+	// jaehyun
+	if (i10_target_is_latency(queue))
+		queue_work_on(cmd->queue->cpu, nvmet_tcp_wq_lat, &cmd->queue->io_work_lat);
+	else
+		queue_work_on(cmd->queue->cpu, nvmet_tcp_wq, &cmd->queue->io_work);
 }
 
 static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)
@@ -498,7 +582,23 @@ static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)
 	int left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;
 	int ret;
 
-	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->data_pdu),
+	/* jaehyun: pdu aggregation */
+	struct nvmet_tcp_queue *queue = cmd->queue;
+
+	if (nvmet_tcp_aggr && !i10_target_is_admin_queue(queue)) {
+		if (i10_target_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		queue->caravan_iovs[queue->nr_iovs].iov_base =
+			cmd->data_pdu + cmd->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+		queue->caravan_len += left;
+		ret = left;
+	}
+	else
+		ret = kernel_sendpage(cmd->queue->sock,
+			virt_to_page(cmd->data_pdu),
 			offset_in_page(cmd->data_pdu) + cmd->offset,
 			left, MSG_DONTWAIT | MSG_MORE);
 	if (ret <= 0)
@@ -525,13 +625,28 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 		u32 left = cmd->cur_sg->length - cmd->offset;
 		int flags = MSG_DONTWAIT;
 
-		if ((!last_in_batch && cmd->queue->send_list_len) ||
-		    cmd->wbytes_done + left < cmd->req.transfer_len ||
-		    queue->data_digest || !queue->nvme_sq.sqhd_disabled)
-			flags |= MSG_MORE;
+		/* jaehyun: pdu aggregation */
+		if (nvmet_tcp_aggr && !i10_target_is_admin_queue(queue)) {
+			if (i10_target_is_caravan_full(queue)) {
+				queue->send_now = true;
+				return 1;
+			}
+			queue->caravan_iovs[queue->nr_iovs].iov_base =
+				kmap(page) + cmd->offset;
+			queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+			queue->caravan_mapped[queue->nr_caravan_mapped++] = page;
+			queue->caravan_len += left;
+			ret = left;
+		}
+		else {
+			if ((!last_in_batch && cmd->queue->send_list_len) ||
+				cmd->wbytes_done + left < cmd->req.transfer_len ||
+				queue->data_digest || !queue->nvme_sq.sqhd_disabled)
+				flags |= MSG_MORE;
 
-		ret = kernel_sendpage(cmd->queue->sock, page, cmd->offset,
+			ret = kernel_sendpage(cmd->queue->sock, page, cmd->offset,
 					left, flags);
+		}
 		if (ret <= 0)
 			return ret;
 
@@ -574,11 +689,30 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd,
 	int flags = MSG_DONTWAIT;
 	int ret;
 
+	struct nvmet_tcp_queue *queue = cmd->queue;
+
 	if (!last_in_batch && cmd->queue->send_list_len)
 		flags |= MSG_MORE;
 	else
 		flags |= MSG_EOR;
 
+	/* jaehyun: pdu aggregation */
+	if (nvmet_tcp_aggr && !i10_target_is_admin_queue(queue)) {
+		if (i10_target_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		queue->caravan_iovs[queue->nr_iovs].iov_base =
+			cmd->rsp_pdu + cmd->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+		queue->caravan_cmds[queue->nr_caravan_cmds++].cmd = cmd;
+		queue->caravan_len += left;
+		cmd->queue->snd_cmd = NULL;
+
+		cmd->offset += left;
+		return 1;
+	}
+
 	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->rsp_pdu),
 		offset_in_page(cmd->rsp_pdu) + cmd->offset, left, flags);
 	if (ret <= 0)
@@ -603,13 +737,30 @@ static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 	int flags = MSG_DONTWAIT;
 	int ret;
 
+	struct nvmet_tcp_queue *queue = cmd->queue;
+
 	if (!last_in_batch && cmd->queue->send_list_len)
 		flags |= MSG_MORE;
 	else
 		flags |= MSG_EOR;
 
-	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->r2t_pdu),
-		offset_in_page(cmd->r2t_pdu) + cmd->offset, left, flags);
+	/* jaehyun: pdu aggregation */
+	if (nvmet_tcp_aggr && !i10_target_is_admin_queue(queue)) {
+		if (i10_target_is_caravan_full(queue)) {
+			queue->send_now = true;
+			return 1;
+		}
+		queue->caravan_iovs[queue->nr_iovs].iov_base = cmd->r2t_pdu
+			+ cmd->offset;
+		queue->caravan_iovs[queue->nr_iovs++].iov_len = left;
+		queue->caravan_len += left;
+		ret = left;
+	}
+	else
+		ret = kernel_sendpage(cmd->queue->sock,
+			virt_to_page(cmd->r2t_pdu),
+			offset_in_page(cmd->r2t_pdu) + cmd->offset,
+			left, flags);
 	if (ret <= 0)
 		return ret;
 	cmd->offset += ret;
@@ -696,6 +847,13 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 	return 1;
 }
 
+/* jaehyun: to check if there's enough room in tcp_sndbuf */
+static inline int i10_target_sndbuf_nospace(struct nvmet_tcp_queue *queue,
+		int length)
+{
+	return sk_stream_wspace(queue->sock->sk) < length;
+}
+
 static int nvmet_tcp_try_send(struct nvmet_tcp_queue *queue,
 		int budget, int *sends)
 {
@@ -703,6 +861,44 @@ static int nvmet_tcp_try_send(struct nvmet_tcp_queue *queue,
 
 	for (i = 0; i < budget; i++) {
 		ret = nvmet_tcp_try_send_one(queue, i == budget - 1);
+
+		/* jaehyun: send i10 caravans */
+		if (nvmet_tcp_aggr && queue->caravan_len &&
+			(queue->send_now || ret <= 0 || i == budget - 1)) {
+			struct msghdr msg =
+				{ .msg_flags = MSG_DONTWAIT | MSG_EOR };
+			int i10_ret, j;
+
+			if (i10_target_sndbuf_nospace(queue,
+				queue->caravan_len)) {
+				set_bit(SOCK_NOSPACE,
+					&queue->sock->sk->sk_socket->flags);
+				return 0;
+			}
+
+			i10_ret = kernel_sendmsg(queue->sock, &msg,
+					queue->caravan_iovs,
+					queue->nr_iovs, queue->caravan_len);
+			if (unlikely(i10_ret <= 0))
+				pr_err("I10_TARGET: kernel_sendmsg fails (i10_ret %d)\n",
+					i10_ret);
+
+			for (j = 0; j < queue->nr_caravan_cmds; j++) {
+				kfree(queue->caravan_cmds[j].cmd->iov);
+				sgl_free(queue->caravan_cmds[j].cmd->req.sg);
+				nvmet_tcp_put_cmd(queue->caravan_cmds[j].cmd);
+			}
+
+			for (j = 0; j < queue->nr_caravan_mapped; j++)
+				kunmap(queue->caravan_mapped[j]);
+
+			queue->nr_iovs = 0;
+			queue->nr_caravan_cmds = 0;
+			queue->nr_caravan_mapped = 0;
+			queue->caravan_len = 0;
+			queue->send_now = false;
+		}
+
 		if (ret <= 0)
 			break;
 		(*sends)++;
@@ -905,6 +1101,9 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 	req = &queue->cmd->req;
 	memcpy(req->cmd, nvme_cmd, sizeof(*nvme_cmd));
 
+	// jaehyun
+	//printk(KERN_DEBUG "(pid %d cpu %d nice %d) nvmet_tcp_done_recv_pdu: command_id %d", current->pid, current->cpu, task_nice(current), req->cmd->common.command_id);
+
 	if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
 			&queue->nvme_sq, &nvmet_tcp_ops))) {
 		pr_err("failed cmd %p id %d opcode %d, data_len: %d\n",
@@ -1171,10 +1370,19 @@ static void nvmet_tcp_io_work(struct work_struct *w)
 	bool pending;
 	int ret, ops = 0;
 
+	/* blk-switch: set thru kthread's nice value */
+	if (task_nice(current) != i10_thru_nice) {
+		set_user_nice(current, i10_thru_nice);
+		printk(KERN_DEBUG "(pid %d cpu %d nice %d): thru kthread -> set nice to %d",
+			current->pid, current->cpu, task_nice(current), i10_thru_nice);
+	}
+
 	do {
 		pending = false;
 
-		ret = nvmet_tcp_try_recv(queue, NVMET_TCP_RECV_BUDGET, &ops);
+		// jaehyun
+		ret = nvmet_tcp_try_recv(queue, nvmet_tcp_io_recv_budget, &ops);
+		//ret = nvmet_tcp_try_recv(queue, NVMET_TCP_RECV_BUDGET, &ops);
 		if (ret > 0) {
 			pending = true;
 		} else if (ret < 0) {
@@ -1185,7 +1393,9 @@ static void nvmet_tcp_io_work(struct work_struct *w)
 			return;
 		}
 
-		ret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);
+		// jaehyun
+		ret = nvmet_tcp_try_send(queue, nvmet_tcp_io_send_budget, &ops);
+		//ret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);
 		if (ret > 0) {
 			/* transmitted message/data */
 			pending = true;
@@ -1197,7 +1407,8 @@ static void nvmet_tcp_io_work(struct work_struct *w)
 			return;
 		}
 
-	} while (pending && ops < NVMET_TCP_IO_WORK_BUDGET);
+	} while (pending && ops < nvmet_tcp_io_work_budget);
+	//} while (pending && ops < NVMET_TCP_IO_WORK_BUDGET);
 
 	/*
 	 * We exahusted our budget, requeue our selves
@@ -1206,6 +1417,51 @@ static void nvmet_tcp_io_work(struct work_struct *w)
 		queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
 }
 
+/* blk-switch: io_work_lat for lat-requests */
+static void nvmet_tcp_io_work_lat(struct work_struct *w)
+{
+	struct nvmet_tcp_queue *queue =
+		container_of(w, struct nvmet_tcp_queue, io_work_lat);
+	bool pending;
+	int ret, ops = 0;
+
+	do {
+		pending = false;
+
+		// jaehyun
+		ret = nvmet_tcp_try_recv(queue, nvmet_tcp_io_recv_budget, &ops);
+		if (ret > 0) {
+			pending = true;
+		} else if (ret < 0) {
+			if (ret == -EPIPE || ret == -ECONNRESET)
+				kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+			else
+				nvmet_tcp_fatal_error(queue);
+			return;
+		}
+
+		// jaehyun
+		ret = nvmet_tcp_try_send(queue, nvmet_tcp_io_send_budget, &ops);
+		if (ret > 0) {
+			/* transmitted message/data */
+			pending = true;
+		} else if (ret < 0) {
+			if (ret == -EPIPE || ret == -ECONNRESET)
+				kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+			else
+				nvmet_tcp_fatal_error(queue);
+			return;
+		}
+
+	} while (pending && ops < nvmet_tcp_io_work_budget);
+
+	/*
+	 * We exahusted our budget, requeue our selves
+	 */
+	if (pending)
+		queue_work_on(queue->cpu, nvmet_tcp_wq_lat, &queue->io_work_lat);
+}
+
 static int nvmet_tcp_alloc_cmd(struct nvmet_tcp_queue *queue,
 		struct nvmet_tcp_cmd *c)
 {
@@ -1343,10 +1599,14 @@ static void nvmet_tcp_release_queue_work(struct work_struct *w)
 
 	nvmet_tcp_restore_socket_callbacks(queue);
 	flush_work(&queue->io_work);
+	// jaehyun
+	flush_work(&queue->io_work_lat);
 
 	nvmet_tcp_uninit_data_in_cmds(queue);
 	nvmet_sq_destroy(&queue->nvme_sq);
 	cancel_work_sync(&queue->io_work);
+	// jaehyun
+	cancel_work_sync(&queue->io_work_lat);
 	sock_release(queue->sock);
 	nvmet_tcp_free_cmds(queue);
 	if (queue->hdr_digest || queue->data_digest)
@@ -1362,8 +1622,174 @@ static void nvmet_tcp_data_ready(struct sock *sk)
 
 	read_lock_bh(&sk->sk_callback_lock);
 	queue = sk->sk_user_data;
-	if (likely(queue))
-		queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
+	if (likely(queue)) {
+		// jaehyun
+		if (i10_target_steering) {
+			int nr_cpus = num_online_cpus(), nr_nodes = num_online_nodes();
+			struct nvmet_tcp_queue *iter_queue;
+			int lat_metric[6], thru_metric[6];
+			int local = queue->cpu / nr_nodes;
+			int i, j, target_cpu = -1, min_metric = 1000000;
+
+			int lat_qo[4][6], thru_qo[4][6];
+			int lat_core[4][6], thru_core[4][6];
+
+			// 0. Reset metrics and core if there's no traffic for 100ms
+			if (queue->nvme_sq.qid != 0 &&
+			   (queue->port->reset_metric == 0 ||
+			   time_after(jiffies, queue->port->reset_metric))) {
+				list_for_each_entry(iter_queue, &nvmet_tcp_queue_list, queue_list) {
+					if (iter_queue->nvme_sq.qid != 0) {
+						iter_queue->metric = 0;
+						iter_queue->cpu = iter_queue->orig_cpu;
+					}
+				}
+				printk(KERN_DEBUG "(pid %d cpu %d nice %d) core steering / metrics are reset!",
+					current->pid, current->cpu, task_nice(current));
+			}
+			queue->port->reset_metric = jiffies + msecs_to_jiffies(200);
+
+			if (i10_thru_printk && queue->nvme_sq.qid != 0 && queue->cpu == 0 &&
+			   (queue->port->print_time == 0 || time_after(jiffies, queue->port->print_time))) {
+				queue->port->print_time = jiffies + msecs_to_jiffies(50);
+
+				for (i = 0; i < nr_nodes; i++) {
+					for (j = 0; j < nr_cpus/nr_nodes; j++) {
+						lat_qo[i][j] = 0;
+						lat_core[i][j] = 0;
+						thru_qo[i][j] = 0;
+						thru_core[i][j] = 0;
+					}
+				}
+
+				list_for_each_entry(iter_queue, &nvmet_tcp_queue_list, queue_list) {
+					if (iter_queue->idx > nr_cpus) {
+						lat_qo[iter_queue->numa_node][iter_queue->cpu/nr_nodes] +=
+							iter_queue->metric;
+							//atomic_read(&iter_queue->sock->sk->sk_rmem_alloc);
+						lat_core[iter_queue->numa_node][iter_queue->cpu/nr_nodes]++;
+					} else {
+						thru_qo[iter_queue->numa_node][iter_queue->cpu/nr_nodes] +=
+							iter_queue->metric;
+							//atomic_read(&iter_queue->sock->sk->sk_rmem_alloc);
+						thru_core[iter_queue->numa_node][iter_queue->cpu/nr_nodes]++;
+					}
+				}
+
+				for (i = 0; i < nr_nodes; i++) {
+					printk(KERN_DEBUG "===== NUMA %d =======", i);
+					printk(KERN_DEBUG "QO: [0](L %d T %d) [1](L %d T %d) [2](L %d T %d) [3](L %d T %d) [4](L %d T %d) [5](L %d T %d)",
+						lat_qo[i][0], thru_qo[i][0],
+						lat_qo[i][1], thru_qo[i][1],
+						lat_qo[i][2], thru_qo[i][2],
+						lat_qo[i][3], thru_qo[i][3],
+						lat_qo[i][4], thru_qo[i][4],
+						lat_qo[i][5], thru_qo[i][5]);
+					printk(KERN_DEBUG "CO: [0](L %d T %d) [1](L %d T %d) [2](L %d T %d) [3](L %d T %d) [4](L %d T %d) [5](L %d T %d)",
+						lat_core[i][0], thru_core[i][0],
+						lat_core[i][1], thru_core[i][1],
+						lat_core[i][2], thru_core[i][2],
+						lat_core[i][3], thru_core[i][3],
+						lat_core[i][4], thru_core[i][4],
+						lat_core[i][5], thru_core[i][5]);
+				}
+				printk(KERN_DEBUG "\n");
+			}
+
+			for (i = 0; i < nr_cpus/nr_nodes; i++) {
+				lat_metric[i] = 0;
+				thru_metric[i] = 0;
+			}
+
+			list_for_each_entry(iter_queue, &nvmet_tcp_queue_list, queue_list) {
+				// 1. update metric
+				if (iter_queue->nvme_sq.qid != 0 &&
+				   iter_queue->numa_node == queue->numa_node) {
+					int sample = atomic_read(&iter_queue->sock->sk->sk_rmem_alloc);
+
+					if (iter_queue->metric == 0)
+						iter_queue->metric = sample;
+					else {
+						iter_queue->metric -= (iter_queue->metric >> 3);
+						iter_queue->metric += (sample >> 3);
+					}
+
+					if (iter_queue->metric < 10)
+						iter_queue->metric = 0;
+
+					if (iter_queue->idx > nr_cpus)
+						// For lat-lanes
+						lat_metric[iter_queue->cpu/nr_nodes] += iter_queue->metric;
+					else
+						// For thru-lanes
+						thru_metric[iter_queue->cpu/nr_nodes] += iter_queue->metric;
+				}
+			}
+
+			// target-steering for lat-apps
+			if (i10_target_is_latency(queue)) {
+				for (i = 0; i < nr_cpus/nr_nodes; i++) {
+					if (i != local && lat_metric[i] && thru_metric[local] &&
+					   lat_metric[i]+thru_metric[i] < lat_metric[local]+thru_metric[local] &&
+					   lat_metric[i]+thru_metric[i] < min_metric) {
+						target_cpu = (i * nr_nodes) + queue->numa_node;
+						min_metric = lat_metric[i]+thru_metric[i];
+					}
+				}
+
+				if (target_cpu >= 0) {
+/*
+					if (i10_thru_printk)
+						printk(KERN_DEBUG "--LAT--(orig %d cpu %d numa %d) [0](%u %u) [1](%u %u) [2](%u %u) [3](%u %u) [4](%u %u) [5](%u %u) cpu %d -> %d",
+							queue->orig_cpu, queue->cpu, queue->numa_node,
+							lat_metric[0], thru_metric[0],
+							lat_metric[1], thru_metric[1],
+							lat_metric[2], thru_metric[2],
+							lat_metric[3], thru_metric[3],
+							lat_metric[4], thru_metric[4],
+							lat_metric[5], thru_metric[5],
+							queue->cpu, target_cpu);
+*/
+					queue->cpu = target_cpu;
+				}
+				queue_work_on(queue->cpu, nvmet_tcp_wq_lat, &queue->io_work_lat);
+			}
+			// target-steering for thru-apps
+			else {
+				for (i = 0; i < nr_cpus/nr_nodes; i++) {
+					if (i != local && lat_metric[local] &&
+					   lat_metric[i] < min_metric) {
+					   //thru_metric[i] < thru_metric[local]) {
+						target_cpu = (i * nr_nodes) + queue->numa_node;
+						min_metric = lat_metric[i];
+					}
+				}
+
+				if (target_cpu >= 0) {
+/*
+					if (i10_thru_printk)
+						 printk(KERN_DEBUG "::THRU::(orig %d cpu %d numa %d) [0](%u %u) [1](%u %u) [2](%u %u) [3](%u %u) [4](%u %u) [5](%u %u) cpu %d -> %d",
+							queue->orig_cpu, queue->cpu, queue->numa_node,
+							lat_metric[0], thru_metric[0],
+							lat_metric[1], thru_metric[1],
+							lat_metric[2], thru_metric[2],
+							lat_metric[3], thru_metric[3],
+							lat_metric[4], thru_metric[4],
+							lat_metric[5], thru_metric[5],
+							queue->cpu, target_cpu);
+*/
+					queue->cpu = target_cpu;
+				}
+				queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
+			}				
+		}
+		else {
+			if (i10_target_is_latency(queue))
+				queue_work_on(queue->cpu, nvmet_tcp_wq_lat, &queue->io_work_lat);
+			else
+				queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
+		}
+	}
 	read_unlock_bh(&sk->sk_callback_lock);
 }
 
@@ -1383,7 +1809,11 @@ static void nvmet_tcp_write_space(struct sock *sk)
 
 	if (sk_stream_is_writeable(sk)) {
 		clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
-		queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
+		// jaehyun
+		if (i10_target_is_latency(queue))
+			queue_work_on(queue->cpu, nvmet_tcp_wq_lat, &queue->io_work_lat);
+		else
+			queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
 	}
 out:
 	read_unlock_bh(&sk->sk_callback_lock);
@@ -1476,6 +1906,8 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
 
 	INIT_WORK(&queue->release_work, nvmet_tcp_release_queue_work);
 	INIT_WORK(&queue->io_work, nvmet_tcp_io_work);
+	// jaehyun
+	INIT_WORK(&queue->io_work_lat, nvmet_tcp_io_work_lat);
 	queue->sock = newsock;
 	queue->port = port;
 	queue->nr_cmds = 0;
@@ -1485,10 +1917,34 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
 	init_llist_head(&queue->resp_list);
 	INIT_LIST_HEAD(&queue->resp_send_list);
 
+	queue->caravan_iovs = kcalloc(I10_TARGET_SEND_BUDGET * 3,
+				sizeof(*queue->caravan_iovs), GFP_KERNEL);
+	if (!queue->caravan_iovs) {
+		ret = -ENOMEM;
+		goto out_free_queue;
+	}
+	queue->caravan_cmds = kcalloc(I10_TARGET_SEND_BUDGET,
+				sizeof(*queue->caravan_cmds), GFP_KERNEL);
+	if (!queue->caravan_cmds) {
+		ret = -ENOMEM;
+		goto out_free_iovs;
+	}
+	queue->caravan_mapped = kcalloc(I10_TARGET_SEND_BUDGET,
+				sizeof(*queue->caravan_mapped), GFP_KERNEL);
+	if (!queue->caravan_mapped) {
+		ret = -ENOMEM;
+		goto out_free_cmds;
+	}
+	queue->nr_iovs = 0;
+	queue->nr_caravan_cmds = 0;
+	queue->nr_caravan_mapped = 0;
+	queue->caravan_len = 0;
+	queue->send_now = false;
+
 	queue->idx = ida_simple_get(&nvmet_tcp_queue_ida, 0, 0, GFP_KERNEL);
 	if (queue->idx < 0) {
 		ret = queue->idx;
-		goto out_free_queue;
+		goto out_free_mapped;
 	}
 
 	ret = nvmet_tcp_alloc_cmd(queue, &queue->connect);
@@ -1499,9 +1955,26 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
 	if (ret)
 		goto out_free_connect;
 
-	port->last_cpu = cpumask_next_wrap(port->last_cpu,
-				cpu_online_mask, -1, false);
+	if (queue->idx != 1)
+		port->last_cpu = cpumask_next_wrap(port->last_cpu,
+					cpu_online_mask, -1, false);
 	queue->cpu = port->last_cpu;
+
+	if (queue->idx > num_online_cpus()) {
+		/* latency-sensitive i10-lanes */
+		queue->cpu++;
+		queue->prio_class = 1;
+	} else
+		/* throughput-bound i10-lanes */
+		queue->prio_class = 0;
+
+	if (queue->cpu >= num_online_cpus())
+		queue->cpu = 0;
+
+	queue->orig_cpu = queue->cpu;
+	queue->numa_node = queue->orig_cpu % num_online_nodes();
+	queue->metric = 0;
+
 	nvmet_prepare_receive_pdu(queue);
 
 	mutex_lock(&nvmet_tcp_queue_mutex);
@@ -1512,7 +1985,13 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
 	if (ret)
 		goto out_destroy_sq;
 
-	queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
+	// jaehyun
+	printk(KERN_DEBUG "nvmet_tcp_alloc_queue - nvmet_tcp_queue[%d]'s tcp connection established on io_cpu %d", queue->idx, queue->cpu);
+
+	if (i10_target_is_latency(queue))
+		queue_work_on(queue->cpu, nvmet_tcp_wq_lat, &queue->io_work_lat);
+	else
+		queue_work_on(queue->cpu, nvmet_tcp_wq, &queue->io_work);
 
 	return 0;
 out_destroy_sq:
@@ -1524,6 +2003,12 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
 	nvmet_tcp_free_cmd(&queue->connect);
 out_ida_remove:
 	ida_simple_remove(&nvmet_tcp_queue_ida, queue->idx);
+out_free_mapped:
+	kfree(queue->caravan_mapped);
+out_free_cmds:
+	kfree(queue->caravan_cmds);
+out_free_iovs:
+	kfree(queue->caravan_iovs);
 out_free_queue:
 	kfree(queue);
 	return ret;
@@ -1600,6 +2085,9 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 
 	port->nport = nport;
 	port->last_cpu = -1;
+	// jaehyun
+	port->reset_metric = 0;
+	port->print_time = 0;
 	INIT_WORK(&port->accept_work, nvmet_tcp_accept_work);
 	if (port->nport->inline_data_size < 0)
 		port->nport->inline_data_size = NVMET_TCP_DEF_INLINE_DATA_SIZE;
@@ -1630,6 +2118,22 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 		goto err_sock;
 	}
 
+	/* jaehyun: Set a fixed size of sndbuf/rcvbuf (8MB) */
+	opt = 8388608;
+	ret = kernel_setsockopt(port->sock, SOL_SOCKET, SO_RCVBUFFORCE,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set SO_RCVBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_setsockopt(port->sock, SOL_SOCKET, SO_SNDBUFFORCE,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set SO_SNDBUFFORCE sock opt %d\n", ret);
+		goto err_sock;
+	}
+
 	ret = kernel_bind(port->sock, (struct sockaddr *)&port->addr,
 			sizeof(port->addr));
 	if (ret) {
@@ -1730,9 +2234,13 @@ static int __init nvmet_tcp_init(void)
 {
 	int ret;
 
-	nvmet_tcp_wq = alloc_workqueue("nvmet_tcp_wq", WQ_HIGHPRI, 0);
+	//nvmet_tcp_wq = alloc_workqueue("nvmet_tcp_wq", WQ_HIGHPRI, 0);
+	nvmet_tcp_wq = alloc_workqueue("nvmet_tcp_wq", 0, 0);
 	if (!nvmet_tcp_wq)
 		return -ENOMEM;
+	nvmet_tcp_wq_lat = alloc_workqueue("nvmet_tcp_wq_lat", WQ_HIGHPRI, 0);
+	if (!nvmet_tcp_wq_lat)
+		return -ENOMEM;
 
 	ret = nvmet_register_transport(&nvmet_tcp_ops);
 	if (ret)
@@ -1741,6 +2249,7 @@ static int __init nvmet_tcp_init(void)
 	return 0;
 err:
 	destroy_workqueue(nvmet_tcp_wq);
+	destroy_workqueue(nvmet_tcp_wq_lat);
 	return ret;
 }
 
@@ -1758,6 +2267,7 @@ static void __exit nvmet_tcp_exit(void)
 	flush_scheduled_work();
 
 	destroy_workqueue(nvmet_tcp_wq);
+	destroy_workqueue(nvmet_tcp_wq_lat);
 }
 
 module_init(nvmet_tcp_init);
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 0bf056d..f2d11d2 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -37,6 +37,9 @@ struct blk_mq_hw_ctx {
 	struct blk_mq_ctx	*dispatch_from;
 	unsigned int		dispatch_busy;
 
+	/* blk-switch */
+	bool			blk_switch;
+
 	unsigned short		type;
 	unsigned short		nr_ctx;
 	struct blk_mq_ctx	**ctxs;
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index bff1def..1e8ebf3 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -140,6 +140,9 @@ struct request {
 	int tag;
 	int internal_tag;
 
+	/* blk-switch: 'y' if req is steered */
+	bool steered;
+
 	/* the following two fields are internal, NEVER access directly */
 	unsigned int __data_len;	/* total data len */
 	sector_t __sector;		/* sector cursor */
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index a260cd7..f638d66 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -45,6 +45,7 @@ enum {
 	NVMF_TRTYPE_RDMA	= 1,	/* RDMA */
 	NVMF_TRTYPE_FC		= 2,	/* Fibre Channel */
 	NVMF_TRTYPE_TCP		= 3,	/* TCP/IP */
+	NVMF_TRTYPE_I10		= 4,	/* i10 */
 	NVMF_TRTYPE_LOOP	= 254,	/* Reserved for host usage */
 	NVMF_TRTYPE_MAX,
 };
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 955e137..7b3ab5f 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -519,6 +519,9 @@ struct skb_shared_info {
 	struct skb_shared_hwtstamps hwtstamps;
 	unsigned int	gso_type;
 	u32		tskey;
+	
+	/* NetChannel: add page_pool */
+        void *page_pool;
 
 	/*
 	 * Warning : all fields before dataref are cleared in __alloc_skb()
diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 668e25a..848c470 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -369,6 +369,9 @@ struct tcp_sock {
 		u32	space;
 		u32	seq;
 		u64	time;
+/* NetChannel: add hol_alloc */
+		int	hol_alloc;
+		int	hol_len;
 	} rcvq_space;
 
 /* TCP-specific MTU probe information. */
@@ -395,6 +398,9 @@ struct tcp_sock {
 	 */
 	struct request_sock __rcu *fastopen_rsk;
 	u32	*saved_syn;
+/* NetChannel: add hol state */
+	atomic_t hol_alloc;
+	atomic_t hol_len;
 };
 
 enum tsq_enum {
diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 1121faa..be39870 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -75,7 +75,9 @@ struct page_pool {
 	unsigned long defer_start;
 	unsigned long defer_warn;
 
-	u32 pages_state_hold_cnt;
+	//u32 pages_state_hold_cnt;
+/* Modified for NetChannel */
+	atomic_t pages_state_hold_cnt;
 
 	/*
 	 * Data structure for allocation side
diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7cf1b49..b9617cc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -315,6 +315,10 @@ void tcp_tasklet_init(void);
 int tcp_v4_err(struct sk_buff *skb, u32);
 
 void tcp_shutdown(struct sock *sk, int how);
+/* NetChannel: add two non-static functions */
+void tcp_cleanup_rbuf(struct sock *sk, int copied);
+struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off);
+ 
 
 int tcp_v4_early_demux(struct sk_buff *skb);
 int tcp_v4_rcv(struct sk_buff *skb);
diff --git a/net/core/dev_ioctl.c b/net/core/dev_ioctl.c
index 5163d90..c47a9f0 100644
--- a/net/core/dev_ioctl.c
+++ b/net/core/dev_ioctl.c
@@ -97,6 +97,8 @@ int dev_ifconf(struct net *net, struct ifconf *ifc, int size)
 	return 0;
 }
 
+/* NetChannel: add export_symbol */
+EXPORT_SYMBOL(dev_ifconf);
 /*
  *	Perform the SIOCxIFxxx calls, inside rcu_read_lock()
  */
@@ -518,3 +520,7 @@ int dev_ioctl(struct net *net, unsigned int cmd, struct ifreq *ifr, bool *need_c
 		return -ENOTTY;
 	}
 }
+
+
+/* NetChannel: add export_symbol */
+EXPORT_SYMBOL(dev_ioctl);
diff --git a/net/core/page_pool.c b/net/core/page_pool.c
index dfc2501..86a560f 100644
--- a/net/core/page_pool.c
+++ b/net/core/page_pool.c
@@ -123,6 +123,7 @@ static struct page *__page_pool_alloc_pages_slow(struct page_pool *pool,
 	struct page *page;
 	gfp_t gfp = _gfp;
 	dma_addr_t dma;
+	int count;
 
 	/* We could always set __GFP_COMP, and avoid this branch, as
 	 * prep_new_page() can handle order-0 with __GFP_COMP.
@@ -159,11 +160,18 @@ static struct page *__page_pool_alloc_pages_slow(struct page_pool *pool,
 	}
 	page->dma_addr = dma;
 
-skip_dma_map:
+ 	/* Track how many pages are held 'in-flight' */skip_dma_map:
 	/* Track how many pages are held 'in-flight' */
-	pool->pages_state_hold_cnt++;
+	//pool->pages_state_hold_cnt++;
+	
+	/* Modified for NetChannel */
+	count = atomic_inc_return(&pool->pages_state_hold_cnt);
+
+
+//	trace_page_pool_state_hold(pool, page, pool->pages_state_hold_cnt);
+
+	trace_page_pool_state_hold(pool, page, count);
 
-	trace_page_pool_state_hold(pool, page, pool->pages_state_hold_cnt);
 
 	/* When page just alloc'ed is should/must have refcnt 1. */
 	return page;
@@ -195,7 +203,10 @@ EXPORT_SYMBOL(page_pool_alloc_pages);
 static s32 page_pool_inflight(struct page_pool *pool)
 {
 	u32 release_cnt = atomic_read(&pool->pages_state_release_cnt);
-	u32 hold_cnt = READ_ONCE(pool->pages_state_hold_cnt);
+	//u32 hold_cnt = READ_ONCE(pool->pages_state_hold_cnt);
+	
+	/* Modified for NetChannel */
+	u32 hold_cnt = atomic_read(&pool->pages_state_hold_cnt);
 	s32 inflight;
 
 	inflight = _distance(hold_cnt, release_cnt);
diff --git a/net/ipv4/inet_connection_sock.c b/net/ipv4/inet_connection_sock.c
index b0010c7..b2ee5b7 100644
--- a/net/ipv4/inet_connection_sock.c
+++ b/net/ipv4/inet_connection_sock.c
@@ -111,6 +111,8 @@ bool inet_rcv_saddr_any(const struct sock *sk)
 #endif
 	return !sk->sk_rcv_saddr;
 }
+/* NetChannel: add export_symbol */
+EXPORT_SYMBOL(inet_rcv_saddr_any);
 
 void inet_get_local_port_range(struct net *net, int *low, int *high)
 {
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index fe3cded..1866fff 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -453,8 +453,11 @@ void tcp_init_sock(struct sock *sk)
 	WRITE_ONCE(sk->sk_sndbuf, sock_net(sk)->ipv4.sysctl_tcp_wmem[1]);
 	WRITE_ONCE(sk->sk_rcvbuf, sock_net(sk)->ipv4.sysctl_tcp_rmem[1]);
 
-	sk_sockets_allocated_inc(sk);
+	 	sk->sk_route_forced_caps = NETIF_F_GSO;sk_sockets_allocated_inc(sk);
 	sk->sk_route_forced_caps = NETIF_F_GSO;
+	/* NetChannel: add hol state */
+	atomic_set(&tp->hol_alloc, 0);
+	atomic_set(&tp->hol_len, 0);
 }
 EXPORT_SYMBOL(tcp_init_sock);
 
@@ -1526,7 +1529,9 @@ static int tcp_peek_sndq(struct sock *sk, struct msghdr *msg, int len)
  * calculation of whether or not we must ACK for the sake of
  * a window update.
  */
-static void tcp_cleanup_rbuf(struct sock *sk, int copied)
+/* NetChannel: make the function non-static */
+void tcp_cleanup_rbuf(struct sock *sk, int copied)
+//static void tcp_cleanup_rbuf(struct sock *sk, int copied)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	bool time_to_ack = false;
@@ -1583,8 +1588,11 @@ static void tcp_cleanup_rbuf(struct sock *sk, int copied)
 	if (time_to_ack)
 		tcp_send_ack(sk);
 }
+EXPORT_SYMBOL(tcp_cleanup_rbuf);
 
-static struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)
+/* NetChannel: make the function non-static */
+struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)
+//static struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)
 {
 	struct sk_buff *skb;
 	u32 offset;
@@ -1607,6 +1615,7 @@ static struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)
 	}
 	return NULL;
 }
+EXPORT_SYMBOL(tcp_recv_skb);
 
 /*
  * This routine provides an alternative to tcp_recvmsg() for routines
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 677facb..96e5c11 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -79,6 +79,7 @@
 #include <trace/events/tcp.h>
 #include <linux/jump_label_ratelimit.h>
 #include <net/busy_poll.h>
+#include <linux/inet.h>
 
 int sysctl_tcp_max_orphans __read_mostly = NR_FILE;
 
@@ -662,6 +663,8 @@ void tcp_rcv_space_adjust(struct sock *sk)
 	tp->rcvq_space.seq = tp->copied_seq;
 	tp->rcvq_space.time = tp->tcp_mstamp;
 }
+/* NetChannel: add export_symbol */
+EXPORT_SYMBOL(tcp_rcv_space_adjust);
 
 /* There is something which you must keep in mind when you analyze the
  * behavior of the tp->ato delayed ack timeout interval.  When a
